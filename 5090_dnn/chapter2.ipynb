{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "chapterId": "ByNLu0mZx-z",
    "id": "chapter_name"
   },
   "source": [
    "#  深層学習のチューニング"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "table"
   },
   "source": [
    "- **[2.1 ハイパーパラメータ](#2.1-ハイパーパラメータ)**\n",
    "    - **[2.1.1 ハイパーパラメータ](#2.1.1-ハイパーパラメータ)**\n",
    "<br><br>\n",
    "- **[2.2 ネットワーク構造](#2.2-ネットワーク構造)**\n",
    "    - **[2.2.1 ネットワーク構造](#2.2.1-ネットワーク構造)**\n",
    "<br><br>\n",
    "- **[2.3 ドロップアウト](#2.3-ドロップアウト)**\n",
    "    - **[2.3.1 ドロップアウト](#2.3.1-ドロップアウト)**\n",
    "<br><br>\n",
    "- **[2.4 活性化関数](#2.4-活性化関数)**\n",
    "    - **[2.4.1 活性化関数](#2.4.1-活性化関数)**\n",
    "    - **[2.4.2 シグモイド関数](#2.4.2-シグモイド関数)**\n",
    "    - **[2.4.3 ReLU](#2.4.3-ReLU)**\n",
    "<br><br>\n",
    "- **[2.5 損失関数](#2.5-損失関数)**\n",
    "    - **[2.5.1 損失関数](#2.5.1-損失関数)**\n",
    "    - **[2.5.2 二乗誤差](#2.5.2-二乗誤差)**\n",
    "    - **[2.5.3 クロスエントロピー誤差](#2.5.3-クロスエントロピー誤差)**\n",
    "<br><br>\n",
    "- **[2.6 最適化関数](#2.6-最適化関数)**\n",
    "    - **[2.6.1 最適化関数](#2.6.1-最適化関数)**\n",
    "<br><br>\n",
    "- **[2.7 学習率](#2.7-学習率)**\n",
    "    - **[2.7.1 学習率](#2.7.1-学習率)**\n",
    "<br><br>\n",
    "- **[2.8 ミニバッチ学習](#2.8-ミニバッチ学習)**\n",
    "    - **[2.8.1 ミニバッチ学習](#2.8.1-ミニバッチ学習)**\n",
    "<br><br>\n",
    "- **[2.9 反復学習](#2.9-反復学習)**\n",
    "    - **[2.9.1 反復学習](#2.9.1-反復学習)**\n",
    "<br><br>\n",
    "- **[2.10 添削問題](#2.10-添削問題)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section_name",
    "sectionId": "SyH8uRXZebf"
   },
   "source": [
    "## 2.1 ハイパーパラメータ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5090,
    "exerciseId": "Syd8938sUlf",
    "id": "quiz_session_name",
    "important": true,
    "isDL": false,
    "timeoutSecs": 10
   },
   "source": [
    "### 2.1.1 ハイパーパラメータ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "深層学習手法を使うと、分類あるいは回帰のアルゴリズムをほとんど自動的に生成できるためとても便利です。  \n",
    "また、ニューラルネットワークモデルはいろいろな場面に適用させることができ汎用的です。  \n",
    "\n",
    "しかし、ネットワークを構成する際に人が調整するべきパラメーターがいくつか存在します。  \n",
    "これらは **<font color=#AA0000>ハイパーパラメータ</font>** と呼ばれます。  \n",
    "\n",
    "以下は、chapter1のMNIST分類のコードに少しだけ変更を加え、またいくつかのパラメーターを明示した典型的な深層学習手法のコードです。  \n",
    "以下のコードのどこがハイパーパラメータに相当するのかを見ていきます。\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from keras.models import Sequential, load_model\n",
    "from keras import optimizers\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 784)[:6000]\n",
    "X_test = X_test.reshape(X_test.shape[0], 784)[:1000]\n",
    "y_train = to_categorical(y_train)[:6000]\n",
    "y_test = to_categorical(y_test)[:1000]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=784))\n",
    "# ハイパーパラメータ：活性化関数\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "# ハイパーパラメータ：隠れ層の数、隠れ層のチャンネル数\n",
    "model.add(Dense(128))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "# ハイパーパラメータ：ドロップアウトする割合（rate）\n",
    "model.add(Dropout(rate=0.5))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "# ハイパーパラメータ：学習率（Ir）\n",
    "sgd = optimizers.SGD(lr=0.01)\n",
    "\n",
    "# ハイパーパラメータ：最適化関数（optimizer）\n",
    "# ハイパーパラメータ：誤差関数（loss）\n",
    "model.compile(optimizer=sgd, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# ハイパーパラメータ：バッチサイズ（batch_size）\n",
    "# ハイパーパラメータ：エポック数（epochs）\n",
    "model.fit(X_train, y_train, batch_size=32, epochs=10, verbose=1)\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"evaluate loss: {0[0]}\\nevaluate acc: {0[1]}\".format(score))\n",
    "```\n",
    "（metricsは評価関数なので、学習自体には関係ありません。評価関数については機械学習概論を参照してください。）\n",
    "\n",
    "上記のようにハイパーパラメータはたくさんあります。  \n",
    "ハイパーパラメータは自動で変更できないものです。これらを適切に設定しないと正しく学習が行われません。  \n",
    "自分で新しくモデルを作る時には最適なハイパーパラメータを吟味する必要があります。    \n",
    "このchapterではそれぞれのハイパーパラメータの意味を理解し、自分でネットワークを構成、調整ができるようにしていきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "- ハイパーパラメータについて説明した文として正しいものを選んでください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "choices"
   },
   "source": [
    "- ハイパーパラメータは学習時にモデルが自動的に調整する。\n",
    "- ハイパーパラメータは自分で調整する必要がある。\n",
    "- ハイパーパラメータは適切に設定するのが良いが、適切でなくとも多くの場合問題なく学習は進行する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- パラメーターのうち人が調整するパラメーターをハイパーパラメーターと言います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "answer"
   },
   "source": [
    "- ハイパーパラメータは自分で調整する必要がある。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section_name",
    "sectionId": "rkIIOA7bgWz"
   },
   "source": [
    "## 2.2 ネットワーク構造"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5090,
    "exerciseId": "BkKUqhIoUgz",
    "id": "code_session_name",
    "important": true,
    "isDL": false,
    "timeoutSecs": 30
   },
   "source": [
    "### 2.2.1 ネットワーク構造 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "**ネットワークの構造（隠れ層の数、隠れ層のユニット数）は自由に決めて生成することができます。**  \n",
    "\n",
    "一般に、隠れ層やユニット数を多くすると、多彩な関数が表現できるようになります。  \n",
    "しかし、隠れ層が多くなると、入力層に近い重みを適切に更新するのが難しく学習がなかなか進みにくくなったり、  \n",
    "隠れ層のユニット数が多くなると重要性の低い特徴量を抽出してしまい過学習(汎化性能が低くなった状態)をしやすくなるなど、  \n",
    "適切にネットワークの構造を設定する必要があります。\n",
    "\n",
    "ネットワーク構造は理論で裏付けて定めることが難しく、実際には他の似たような実装例を参考にするなど経験に基づいて決定される傾向があります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "- 次の3つの中から一番精度の出るモデルを予想し、以下のコードを一部変更してください。\n",
    "- ネットワーク構造、特に隠れ層の構造がモデルの学習に与える影響を確認してください\n",
    " - A: ユニット数256の全結合隠れ層1つと、ユニット数128の全結合隠れ層1つをもつモデル（ハイパーパラメーターの章のものと同じモデル）\n",
    " - B: ユニット数256の全結合隠れ層1つと、ユニット数128の全結合隠れ層3つをもつモデル\n",
    " - C: ユニット数256の全結合隠れ層1つと、ユニット数1568の全結合隠れ層1つをもつモデル\n",
    "- 条件は以下のようにします。\n",
    " - コードを二行コメントアウトし、他は変えないでください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "index",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from keras.models import Sequential, load_model\n",
    "from keras import optimizers\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 784)[:6000]\n",
    "X_test = X_test.reshape(X_test.shape[0], 784)[:1000]\n",
    "y_train = to_categorical(y_train)[:6000]\n",
    "y_test = to_categorical(y_test)[:1000]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=784))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "def funcA():\n",
    "    model.add(Dense(128))\n",
    "    model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "def funcB():\n",
    "    model.add(Dense(128))\n",
    "    model.add(Activation(\"sigmoid\"))\n",
    "    model.add(Dense(128))\n",
    "    model.add(Activation(\"sigmoid\"))\n",
    "    model.add(Dense(128))\n",
    "    model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "def funcC():\n",
    "    model.add(Dense(1568))\n",
    "    model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "# 二つコメントアウトしてください。\n",
    "#---------------------------\n",
    "funcA()\n",
    "funcB()\n",
    "funcC()\n",
    "#---------------------------\n",
    "\n",
    "model.add(Dropout(rate=0.5))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.1)\n",
    "\n",
    "model.compile(optimizer=sgd, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=32, epochs=3, verbose=1)\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"evaluate loss: {0[0]}\\nevaluate acc: {0[1]}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- 全パターン試してみてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "answer",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from keras.models import Sequential, load_model\n",
    "from keras import optimizers\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 784)[:6000]\n",
    "X_test = X_test.reshape(X_test.shape[0], 784)[:1000]\n",
    "y_train = to_categorical(y_train)[:6000]\n",
    "y_test = to_categorical(y_test)[:1000]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=784))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "def funcA():\n",
    "    model.add(Dense(128))\n",
    "    model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "def funcB():\n",
    "    model.add(Dense(128))\n",
    "    model.add(Activation(\"sigmoid\"))\n",
    "    model.add(Dense(128))\n",
    "    model.add(Activation(\"sigmoid\"))\n",
    "    model.add(Dense(128))\n",
    "    model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "def funcC():\n",
    "    model.add(Dense(1568))\n",
    "    model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "# 二つコメントアウトしてください。\n",
    "#---------------------------\n",
    "funcA()\n",
    "#funcB()\n",
    "#funcC()\n",
    "#---------------------------\n",
    "\n",
    "model.add(Dropout(rate=0.5))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.1)\n",
    "\n",
    "model.compile(optimizer=sgd, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=32, epochs=3, verbose=1)\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"evaluate loss: {0[0]}\\nevaluate acc: {0[1]}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section_name",
    "sectionId": "BkPIOAXZebG"
   },
   "source": [
    "## 2.3 ドロップアウト"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5090,
    "exerciseId": "HJqI92UsIgG",
    "id": "code_session_name",
    "important": true,
    "isDL": false,
    "timeoutSecs": 30
   },
   "source": [
    "### 2.3.1 ドロップアウト"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "**<font color=#AA0000>ドロップアウト</font>** は、 **過学習を防ぎモデルの精度をあげるための手法の一つ** です。  \n",
    "\n",
    "ドロップアウトを使うと、ユニットの一部が学習のたびにランダムに削除（より正確には0で上書き）されます。  \n",
    "これにより、ニューラルネットは特定のニューロンの存在に依存できなくなり、より **汎用的な（学習データ以外でも通用しやすい）特徴を学習する** ようになります。 \n",
    "その結果、学習データに対する過学習を防ぐことができます。  \n",
    "\n",
    "ドロップアウトは以下のようにして使います。\n",
    "```python\n",
    "model.add(Dropout(rate=0.5))\n",
    "```\n",
    "ここでrateは削除するユニットの割合です。\n",
    "\n",
    "**ドロップアウトを使う位置、引数のrateはともにハイパーパラメータです。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "- ドロップアウトを実装して、訓練データとテストデータそれぞれの正解率が近くなっていることを確認してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "index"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from keras.models import Sequential, load_model\n",
    "from keras import optimizers\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 784)[:6000]\n",
    "X_test = X_test.reshape(X_test.shape[0], 784)[:1000]\n",
    "y_train = to_categorical(y_train)[:6000]\n",
    "y_test = to_categorical(y_test)[:1000]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=784))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "#---------------------------\n",
    "#        ここを書いて下さい       \n",
    "#---------------------------\n",
    "\n",
    "model.add(Dense(10))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.1)\n",
    "\n",
    "model.compile(optimizer=sgd, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(X_train, y_train, batch_size=32, epochs=5, verbose=1, validation_data=(X_test, y_test))\n",
    "\n",
    "#acc, val_accのプロット\n",
    "plt.plot(history.history[\"acc\"], label=\"acc\", ls=\"-\", marker=\"o\")\n",
    "plt.plot(history.history[\"val_acc\"], label=\"val_acc\", ls=\"-\", marker=\"x\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- ドロップアウトの実装は、`Dropout()`を用います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "answer"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from keras.models import Sequential, load_model\n",
    "from keras import optimizers\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 784)[:6000]\n",
    "X_test = X_test.reshape(X_test.shape[0], 784)[:1000]\n",
    "y_train = to_categorical(y_train)[:6000]\n",
    "y_test = to_categorical(y_test)[:1000]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=784))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "#---------------------------\n",
    "model.add(Dropout(rate=0.5))\n",
    "#---------------------------\n",
    "model.add(Dense(10))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.1)\n",
    "\n",
    "model.compile(optimizer=sgd, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(X_train, y_train, batch_size=32, epochs=5, verbose=1, validation_data=(X_test, y_test))\n",
    "\n",
    "#acc, val_accのプロット\n",
    "plt.plot(history.history[\"acc\"], label=\"acc\", ls=\"-\", marker=\"o\")\n",
    "plt.plot(history.history[\"val_acc\"], label=\"val_acc\", ls=\"-\", marker=\"x\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section_name",
    "sectionId": "BJuLO0QbgZG"
   },
   "source": [
    "## 2.4 活性化関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5090,
    "exerciseId": "SJo89hLjLgf",
    "id": "quiz_session_name",
    "important": true,
    "isDL": false,
    "timeoutSecs": 10
   },
   "source": [
    "### 2.4.1 活性化関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "**<font color=#AA0000>活性化関数</font>** とは、主に全結合層の後に適用する関数で、もともとニューロンの発火に相当していたものです。  \n",
    "\n",
    "全結合層では、入力を線形変換したものを出力しますが、 **活性化関数を用いることで非線形性をもたせます** 。\n",
    "\n",
    "活性化関数を使わない場合、以下のような一本の直線で分離できない（線形分離不可能）データは分類できないことが数学的にわかっています。\n",
    "\n",
    "<img src=\"https://aidemyexcontentspic.blob.core.windows.net/contents-pic/5090_dnn/dnn_chap2_10.png\">\n",
    "\n",
    "**非線形性をもたせることで、適切に学習が進めば線形分離不可能なモデルでも必ず分類することができます。**\n",
    "\n",
    "\n",
    "**活性化関数もハイパーパラメータです。**  \n",
    "よく使われる活性化関数はいくつかあり、適切に選ぶ必要があります。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "- 活性化関数を使う理由として正しい選択肢を選んでください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "choices"
   },
   "source": [
    "- モデルに線形性をもたせ、線形分離可能なデータに対応させるため。\n",
    "- モデルに線形性をもたせ、線形分離不可能なデータに対応させるため。\n",
    "- モデルに非線形性をもたせ、線形分離可能なデータに対応させるため。\n",
    "- モデルに非線形性をもたせ、線形分離不可能なデータに対応させるため。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- モデルが線型性の場合は、線形分離不可能なデータを分類できません。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "answer"
   },
   "source": [
    "- モデルに非線形性をもたせ、線形分離不可能なデータに対応させるため。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5090,
    "exerciseId": "By2Ucn8jIlf",
    "id": "quiz_session_name",
    "important": true,
    "isDL": false,
    "timeoutSecs": 10
   },
   "source": [
    "### 2.4.2 シグモイド関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "活性化関数として用いられる関数の1つに **<font color=#AA0000>シグモイド関数</font>** というものがあり、この関数は次式で与えられます。\n",
    "\n",
    "$$\n",
    "sigmoid(x) = \\frac{1}{1+e^{-x}}\n",
    "$$\n",
    "\n",
    "<img src=\"https://aidemyexcontentspic.blob.core.windows.net/contents-pic/5090_dnn/dnn_chap2_20.png\">\n",
    "\n",
    "青いグラフがシグモイド関数で、オレンジ色のグラフがシグモイド関数の導関数です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "- 説明文のグラフからわかる、シグモイド関数の説明として正しいものを1つ選んでください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "choices"
   },
   "source": [
    "- 出力は必ず区間 `(0,1)` に収まるので、極端な出力値が少ない。\n",
    "- どのような区間にも収まらず、極端な出力値が生成される可能性がある。 \n",
    "- 出力が広い値をとるので、学習速度が早くなる。\n",
    "- 出力の範囲が限られていないので、学習速度が遅くなる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- 縦軸の値に注目しましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "answer"
   },
   "source": [
    "出力は必ず区間 `(0,1)` に収まるので、極端な出力値が少ない。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5090,
    "exerciseId": "BkpIqhLjUgf",
    "id": "quiz_session_name",
    "important": true,
    "isDL": false,
    "timeoutSecs": 10
   },
   "source": [
    "### 2.4.3 ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "もうひとつ活性化関数によく用いられる **<font color=#AA0000>ReLU（ランプ関数）</font>** というものについて説明します。  \n",
    "ReLUはRectified Linear Unitの略で次式のような関数です。\n",
    "\n",
    "$$\n",
    "\\mathrm{ReLU}(x) = \\begin{cases} 0 (x<0) \\\\ x (x\\geq 0)\\end{cases}\n",
    "$$\n",
    "\n",
    "<img src=\"https://aidemyexcontentspic.blob.core.windows.net/contents-pic/5090_dnn/dnn_chap2_30.png\">\n",
    "\n",
    "青いグラフがReLUで、オレンジ色のグラフがReLUの導関数です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "- 説明文のグラフからわかる、ReLUの説明として正しいものを1つ選んでください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "choices"
   },
   "source": [
    "- 出力は必ず区間(0,1)に収まるので、極端な出力値が少ない。\n",
    "- 出力はどのような区間にも収まらず、極端な出力値が生成されうる。\n",
    "- 出力が広い値をとるので、学習速度が遅くなる。\n",
    "- 出力の範囲が限られているので、学習速度が遅くなる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- 一般的に、出力が大きい値だと学習速度は早くなります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "answer"
   },
   "source": [
    "- 出力はどのような区間にも収まらず、極端な出力値が生成されうる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section_name",
    "sectionId": "rktIOCX-gZz"
   },
   "source": [
    "## 2.5 損失関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5090,
    "exerciseId": "S1A85hIsIxz",
    "id": "quiz_session_name",
    "important": true,
    "isDL": false,
    "timeoutSecs": 10
   },
   "source": [
    "### 2.5.1 損失関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "学習時に、モデルの出力と教師データとの差（間違え具合）を評価する関数を **<font color=#AA0000>損失関数（誤差関数）</font>** といいます。\n",
    "\n",
    "損失関数には **二乗誤差** や **クロスエントロピー誤差** などが用いられます。  \n",
    "\n",
    "この損失関数を最小化するように誤差逆伝播法という手法で各層の重みは更新されます。  \n",
    "\n",
    "損失関数としてなぜ正解率を使わないのか疑問に思うかもしれません。  \n",
    "確かに正解率を使うことはできなくはないですが、  \n",
    "モデルがどの程度間違えているかやどういう間違え方（どのクラスだと誤認したのか）をしているかなどを総合的に評価するために、二乗誤差やクロスエントロピー誤差を損失関数として使うのが一般的です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    " - 損失関数の性質を示した文として適切なものを選んでください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "choices"
   },
   "source": [
    "- 一般に、損失関数を最大化するように各層の重みを更新する。\n",
    "- 損失関数は重みを更新する際に重要な役割を持つので、適切なものを選ぶ必要がある。\n",
    "- 損失関数には正解率を求める式をそのまま使うのが良い。\n",
    "- 損失関数は1種類しかないので、これはハイパーパラメータではない。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- 損失関数には様々な種類が存在し、値を最小化するように重みの更新をします。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "answer"
   },
   "source": [
    "- 損失関数は重みを更新する際に重要な役割を持つので、適切なものを選ぶ必要がある。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5090,
    "exerciseId": "BJJvch8iIlG",
    "id": "quiz_session_name",
    "important": true,
    "isDL": false,
    "timeoutSecs": 10
   },
   "source": [
    "### 2.5.2 二乗誤差"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "**<font color=#AA0000>二乗誤差</font>** は、最小二乗法として統計学など様々な分野で用いられる誤差関数です。\n",
    "\n",
    "$$\n",
    "{E} = \\sum_{i=1}^N ({t_i - y_i})^{2}\n",
    "$$\n",
    "\n",
    "**連続値の評価に優れているため主に回帰モデルの誤差関数** として使われます。上式の$y_i$、$t_i$はそれぞれ、予測ラベル、正解ラベルを表しています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "- 二乗誤差の説明として正しいものを1つ選んでください。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "choices"
   },
   "source": [
    "- 回帰に向いており、最小値の付近ではゆっくりと更新が行われるため、学習が収束しやすい\n",
    "- 回帰に向いており、最小値の付近ではゆっくりと更新が行われるため、学習が収束しにくい\n",
    "- 分類に向いており、最小値の付近ではゆっくりと更新が行われるため、学習が収束しやすい\n",
    "- 分類に向いており、最小値の付近ではゆっくりと更新が行われるため、学習が収束しにくい"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- 下に凸の放物線をイメージしてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "answer"
   },
   "source": [
    "- 回帰に向いており、最小値の付近ではゆっくりと更新が行われるため、学習が収束しやすい"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5090,
    "exerciseId": "rygv9n8i8xf",
    "id": "quiz_session_name",
    "important": true,
    "isDL": false,
    "timeoutSecs": 30
   },
   "source": [
    "### 2.5.3 クロスエントロピー誤差"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "**<font color=#AA0000>クロスエントロピー誤差</font>** は、 **二値分類の評価に特化しているため、主に分類モデルの誤差関数** として使われます。  \n",
    "\n",
    "$$\n",
    "E=\\sum_{i=1}^N (-t_i\\log y_i-(1-t_i)\\log (1-y_i))\n",
    "$$\n",
    "\n",
    "それでは、この関数がどのような特性をもつのかみていきましょう。\n",
    "\n",
    "(i) $t_i$ << $y_i$ のとき\n",
    "$-t_i\\log y_i$ はほぼ0で、 $-(1-t_i)\\log (1-y_i)$ は正の無限大です。  \n",
    "(ii) $t_i$ >> $y_i$ のとき\n",
    "$-t_i\\log y_i$ は正の無限大で、 $-(1-t_i)\\log (1-y_i)$ はほぼ0です。  \n",
    "(iii) $t_i$ ≒ $y_i$ のとき  $-t_i\\log y_i-(1-t_i)\\log (1-y_i)$ は 0.69... ~ 0 の値を取ることが簡単な計算で求まります。\n",
    "\n",
    "したがって $-t_i\\log y_i-(1-t_i)\\log (1-y_i)$ は、  \n",
    "$|t_i - y_i|$ が大きいとき極端に大きな値を返し、$|t_i - y_i|$ が小さいとき0に近い値をとることがわかります。\n",
    "\n",
    "分類の学習において、予測ラベル$y_i$と正解ラベル$t_i$の値は近いほど良いのでこの関数は有用です。\n",
    "これらのことから、クロスエントロピー誤差は、 **0~1の2つの数の差を評価する上で合理的な関数** であると言えます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "- クロスエントロピー誤差について正しい選択肢を選んでください。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "choices"
   },
   "source": [
    "- 正解ラベルと予測ラベルの値が近いほど小さい値をとなる。\n",
    "- 多クラス分類に特化した誤差関数である。\n",
    "- 回帰問題に頻繁に用いられる誤差関数である。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- クロスエントロピーは二値分類の評価に特化しており、主に分類モデルの誤差関数として使われます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "answer"
   },
   "source": [
    "- 正解ラベルと予測ラベルの値が近いほど小さい値をとなる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section_name",
    "sectionId": "rJ5LOAXWgbf"
   },
   "source": [
    "## 2.6 最適化関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5090,
    "exerciseId": "rJWv53IsUlG",
    "id": "quiz_session_name",
    "important": true,
    "isDL": false,
    "timeoutSecs": 30
   },
   "source": [
    "### 2.6.1 最適化関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "重みの更新は、誤差関数を各重みで微分した値を元に、更新すべき方向とどの程度更新するかを決めます。  \n",
    "微分によって求めた値を、 **学習率、エポック数、過去の重みの更新量など** を踏まえてどのように重みの更新に反映するかを定めるのが **<font color=#AA0000>最適化関数</font>** です。\n",
    "\n",
    "<img src=\"https://aidemyexcontentspic.blob.core.windows.net/contents-pic/5090_dnn/dnn_chap2_40.gif\">\n",
    "\n",
    "最適化関数はハイパーパラメータです。\n",
    "上図が示すように、最適化関数にはいくつか種類があり、正しく選択しないと学習に悪影響を|及ぼします。\n",
    "\n",
    "> 出典:http://cs231n.github.io/neural-networks-3/#add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    " - 最適化関数の性質を示した文として適切なものを選んでください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "choices"
   },
   "source": [
    "- 一般に、最適化関数を最大化するように各層の重みを更新する。\n",
    "- 最適化関数はどれを選んでも最適化されるため、選ぶ必要はない。\n",
    "- 最適化関数は損失関数、エポック数など複数の情報を踏まえて重みの更新を行う。\n",
    "- 最適化関数には1種類しかないので、これはハイパーパラメータではない。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- 最適化関数は様々な要素を踏まえて重みの更新を行いますが、手法によって重みの更新の仕方が異なり、ハイパーパラメータの一種となっています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "answer"
   },
   "source": [
    "- 最適化関数は損失関数、エポック数など複数の情報を踏まえて重みの更新を行う。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section_name",
    "sectionId": "S1sUOCX-l-M"
   },
   "source": [
    "## 2.7 学習率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5090,
    "exerciseId": "HkMP93IoLlM",
    "id": "code_session_name",
    "important": true,
    "isDL": false,
    "timeoutSecs": 30
   },
   "source": [
    "### 2.7.1 学習率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "**<font color=#AA0000>学習率</font>** とは、 **各層の重みを一度にどの程度変更するかを決めるハイパーパラメーター** です。\n",
    "\n",
    "以下は、最小化を行おうとしているモデルと、学習率が与える影響を図示したものです。右上の点が初期値です。\n",
    "\n",
    "<img src=\"https://aidemyexcontentspic.blob.core.windows.net/contents-pic/5090_dnn/dnn_chap2_50.png\">\n",
    "\n",
    "1. 学習率が低すぎて、ほどんど更新が進んでいません。\n",
    "1. 適切な学習率のため、少ない回数で値が収束しています。\n",
    "1. 収束はしますが、値が大きいため、更新の仕方に無駄があります。\n",
    "1. 学習率が大きすぎて、値が発散してしまっています。（上側に更新されており、値がどんどん大きくなっています。） \n",
    "\n",
    "このように、 **損失関数に対して適切な学習率を設定する必要** があります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "- 次の3つの中から一番精度の出る学習率を予想し、以下のコードの一部を変更してください。\n",
    "- 学習率がモデルの学習に与える影響を確認してください。\n",
    "  - `funcA() lr: 0.01`\n",
    "  - `funcB() lr: 0.1`\n",
    "  - `funcC() lr: 1.0`\n",
    "- コードを二行コメントアウトし、他は変えないでください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "index"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from keras.models import Sequential, load_model\n",
    "from keras import optimizers\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 784)[:6000]\n",
    "X_test = X_test.reshape(X_test.shape[0], 784)[:1000]\n",
    "y_train = to_categorical(y_train)[:6000]\n",
    "y_test = to_categorical(y_test)[:1000]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=784))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "model.add(Dropout(rate=0.5))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "\n",
    "def funcA():\n",
    "    global lr\n",
    "    lr = 0.01\n",
    "\n",
    "def funcB():\n",
    "    global lr\n",
    "    lr = 0.1\n",
    "\n",
    "def funcC():\n",
    "    global lr\n",
    "    lr = 1.0\n",
    "\n",
    "# 二つコメントアウトして学習率を決めてください。\n",
    "#---------------------------\n",
    "funcA()\n",
    "funcB()\n",
    "funcC()\n",
    "#---------------------------\n",
    "\n",
    "sgd = optimizers.SGD(lr=lr)\n",
    "\n",
    "model.compile(optimizer=sgd, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=32, epochs=3, verbose=1)\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"evaluate loss: {0[0]}\\nevaluate acc: {0[1]}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- 全てのパターンを試してみましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "answer"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from keras.models import Sequential, load_model\n",
    "from keras import optimizers\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 784)[:6000]\n",
    "X_test = X_test.reshape(X_test.shape[0], 784)[:1000]\n",
    "y_train = to_categorical(y_train)[:6000]\n",
    "y_test = to_categorical(y_test)[:1000]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=784))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "model.add(Dropout(rate=0.5))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "\n",
    "def funcA():\n",
    "    global lr\n",
    "    lr = 0.01\n",
    "\n",
    "def funcB():\n",
    "    global lr\n",
    "    lr = 0.1\n",
    "\n",
    "def funcC():\n",
    "    global lr\n",
    "    lr = 1.0\n",
    "\n",
    "# 二つコメントアウトして学習率を決めてください。\n",
    "#---------------------------\n",
    "#funcA()\n",
    "funcB()\n",
    "#funcC()\n",
    "#---------------------------\n",
    "\n",
    "sgd = optimizers.SGD(lr=lr)\n",
    "\n",
    "model.compile(optimizer=sgd, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=32, epochs=3, verbose=1)\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"evaluate loss: {0[0]}\\nevaluate acc: {0[1]}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section_name",
    "sectionId": "B13IdRmWgZM"
   },
   "source": [
    "## 2.8 ミニバッチ学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5090,
    "exerciseId": "BkXPcnIoLxz",
    "id": "code_session_name",
    "important": true,
    "isDL": false,
    "timeoutSecs": 30
   },
   "source": [
    "### 2.8.1 ミニバッチ学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "モデルの学習を行う際、一度にモデルに渡す入力データの数は変えることができます。  \n",
    "一度に渡すデータの数を、 **バッチサイズ** といい、これもハイパーパラメータです。\n",
    "\n",
    "一度に複数のデータを渡した時、モデルはそれぞれのデータでの損失と損失関数の勾配（重みをどのように更新するべきか）を求めますが、  \n",
    "重みの更新は、１回のみ、求めた勾配の平均を使って行われます。\n",
    "\n",
    "**複数のデータを用いて重みの更新を行うことで、極端に変わったデータの影響をあまり受けずに済み、また並列計算が行えるので計算時間を短縮することができます。**  \n",
    "一方、複数のデータを用いて重みの更新を行うと、極端な重みの更新が発生しなくなり、損失関数の局所解から抜け出せなくなる恐れがあります。\n",
    "\n",
    "**癖の強いデータが多い時はバッチサイズを大きくする、同じようなデータが多いときはバッチサイズを小さくする** などと、\n",
    "バッチサイズをうまく調整する必要があります。  \n",
    "\n",
    "バッチサイズを1とする手法を**オンライン学習(確率的勾配法)**  \n",
    "\n",
    "バッチサイズを全データ数とする手法を**バッチ学習（最急降下法）**\n",
    "\n",
    "これらの中間となる手法を**ミニバッチ学習**と言います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "- 次の3つの中から一番精度の出るバッチサイズを予想し、以下のコードの一部を変更してください。\n",
    "- バッチサイズがモデルの学習に与える影響を確認してください。\n",
    "  - `funcA() batch_size: 16`\n",
    "  - `funcB() batch_size: 32`\n",
    "  - `funcC() batch_size: 64`\n",
    "- コードを二行コメントアウトし、他は変えないでください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "index"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from keras.models import Sequential, load_model\n",
    "from keras import optimizers\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 784)[:6000]\n",
    "X_test = X_test.reshape(X_test.shape[0], 784)[:1000]\n",
    "y_train = to_categorical(y_train)[:6000]\n",
    "y_test = to_categorical(y_test)[:1000]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=784))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "model.add(Dropout(rate=0.5))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.1)\n",
    "\n",
    "model.compile(optimizer=sgd, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "def funcA():\n",
    "    global batch_size\n",
    "    batch_size = 16\n",
    "\n",
    "def funcB():\n",
    "    global batch_size\n",
    "    batch_size = 32\n",
    "\n",
    "def funcC():\n",
    "    global batch_size\n",
    "    batch_size = 64\n",
    "\n",
    "# 二つコメントアウトしてbatch_sizeを決めてください。\n",
    "#---------------------------\n",
    "# batch_size: 16\n",
    "funcA()\n",
    "# batch_size: 32\n",
    "funcB()\n",
    "#batch_size: 64\n",
    "funcC()\n",
    "#---------------------------\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=3, verbose=1)\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"evaluate loss: {0[0]}\\nevaluate acc: {0[1]}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- 全てのパターン実行してみましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "answer"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from keras.models import Sequential, load_model\n",
    "from keras import optimizers\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 784)[:6000]\n",
    "X_test = X_test.reshape(X_test.shape[0], 784)[:1000]\n",
    "y_train = to_categorical(y_train)[:6000]\n",
    "y_test = to_categorical(y_test)[:1000]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=784))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "model.add(Dropout(rate=0.5))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.1)\n",
    "\n",
    "model.compile(optimizer=sgd, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "def funcA():\n",
    "    global batch_size\n",
    "    batch_size = 16\n",
    "\n",
    "def funcB():\n",
    "    global batch_size\n",
    "    batch_size = 32\n",
    "\n",
    "def funcC():\n",
    "    global batch_size\n",
    "    batch_size = 64\n",
    "\n",
    "# 二つコメントアウトしてbatch_sizeを決めてください。\n",
    "#---------------------------\n",
    "#funcA()\n",
    "#funcB()\n",
    "funcC()\n",
    "#---------------------------\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=3, verbose=1)\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"evaluate loss: {0[0]}\\nevaluate acc: {0[1]}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section_name",
    "sectionId": "BJpL_RXWlWG"
   },
   "source": [
    "## 2.9 反復学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5090,
    "exerciseId": "Sy4v9n8oUxz",
    "id": "code_session_name",
    "important": true,
    "isDL": false,
    "timeoutSecs": 40
   },
   "source": [
    "### 2.9.1 反復学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "一般に、モデルの精度をあげるため同じ訓練データを使って何度か学習させるということを行います。これを **反復学習** といいます。  \n",
    "\n",
    "この学習を行う回数を **エポック数** といい、これもハイパーパラメータです。\n",
    "\n",
    "エポック数は大きくすればモデルの精度が上がり続ける、というものではありません。  \n",
    "正解率は途中から伸びなくなるだけでなく、繰り返し学習をすることで損失関数を最小化させようとして過学習が起こります。  \n",
    "適切なタイミングで学習を打ち切ることが必要となってきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "- 次の3つの中から一番精度の出るエポック数を予想し、以下のコードの一部を変更してください。\n",
    "- エポック数がモデルの学習に与える影響を確認してください。\n",
    "  - `funcA() epochs: 5`\n",
    "  - `funcB() epochs: 10`\n",
    "  - `funcC() epochs: 60`\n",
    "- コードを二行コメントアウトし、他は変えないでください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "index",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from keras.models import Sequential, load_model\n",
    "from keras import optimizers\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 784)[:1500]\n",
    "X_test = X_test.reshape(X_test.shape[0], 784)[:6000]\n",
    "y_train = to_categorical(y_train)[:1500]\n",
    "y_test = to_categorical(y_test)[:6000]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=784))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "# 今回はDropoutを使いません。\n",
    "#model.add(Dropout(rate=0.5))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.1)\n",
    "\n",
    "model.compile(optimizer=sgd, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "def funcA():\n",
    "    global epochs\n",
    "    epochs = 5\n",
    "\n",
    "def funcB():\n",
    "    global epochs\n",
    "    epochs = 10\n",
    "\n",
    "def funcC():\n",
    "    global epochs\n",
    "    epochs = 60\n",
    "\n",
    "# 二つコメントアウトしてエポック数を決めてください。\n",
    "#---------------------------\n",
    "# epochs: 5\n",
    "funcA()\n",
    "# epochs: 10\n",
    "funcB()\n",
    "# epochs: 60\n",
    "funcC()\n",
    "#---------------------------\n",
    "\n",
    "history = model.fit(X_train, y_train, batch_size=32, epochs=epochs, verbose=1, validation_data=(X_test, y_test))\n",
    "\n",
    "#acc, val_accのプロット\n",
    "plt.plot(history.history[\"acc\"], label=\"acc\", ls=\"-\", marker=\"o\")\n",
    "plt.plot(history.history[\"val_acc\"], label=\"val_acc\", ls=\"-\", marker=\"x\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"evaluate loss: {0[0]}\\nevaluate acc: {0[1]}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- 全てのパターン実行してみましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "answer",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from keras.models import Sequential, load_model\n",
    "from keras import optimizers\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 784)[:1500]\n",
    "X_test = X_test.reshape(X_test.shape[0], 784)[:6000]\n",
    "y_train = to_categorical(y_train)[:1500]\n",
    "y_test = to_categorical(y_test)[:6000]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=784))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "# 今回はDropoutを使いません。\n",
    "#model.add(Dropout(rate=0.5))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.1)\n",
    "\n",
    "model.compile(optimizer=sgd, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "def funcA():\n",
    "    global epochs\n",
    "    epochs = 5\n",
    "\n",
    "def funcB():\n",
    "    global epochs\n",
    "    epochs = 10\n",
    "\n",
    "def funcC():\n",
    "    global epochs\n",
    "    epochs = 60\n",
    "\n",
    "# 二つコメントアウトしてエポック数を決めてください。\n",
    "#---------------------------\n",
    "#funcA()\n",
    "funcB()\n",
    "#funcC()\n",
    "#---------------------------\n",
    "\n",
    "history = model.fit(X_train, y_train, batch_size=32, epochs=epochs, verbose=1, validation_data=(X_test, y_test))\n",
    "\n",
    "#acc, val_accのプロット\n",
    "plt.plot(history.history[\"acc\"], label=\"acc\", ls=\"-\", marker=\"o\")\n",
    "plt.plot(history.history[\"val_acc\"], label=\"val_acc\", ls=\"-\", marker=\"x\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"evaluate loss: {0[0]}\\nevaluate acc: {0[1]}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "exerciseId": "sG2C2U7huf",
    "id": "movie_session_name",
    "important": false,
    "isDL": false,
    "timeoutSecs": 10
   },
   "source": [
    "### ディープラーニングのまとめ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "- このコースのまとめ\n",
    "- 次にオススメのコース"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "- 次の動画をみてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "movielink",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "https://www.youtube.com/embed/YWCbzfqt2GU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "chapter_exam"
   },
   "source": [
    "## 2.10 添削問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "chapter2でハイパーパラメータを一通り学習し、自分で適切なディープニューラルネットワークを構成、調整できるようになりました。  \n",
    "ここではハイパーパラメータのチューニングのみでMNIST分類の精度向上を行い、ハイパーパラメータをより深く理解していきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "- MNISTの分類モデルをディープニューラルネットワークで実装してください。条件は以下のようにします。\n",
    " - テストデータによる正解率は87%以上を出してください。\n",
    " - テストデータ、訓練データそれぞれによる正解率の差は1%未満になるようにしてください。\n",
    " - エポック数は5で固定とします。\n",
    " - X_train, y_train, X_test, y_test の定義文は変更しないでください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "index",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from keras.models import Sequential, load_model\n",
    "from keras import optimizers\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 784)[:6000]\n",
    "X_test = X_test.reshape(X_test.shape[0], 784)[:10000]\n",
    "y_train = to_categorical(y_train)[:6000]\n",
    "y_test = to_categorical(y_test)[:10000]\n",
    "\n",
    "#---------------------------\n",
    "#        ここを書いて下さい       \n",
    "#---------------------------\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"evaluate loss: {0[0]}\\nevaluate acc: {0[1]}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- 2.1.1のコードを参考にして構いません。コード中のハイパーパラメータを一つ変更するだけでも正解率87%を出すことができます。\n",
    "- 重みの初期化の仕様上正解率は毎回同じ値にはなりませんが、ほぼ毎回87%以上出る状態を目指してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "answer"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from keras.models import Sequential, load_model\n",
    "from keras import optimizers\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 784)[:6000]\n",
    "X_test = X_test.reshape(X_test.shape[0], 784)[:10000]\n",
    "y_train = to_categorical(y_train)[:6000]\n",
    "y_test = to_categorical(y_test)[:10000]\n",
    "\n",
    "#---------------------------\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=784))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "model.add(Dropout(rate=0.5))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.1)\n",
    "model.compile(optimizer=sgd, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=128, epochs=10, verbose=1)\n",
    "#---------------------------\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"evaluate loss: {0[0]}\\nevaluate acc: {0[1]}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "261.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}