{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "chapterId": "lBOxjHRhv",
    "id": "chapter_name"
   },
   "source": [
    "# 強化学習入門 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "table"
   },
   "source": [
    "- **[1.1 強化学習とは](#1.1-強化学習とは)**\n",
    "    - **[1.1.1 機械学習の分類](#1.1.1-機械学習の分類)**\n",
    "    - **[1.1.2 強化学習というタスク](#1.1.2-強化学習というタスク)**\n",
    "    - **[1.1.3 N腕バンディット問題](#1.1.3-N腕バンディット問題)**\n",
    "    - **[1.1.4 エージェントの作成](#1.1.4-エージェントの作成)**\n",
    "    - **[1.1.5 環境の作成](#1.1.5-環境の作成)**\n",
    "    - **[1.1.6 報酬の定義](#1.1.6-報酬の定義)**\n",
    "    - **[1.1.7 まとめ](#1.1.7-まとめ)**\n",
    "<br><br>\n",
    "- **[1.2 N腕バンディッド問題における方策](#1.2-N腕バンディッド問題における方策)**\n",
    "    - **[1.2.1 greedy手法](#1.2.1-greedy手法)**\n",
    "    - **[1.2.2 greedy手法](#1.2.2-greedy手法)**\n",
    "    - **[1.2.3 楽観的初期値法](#1.2.3-楽観的初期値法)**\n",
    "    - **[1.2.4 soft-max法](#1.2.4-soft-max法)**\n",
    "    - **[1.2.5 UCB1アルゴリズム](#1.2.5-UCB1アルゴリズム)**\n",
    "    - **[1.2.6 まとめ](#1.2.6-まとめ)**\n",
    "    - **[1.2.7 探索と利用のトレードオフ](#1.2.7-探索と利用のトレードオフ)**\n",
    "<br><br>\n",
    "- **[1.3 添削問題](#1.3-添削問題)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section_name",
    "sectionId": "P9zamBB2mn"
   },
   "source": [
    "## 強化学習とは"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "exerciseId": "Bhv_DJT289",
    "id": "quiz_session_name",
    "important": false,
    "isDL": false,
    "timeoutSecs": 10
   },
   "source": [
    "###  1.1.1機械学習の分類"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "機械学習は主に3つの分野に分かれます。\n",
    "\n",
    "- **<font color=#AA0000>教師あり学習</font>**<br>\n",
    "**正解ラベル付きのトレーニングデータからモデルを学習し、未知のデータに対して予想することを目的とします。** 教師あり学習は以下の２つに分類されます。\n",
    "\n",
    "    - 分類問題(コンテンツ:<a href=\"https://aidemy.net/courses/5010\" target=\"_blank\">[教師あり学習(分類)]</a>)<br>\n",
    "     カテゴリ別に分けてあるデータを学習し、未知のデータのカテゴリ(離散値)を予測します。\n",
    "     古典的な学習方法しては、SVMや決定木、ロジスティック回帰などがあり、deepな手法としては、VGG16などをファインチューニングした画像識別モデルなどがあります。\n",
    "       \n",
    "    - 回帰問題(コンテンツ:<a href=\"https://aidemy.net/courses/5010\" target=\"_blank\">[教師あり学習(回帰)]</a>)<br>\n",
    "      分類問題と違って、こちらは連続値を予測します。株価の予測などはこちらに分類されます。\n",
    "      \n",
    "\n",
    "- **<font color=#AA0000>教師なし学習</font>**(コンテンツ:<a href=\"https://aidemy.net/courses/5030\" target=\"_blank\">[教師なし学習]</a>)<br>\n",
    "**正解ラベルのないデータや構造が不明なデータに対し、データの構造や関係性を見出すことを目的とします。**  古典的なアルゴリズムとしては、主成分分析やk-means法などがあげられます。deepな手法としては、オートエンコーダや生成モデル、制約的ボルツマンマシンなどがあげられます。\n",
    "\n",
    "- **<font color=#AA0000>強化学習</font>**<br>\n",
    "**エージェントと環境が相互作用するような状況下で、もっとも最適な行動を発見することを目的とします。** 強化学習は、前提条件からゲームと相性が良いです。ゲームにおいていえば強化学習によって「勝ち方を発見すること」ができます。このコンテンツでは、この強化学習の基礎的な手法をコードを書きながら学びます。強化学習の理論部分だけを理解したいという場合は、実装クイズを飛ばして進めていくと良いでしょう。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "次のうち、正しく述べている文章を選択して下さい。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "choices"
   },
   "source": [
    "- 教師あり学習は、学習データに対して予測を行う。\n",
    "- 教師なし学習は、正解ラベル付きのデータを元に学習を行う。\n",
    "- 強化学習は、エージェントと環境が相互作用するような状況下で、エージェントのもっとも最適な行動を発見することである。\n",
    "- 上記の全て"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "文書を読んで、機械学習の３つの分野の意味を理解しましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "answer"
   },
   "source": [
    "- 強化学習は、エージェントと環境が相互作用するような状況下で、エージェントのもっとも最適な行動を発見することである。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "exerciseId": "il5bFUSeWF",
    "id": "quiz_session_name",
    "important": false,
    "isDL": false,
    "timeoutSecs": 10
   },
   "source": [
    "### 1.1.2強化学習というタスク"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "強化学習では、下の図のような**エージェント**と**環境**が相互作用する状況を前提に考えます。\n",
    "\n",
    "<img src=\"https://aidemyexstorage.blob.core.windows.net/aidemycontents/1535038969792547.png\">\n",
    "<center>図1.1.2 強化学習が前提としている状況</center>\n",
    "\n",
    "ある**エージェント**が、**状態**$s$にあるとして、環境に対して**行動**$a$を取ります。そうすると、**環境**はその行動の評価として**報酬**$R$を返し、エージェントは次の状態$s'$へと移ります。これを繰り返し**行動を強化しながら**タスクを進めます。\n",
    "\n",
    "強化学習において、上図では報酬(**即時報酬**)のみ最大化しようとしていますが、本来は、即時報酬だけでなく、そのあとに得られる報酬(**遅延報酬**)も含めた「**収益**」を最大化することが必要になります。<br>\n",
    "例えば、株の売買により利益(または損失)を得る問題は強化学習にあたります。ここでの報酬は、売買する度の利益をさします。持っている株をすべて売り出せば確かにその時点では最もキャッシュを得ることができますが、より長期的な意味での価値を最大化するには、株をもう少し手元に置いておいたほうが良いかもしれません。このように、「未来の報酬の総和を最大化するための行動を選択する」ことを目的とします。\n",
    "\n",
    "強化学習の例:<br>\n",
    "・ブロック崩しでスコアをあげる<br>\n",
    "・ビットコインを取引して増やす<br>\n",
    "・安全に駐車をする<br>\n",
    "・ロボットを歩かせる<br>\n",
    "など<br>\n",
    "\n",
    "つまり、**本質的な将来の価値を最大化することを目的とした問題は「強化学習に適したタスク」** と言えます。<br>\n",
    "\n",
    "強化学習が今注目を浴びている理由は、それぞれの行動に対して評価することが難しい場合(不確実性のある環境)において真価を発揮するためであり、ゲームやロボットの自動制御、ファイナンス等で利用されています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "次の選択肢から強化学習を用いるべき事例を選んでください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "choices"
   },
   "source": [
    "- 手書きの数字と実際の数字が結びつけられたデータで学習し、手書きの数字を見せられた時にそれがどの数字を表しているのかを出力する。\n",
    "- 複数のマイクで取得された音声データで学習し、音の発信源ごとにクラスタリングを行う。\n",
    "- ロボットアームで物を掴むという環境下で試行錯誤したデータから、最適なアームの制御法を獲得する。\n",
    "- データから特徴を類推する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- 強化学習は行動に対して評価をすることが難しいものに適しています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "answer"
   },
   "source": [
    "- ロボットアームで物を掴むという環境下で試行錯誤したデータから、最適なアームの制御法を獲得する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "exerciseId": "K0LEgCVL6W",
    "id": "quiz_session_name",
    "important": false,
    "isDL": false,
    "timeoutSecs": 10
   },
   "source": [
    "### 1.1.3 N腕バンディット問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "N腕バンディット問題(多腕バンディット問題)とは、強化学習のもっとも簡単な理論を理解するのに適した環境と、\n",
    "ここからは例題として **<font color=#AA0000>N腕バンディッド問題</font>** という問題を扱いながら強化学習についての理解を深めていきましょう。\n",
    "\n",
    "**N腕バンディッド問題** とは、  \n",
    "\n",
    "* 事前に確率の定義されたスロットマシーンがN台ならんでいる。(ユーザーはどのスロットマシーンがどのくらい当たるかは事前に知らされていない)  \n",
    "\n",
    "* 一度スロットマシーンを引くと、それぞれ事前に設定されていた確率に基づき、当たりならば1、外れならば0  \n",
    "という形で報酬が支払われる。(簡単化のため、今回はスロットの当たりの確率は変わらない)  \n",
    "\n",
    "* ユーザーは1回の試行につきどれか1つのスロットマシーンを引くことができる。  \n",
    "\n",
    "\n",
    "<img src=\"https://aidemyexstorage.blob.core.windows.net/aidemycontents/1537893450430895.png\">\n",
    "<center>図1.1.3 N腕バンディット問題</center>\n",
    "\n",
    "このような環境下で **試行回数あたりの平均報酬量を最大化する** ためにはどのようにするべきかを考える問題の事です。当然、一番確率の大きいスロットマシーンを引き続けるのが理にかなっていますが、エージェント自身はそれぞれの内部の確率を知らないために、実際に引くことによって得た報酬量からそれぞれの確率を推測しなければならないことが重要な点となっています。\n",
    "\n",
    "\n",
    "ここから、この**N腕バンディッド問題**を題材に実装していきながら、強化学習について勉強していきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "- 強化学習において、次の選択肢から正しいものを選んでください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "choices"
   },
   "source": [
    "- 報酬を最大にするために、あたりの出る確率の高いと思われるスロットのみを引いたほうがいい。\n",
    "- 確率がわからないのですべてのマシンを均等に引いた。\n",
    "- 確率が明らかに低そうなマシンはあまり引くべきではない。\n",
    "- すべて誤りである。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- 強化学習では利益を出すためにいい確率のものを多くひくことはもちろん大切ですが、確率のいいものを探すために試行することも大切であり、極端に片方に偏るモデルはいいモデルとは言えません。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "answer"
   },
   "source": [
    "- 確率が明らかに低そうなマシンはあまり引くべきではない。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "exerciseId": "pZxNNjCx_0",
    "id": "code_session_name",
    "important": false,
    "isDL": false,
    "timeoutSecs": 10
   },
   "source": [
    "### 1.1.4 エージェントの作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    " **<font color=#AA0000>エージェント</font>** とは、**環境の中で行動を決定し、環境に対して影響を与えるもの**を指しています。N腕バンディット問題では、どのスロットマシーンを使用するかを**判断**し**報酬を受け取り**、**次の判断をする**ユーザーが **<font color=#AA0000>エージェント</font>** に該当します。<br>\n",
    " \n",
    " 取得した報酬から、どのようなアルゴリズムに基づいて次の腕を決めるかという指標のことを **<font color=#AA0000>方策</font>** と言います。例えばN腕バンディッド問題において方策が「常に0」なのであれば、エージェントは常に0の腕を引き続けることになります。\n",
    " \n",
    "この **エージェント** の最適な **<font color=#AA0000>方策</font>** を決定するのかが、この章の目標です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "- 5個のスロットマシーンの中から次に引くマシーンを**ランダムに**選択するエージェントを実装して下さい。\n",
    "- 0〜4の数字をランダムに返す関数を実装しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "index"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# 再現性を出すためにseedを設定します\n",
    "random.seed(0)\n",
    "\n",
    "# 関数random_selectを作ってください\n",
    "def random_select():\n",
    "    slot_num = \n",
    "    return slot_num\n",
    "\n",
    "\n",
    "# 関数の呼び出し\n",
    "slot_num = random_select()\n",
    "print(slot_num)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- `random.randint()`関数などを使用しましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "answer"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# 再現性を出すためにseedを設定します\n",
    "random.seed(0)\n",
    "\n",
    "# 関数random_selectを作ってください\n",
    "def random_select():\n",
    "    slot_num = np.random.randint(0, 5)\n",
    "    return slot_num\n",
    "\n",
    "\n",
    "# 関数の呼び出し\n",
    "slot_num = random_select()\n",
    "print(slot_num)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "exerciseId": "gRxlsTnQQV",
    "id": "code_session_name",
    "important": false,
    "isDL": false,
    "timeoutSecs": 10
   },
   "source": [
    "### 1.1.5 環境の作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    " **<font color=#AA0000>環境</font>** とは、エージェントが行動をおこす対象です。エージェントの行動を受け、状況を観測し、報酬をエージェントに送信し、時間を1つ進めるという役割があります。\n",
    " \n",
    "N腕バンディッド問題においては、エージェントがあるスロットマシーンを引いた時、そのスロットの確率によって当たりか外れかを出すプロセスに該当します。\n",
    "\n",
    "NumPyの`numpy.random.binomial(n, p)`で、試行回数n、確率pの二項分布を求めることが出来ます。\n",
    "```python\n",
    "# 二項分布の計算（試行回数：1000, 確率：0.5, サンプル数：5）\n",
    "x = np.random.binomial(1000, 0.5, 5)\n",
    "print(x)\n",
    "出力:\n",
    "[489 514 497 499 519]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "N腕バンディッド問題の**環境**を関数で実装して下さい。   \n",
    "スロットマシーンは全部で5台あり、それぞれ当たりが出る確率は0.3, 0.4, 0.5, 0.6, 0.7とします。  \n",
    "引数は、引くスロットマシーンの番号(0,1,2,3,4)とします。    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "index"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# 関数environmentsを作ってください\n",
    "def environments(band_number):\n",
    "    # 各マシンの確率をNumPy配列にしてください\n",
    "    coins_p = \n",
    "    \n",
    "    # それぞれの確率で結果(0か1)を求めてください\n",
    "    results = \n",
    "    \n",
    "    #腕の番号がband_numberの結果を出力してください\n",
    "    result = \n",
    "    \n",
    "    return result\n",
    "\n",
    "# 関数の呼び出し\n",
    "band_number = 1\n",
    "result = environments(band_number)\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- Numpy配列にするには、`np.array()`を使います。\n",
    "- `result` には指定したスロットが当たりか外れか(1 か 0)を返してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "answer"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# 関数environmentsを作ってください\n",
    "def environments(band_number):\n",
    "    # 各マシンの確率をNumPy配列にしてください\n",
    "    coins_p = np.array([0.3, 0.4, 0.5, 0.6, 0.7])\n",
    "    \n",
    "    # それぞれの確率で結果(0か1)を求めてください\n",
    "    results = np.random.binomial(1, coins_p)\n",
    "    \n",
    "    #腕の番号がband_numberの結果を出力してください\n",
    "    result = results[band_number]\n",
    "    \n",
    "    return result\n",
    "\n",
    "# 関数の呼び出し\n",
    "band_number = 1\n",
    "result = environments(band_number)\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "exerciseId": "EEDax6Ct4x",
    "id": "code_session_name",
    "important": false,
    "isDL": false,
    "timeoutSecs": 10
   },
   "source": [
    "### 1.1.6 報酬の定義"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    " **<font color=#AA0000>報酬(reward)</font>** とは、環境からエージェントに与えられる信号のことで、**エージェントの一連の行動の望ましさを評価する指標**の事です。 N腕バンディッド問題において言えば**スロットマシーンから得られた返り値**がそのまま該当します。また、引いた直後(行動の直後)に得られた報酬を**即時報酬**と言います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "- 先ほどの環境関数を使用し、time番目を実行したN腕バンディッド問題の報酬関数`reward()`を実装して下さい。\n",
    "    - 引数`record`は    \n",
    "あらかじめ確保された総試行回数分のNumPy配列で、i回目の試行で得られた報酬の結果を配列中のi番目の位置に格納します。\n",
    "<br><br>\n",
    "    - 引数`results`は、    \n",
    "`[[腕の番号, 選択回数, 報酬合計, 報酬合計 / 選択回数], [腕の番号, 選択回数, 報酬合計, 報酬合計 / 選択回数], [], []...]`  \n",
    "の形の二重リスト構造です。\n",
    "<br><br>\n",
    "    - 引数`slot_num`は  \n",
    "今回の試行で選択するスロットマシーンの番号(0,1,2,3,4のどれか)を示しています。  \n",
    "<br><br>\n",
    "    - 引数`time`は  \n",
    "現在行なっている試行が何回目かを示す変数です。   \n",
    "問題ではtime = 1000としているので、1000回目の試行をしたという意味になります。   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "index"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# 環境の定義\n",
    "def environments(band_number):\n",
    "    coins_p = np.array([0.3, 0.4, 0.5, 0.6, 0.7])\n",
    "    results = np.random.binomial(1, coins_p)\n",
    "    result = results[band_number]\n",
    "    return result\n",
    "\n",
    "# 関数rewardを作ってください\n",
    "def reward(record, results, slot_num, time):\n",
    "    # environments関数を用いて結果を取得してください\n",
    "    result = \n",
    "    \n",
    "    # recordのtime番目の値をresultにしてください\n",
    "    \n",
    "    \n",
    "    # resultsの各値を更新してください\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return results, record\n",
    "\n",
    "# 各変数の定義\n",
    "times = 1000\n",
    "results = [[0, 0, 0, 0], [1, 0, 0, 0], [2, 0, 0, 0], [3, 0, 0, 0], [4, 0, 0, 0]]\n",
    "record = np.zeros(times)\n",
    "slot_num = 1\n",
    "time = 1000\n",
    "\n",
    "# 関数の呼び出し\n",
    "results, record = reward(record, results, slot_num, time)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- 引数slot_numのスロットを引き、recordのtime番目の値と、resultsの各値を更新する関数を作りましょう。\n",
    "- 1000番目の結果を反映させたresult,recordを更新してください。\n",
    "- 1000回ではなく1000番目の結果であることに注意してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "answer"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# 環境の定義\n",
    "def environments(band_number):\n",
    "    coins_p = np.array([0.3, 0.4, 0.5, 0.6, 0.7])\n",
    "    results = np.random.binomial(1, coins_p)\n",
    "    result = results[band_number]\n",
    "    return result\n",
    "\n",
    "# 関数rewardを作ってください\n",
    "def reward(record, results, slot_num, time):\n",
    "    # environments関数を用いて結果を取得してください\n",
    "    result = environments(slot_num)\n",
    "    \n",
    "    # recordのtime番目の値をresultにしてください\n",
    "    record[time-1] = result\n",
    "    \n",
    "    # resultsの各値を更新してください\n",
    "    results[slot_num][1] += 1\n",
    "    results[slot_num][2] += result\n",
    "    results[slot_num][3] = results[slot_num][2] / results[slot_num][1]\n",
    "    \n",
    "    return results, record\n",
    "\n",
    "# 各変数の定義\n",
    "times = 1000\n",
    "results = [[0, 0, 0, 0], [1, 0, 0, 0], [2, 0, 0, 0], [3, 0, 0, 0], [4, 0, 0, 0]]\n",
    "record = np.zeros(times)\n",
    "slot_num = 1\n",
    "time = 1000\n",
    "\n",
    "# 関数の呼び出し\n",
    "results, record = reward(record, results, slot_num, time)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "exerciseId": "5pYA9mbxOo",
    "id": "code_session_name",
    "important": false,
    "isDL": false,
    "timeoutSecs": 10
   },
   "source": [
    "### 2.1.7 まとめ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "以上ここまで\n",
    "* 環境\n",
    "* エージェント\n",
    "* 報酬\n",
    "\n",
    "という3つの言葉が出てきました。  \n",
    "この3つの要素を数式で表現し、最適化していくことが強化学習の最も大切なフローです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "以下の条件でコードを追加し、平均報酬の推移をプロットして下さい。 \n",
    "* 方策は毎回ランダムに選ぶものとします。\n",
    "* 引く回数は100回とします。\n",
    "* 腕の数は5本とします。\n",
    "* それぞれの腕において当たりの出る確率は`np.array([0.3, 0.4, 0.5, 0.6, 0.7])`で定義されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "index"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# 手法を定義する関数です\n",
    "def randomselect():\n",
    "    slot_num = np.random.randint(0, 4)\n",
    "    return slot_num\n",
    "\n",
    "# 環境を定義する関数です\n",
    "def environments(band_number):\n",
    "    coins_p = np.array([0.3, 0.4, 0.5, 0.6, 0.7])\n",
    "    results = np.random.binomial(1, coins_p)\n",
    "    result = results[band_number]\n",
    "    return result\n",
    "\n",
    "# 報酬を定義する関数です\n",
    "def reward(record, results, slot_num, time):\n",
    "    result = environments(slot_num)\n",
    "    record[time] = result\n",
    "    results[slot_num][1] += 1\n",
    "    results[slot_num][2] += result\n",
    "    results[slot_num][3] = results[slot_num][2] / results[slot_num][1]\n",
    "    return results, record\n",
    "\n",
    "    \n",
    "# 初期変数を設定しています\n",
    "times = 10000\n",
    "results = [[0, 0, 0, 0], [1, 0, 0, 0], [2, 0, 0, 0], [3, 0, 0, 0], [4, 0, 0, 0]]\n",
    "record = np.zeros(times)\n",
    "\n",
    "# slot_numを取得して、results,recordを書き換えてください\n",
    "for time in range(0, times):\n",
    "    slot_num = \n",
    "    results, record = \n",
    "\n",
    "\n",
    "\n",
    "# 各マシーンの試行回数と結果を出力しています\n",
    "print(results)\n",
    "\n",
    "# recordを用いて平均報酬の推移をプロットしてください\n",
    "\n",
    "\n",
    "# 表を出力しています\n",
    "plt.xlabel(\"試行回数\")\n",
    "plt.ylabel(\"平均報酬\")\n",
    "plt.title(\"試行回数と平均報酬の推移\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- `np.cumsum`関数によって配列の累積和を出すことができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####   解答例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "answer"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# 手法を定義する関数です\n",
    "def randomselect():\n",
    "    slot_num = random.randint(0, 4)\n",
    "    return slot_num\n",
    "\n",
    "# 環境を定義する関数です\n",
    "def environments(band_number):\n",
    "    coins_p = np.array([0.3, 0.4, 0.5, 0.6, 0.7])\n",
    "    results = np.random.binomial(1, coins_p)\n",
    "    result = results[band_number]\n",
    "    return result\n",
    "\n",
    "# 報酬を定義する関数です\n",
    "def reward(record, results, slot_num, time):\n",
    "    result = environments(slot_num)\n",
    "    record[time] = result\n",
    "    results[slot_num][1] += 1\n",
    "    results[slot_num][2] += result\n",
    "    results[slot_num][3] = results[slot_num][2] / results[slot_num][1]\n",
    "    return results, record\n",
    "\n",
    "\n",
    "# 初期変数を設定しています\n",
    "times = 10000\n",
    "results = [[0, 0, 0, 0], [1, 0, 0, 0], [2, 0, 0, 0], [3, 0, 0, 0], [4, 0, 0, 0]]\n",
    "record = np.zeros(times)\n",
    "\n",
    "# slot_numを取得して、results,recordを書き換えてください\n",
    "for time in range(0, times):\n",
    "    slot_num = randomselect()\n",
    "    results, record = reward(record, results, slot_num, time)\n",
    "\n",
    "# 各マシーンの試行回数と結果を出力しています\n",
    "print(results)\n",
    "\n",
    "# recordを用いて平均報酬の推移をプロットしてください\n",
    "plt.plot(np.cumsum(record) / np.arange(1, record.size + 1))\n",
    "\n",
    "# 表を出力しています\n",
    "plt.xlabel(\"試行回数\")\n",
    "plt.ylabel(\"平均報酬\")\n",
    "plt.title(\"試行回数と平均報酬の推移\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section_name",
    "sectionId": "S1PheyMQ8M"
   },
   "source": [
    "## 1.2 N腕バンディッド問題における方策"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "exerciseId": "J02GwG5yJY",
    "id": "code_session_name",
    "important": false,
    "isDL": false,
    "timeoutSecs": 10
   },
   "source": [
    "### 1.2.1 greedy手法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "ここでは、N腕バンディッド問題をどのような**方策**で進めていくのが得策なのか議論しながら代表的な手法を紹介していきます。また実際に各手法を実装していき、それぞれの特徴について理解を深めましょう。\n",
    "\n",
    "基本的な方針としては、最初のうちは確率に対する情報が全くないため、ランダムに腕を選ぶしかありません。試行を重ねるにつれて腕の成功率に関する情報が溜まってきます。そこでもっとも確率の高い腕を探し、その腕を選び続ける(**利用**と言います)選択を取ることとします。しかし試行回数が少ないうちは確率の偏りがあるため、一見成功率の低く見える腕も選択する(**探索**をする)必要があります。\n",
    " \n",
    "最も単純な手法として、 **<font color=#AA0000>greedy手法</font>** という手法があります。基本的なアルゴリズムとしては、**これまでの結果から最も期待値の大きいスロットマシーンを選択する**というものになります。もちろんこれだけでは何も情報がない状態では判断することができないので、最初にある程度**探索**してデータを集める必要があります。N腕バンディッド問題でいえば各マシーンをn回動かすことが必要になります。\n",
    "\n",
    "ここで実装するgreedyアルゴリズムを以下のように定義します。\n",
    "\n",
    "1. まだn回選んだことのないスロットマシーンがある場合、それを選択する\n",
    "2. 全てのマシンをn回選択した場合、全てのマシンに対してこれまでの報酬の期待値$u_{i}$を計算し、一番大きいものを選択する  \n",
    "$u_{i}$ = これまでのマシンiの報酬の和 / これまでのマシンiのプレイ回数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "- 現在の状況`result`、制限回数 ` n ` を引数として、次に引く腕を選ぶgreedy手法を実装して下さい。\n",
    "- 引数`results`は、    \n",
    "`[[腕の番号, 選択回数, 報酬合計, 報酬合計 / 選択回数], [腕の番号, 選択回数, 報酬合計, 報酬合計 / 選択回数], [], []...]`  \n",
    "の形の二重リスト構造です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "index"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "# greedyアルゴリズムを実装して下さい\n",
    "def greedy(results, n):\n",
    "    # 試行回数がnより少ないマシンがある場合、slot_numをそのマシンの腕番号にする \n",
    "    # 以下のfor文内を埋めてください\n",
    "    slot_num = None\n",
    "    for i, d in enumerate(results):\n",
    "        \n",
    "        \n",
    "        \n",
    "    # どのマシンの試行回数もnより大きい場合、slot_numを報酬の期待値の高いものにする\n",
    "    # 以下のif分内を埋めてください\n",
    "    if slot_num == None:\n",
    "        slot_num = \n",
    "        \n",
    "    return slot_num\n",
    "\n",
    "# 環境を定義する関数です\n",
    "def environments(band_number):\n",
    "    coins_p = np.array([0.3, 0.4, 0.5, 0.6, 0.7])\n",
    "    results = np.random.binomial(1, coins_p)\n",
    "    result = results[band_number]\n",
    "    return result\n",
    "\n",
    "# 報酬を定義する関数です\n",
    "def reward(record, results, slot_num, time):\n",
    "    result = environments(slot_num)\n",
    "    record[time] = result\n",
    "    results[slot_num][1] += 1\n",
    "    results[slot_num][2] += result\n",
    "    results[slot_num][3] = results[slot_num][2] / results[slot_num][1]\n",
    "    return results, record\n",
    "\n",
    "\n",
    "# 初期変数を設定して下さい\n",
    "times = 10000\n",
    "results = [[0, 0, 0, 0], [1, 0, 0, 0], [2, 0, 0, 0], [3, 0, 0, 0], [4, 0, 0, 0]]\n",
    "record = np.zeros(times)\n",
    "n = 100\n",
    "\n",
    "for time in range(0, times):\n",
    "    slot_num = greedy(results,n)\n",
    "    results, record = reward(record, results, slot_num, time)\n",
    "    \n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- n回施行されていないマシンはif文で分岐して施行しましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "answer"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "# greedyアルゴリズムを実装して下さい\n",
    "def greedy(results, n):\n",
    "    # 試行回数がnより少ない場合、slot_numはそのままにする\n",
    "    # 以下のfor文内を埋めてください\n",
    "    slot_num = None\n",
    "    for i, d in enumerate(results):\n",
    "        if d[1] < n:\n",
    "            slot_num = i\n",
    "            break\n",
    "        \n",
    "    # 試行回数がnより大きい場合、slot_numを報酬の期待値の高いものにする\n",
    "    # 以下のif分内を埋めてください\n",
    "    if slot_num == None:\n",
    "        slot_num = np.array([row[3] for row in results]).argmax()\n",
    "        \n",
    "    return slot_num\n",
    "\n",
    "# 環境を定義する関数です\n",
    "def environments(band_number):\n",
    "    coins_p = np.array([0.3, 0.4, 0.5, 0.6, 0.7])\n",
    "    results = np.random.binomial(1, coins_p)\n",
    "    result = results[band_number]\n",
    "    return result\n",
    "\n",
    "# 報酬を定義する関数です\n",
    "def reward(record, results, slot_num, time):\n",
    "    result = environments(slot_num)\n",
    "    record[time] = result\n",
    "    results[slot_num][1] += 1\n",
    "    results[slot_num][2] += result\n",
    "    results[slot_num][3] = results[slot_num][2] / results[slot_num][1]\n",
    "    return results, record\n",
    "\n",
    "# 初期変数を設定して下さい\n",
    "times = 10000\n",
    "results = [[0, 0, 0, 0], [1, 0, 0, 0], [2, 0, 0, 0], [3, 0, 0, 0], [4, 0, 0, 0]]\n",
    "record = np.zeros(times)\n",
    "n = 100\n",
    "\n",
    "for time in range(0, times):\n",
    "    slot_num = greedy(results,n)\n",
    "    results, record = reward(record, results, slot_num, time)\n",
    "    \n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "exerciseId": "olTXLzEc4h",
    "id": "code_session_name",
    "important": false,
    "isDL": false,
    "timeoutSecs": 10
   },
   "source": [
    "### 1.2.2 ε-greedy手法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "先ほどの問題について考えてみましょう。  \n",
    "nが少なくなればなるほど、greedy手法は間違ったマシン(確率が最大でないマシン)を選択する可能性が高くなります。かといってnを1000回にしても、高い精度で最適なスロットを決定できますが、それには5000回もの間最適でない手法でマシンを選択していることになりその分損をしていることがわかります。\n",
    "\n",
    "このジレンマの解決方法としてよく知られた手法の一つに **<font color=#AA0000>ε-greedy手法</font>**(イプシロン-greedy手法)があります。**ε-greedy手法**とは利用と探索を**織り交ぜる**ことによって探索コストを減らしつつ、間違ったマシンを選択し続けるリスクを減らすことのできるgreedy手法の改善手法です。\n",
    "\n",
    "**ε-greedy手法**を以下のように定義します。\n",
    " 1. まだ選択したことがないマシンがある場合、それを選択する\n",
    " 2. 確率εで全てのマシンの中からランダムに選択する(探索)\n",
    " 3. 確率1-εでこれまでの報酬の平均が最大のマシンを選択する(利用)\n",
    "\n",
    "εは我々が決める値です。(εは0より大きく、1以下の数です。)  \n",
    "`ε = 0` でgreedy手法と等価になります  \n",
    "このようにすることで、見積もりが不確かな場合でも期待値が高いマシンに選択を集中させることで無駄な探索コストを減らすと同時に、いつかは全ての腕が一定回数探索されるので間違えたマシンを選択するリスクも減らすことができます。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "ε-greedy手法を実装して下さい。\n",
    "εの値を`0.01 0.1 0.2` と変化させてその収束の度合いをプロットしています。グラフをみて、εによる収束の違いを確認してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "index"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# ε-greedy手法を実装して下さい\n",
    "def epsilon_greedy(results,epsilon):\n",
    "    # 確率εで全てのマシンの中からランダムに選択してください(手順2)\n",
    "    if np.random.binomial(1, epsilon):\n",
    "        slot_num = \n",
    "        \n",
    "    # 確率1-εでn=0のgreedy手法をとってください(手順1,3)\n",
    "    else:\n",
    "        slot_num =\n",
    "        for i, d in enumerate(results):\n",
    "            if d[1] == 0:\n",
    "                slot_num = \n",
    "                break\n",
    "\n",
    "        if slot_num == None:\n",
    "            slot_num = \n",
    "            \n",
    "    return slot_num\n",
    "\n",
    "# 環境を定義する関数です\n",
    "def environments(band_number):\n",
    "    coins_p = np.array([0.3, 0.4, 0.5, 0.6, 0.7])\n",
    "    results = np.random.binomial(1, coins_p)\n",
    "    result = results[band_number]\n",
    "    return result\n",
    "\n",
    "# 報酬を定義する関数です\n",
    "def reward(record, results, slot_num, time):\n",
    "    result = environments(slot_num)\n",
    "    record[time] = result\n",
    "    results[slot_num][1] += 1\n",
    "    results[slot_num][2] += result\n",
    "    results[slot_num][3] = results[slot_num][2] / results[slot_num][1]\n",
    "    return results, record\n",
    "\n",
    "    \n",
    "# 初期変数を設定しています\n",
    "times = 10000\n",
    "results = [[0, 0, 0, 0], [1, 0, 0, 0], [2, 0, 0, 0], [3, 0, 0, 0], [4, 0, 0, 0]]\n",
    "record = np.zeros(times)\n",
    "epsilons = [0.01, 0.1, 0.2]\n",
    "\n",
    "for epsilon in epsilons:\n",
    "    for time in range(0, times):\n",
    "        slot_num = epsilon_greedy(results, epsilon)\n",
    "        results, record = reward(record, results, slot_num, time)\n",
    "\n",
    "    #epsilon を変更させて収束の度合いをプロットしています\n",
    "    plt.plot(np.cumsum(record) / np.arange(1, record.size + 1), label =\"epsilon = %s\" %str(epsilon))\n",
    "    \n",
    "\n",
    "#グラフと結果の出力をしています        \n",
    "plt.legend()\n",
    "plt.xlabel(\"試行回数\")\n",
    "plt.ylabel(\"平均報酬\")\n",
    "plt.title(\"ε-greedyのεによる収束の違い\")\n",
    "plt.show()\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- アルゴリズムの手順1と手順3の内容はgreedy手法のものと全く同一です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "answer"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# ε-greedy手法を実装して下さい\n",
    "def epsilon_greedy(results,epsilon):\n",
    "    # 確率εで全てのマシンの中からランダムに選択してください(手順2)\n",
    "    if np.random.binomial(1, epsilon):\n",
    "        slot_num = random.randint(0, 4)\n",
    "        \n",
    "    # 確率1-εでn=0のgreedy手法をとってください(手順1, 3)\n",
    "    else:\n",
    "        slot_num = None\n",
    "        for i, d in enumerate(results):\n",
    "            if d[1] == 0:\n",
    "                slot_num = i\n",
    "                break\n",
    "\n",
    "        if slot_num == None:\n",
    "            slot_num = np.array([row[3] for row in results]).argmax()\n",
    "            \n",
    "    return slot_num\n",
    "\n",
    "# 環境を定義する関数です\n",
    "def environments(band_number):\n",
    "    coins_p = np.array([0.3, 0.4, 0.5, 0.6, 0.7])\n",
    "    results = np.random.binomial(1, coins_p)\n",
    "    result = results[band_number]\n",
    "    return result\n",
    "\n",
    "# 報酬を定義する関数です\n",
    "def reward(record, results, slot_num, time):\n",
    "    result = environments(slot_num)\n",
    "    record[time] = result\n",
    "    results[slot_num][1] += 1\n",
    "    results[slot_num][2] += result\n",
    "    results[slot_num][3] = results[slot_num][2] / results[slot_num][1]\n",
    "    return results, record\n",
    "\n",
    "\n",
    "    \n",
    "# 初期変数を設定しています\n",
    "times = 10000\n",
    "results = [[0, 0, 0, 0], [1, 0, 0, 0], [2, 0, 0, 0], [3, 0, 0, 0], [4, 0, 0, 0]]\n",
    "record = np.zeros(times)\n",
    "epsilons = [0.01, 0.1, 0.2]\n",
    "for epsilon in epsilons:\n",
    "    for time in range(0, times):\n",
    "        slot_num = epsilon_greedy(results,epsilon)\n",
    "        results, record = reward(record, results, slot_num, time)\n",
    "        \n",
    "    #epsilon を変更させて収束の度合いをプロットしています\n",
    "    plt.plot(np.cumsum(record) / np.arange(1, record.size + 1), label =\"epsilon = %s\" %str(epsilon))\n",
    "\n",
    "#グラフと結果の出力をしています\n",
    "plt.legend()\n",
    "plt.xlabel(\"試行回数\")\n",
    "plt.ylabel(\"平均報酬\")\n",
    "plt.title(\"ε-greedyのεによる収束の違い\")\n",
    "plt.show()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "exerciseId": "InkKz9QD57",
    "id": "code_session_name",
    "important": false,
    "isDL": false,
    "timeoutSecs": 10
   },
   "source": [
    "### 1.2.3 楽観的初期値法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "ここでgreedy手法のリスクについて考えてみましょう。\n",
    "\n",
    "真の期待値がそれぞれ0.4、0.6である対象A、Bがあるとします。\n",
    "\n",
    "**真の期待値が小さいものを、誤って期待値が大きいと予測した場合**（Aの期待値を0.8のように予測した時）は回数を重ねていくにつれ真の期待値に収束していきます。つまり、期待値が高いと予測したAを選択し続けることにより、Aの期待値の予想は0.4へと収束していくので、十分な回数の試行によって最終的にはBを選択するようになるのです。\n",
    "\n",
    "しかし問題になるのは**真の期待値が大きいものを、誤って期待値が小さいと予測した場合**（Bの期待値を0.2のように予測したとき）です。このときは期待値が低いと予測したBを選択することはなく、最後まで期待値がより高い（0.4）と勘違いしたAだけを選び続けてしまうのです。\n",
    "\n",
    "N腕バンディッド問題においていうならば、全ての腕をn回試行した段階で、真の確率が最も高い腕を選択できない可能性があるということです。\n",
    "\n",
    "このような場合にはgreedy手法では修正が非常に困難であり、(真の期待値が0.4の方を選択し試行を続けるため、真の期待値が0.6の方が選択されないので修正するためのデータが蓄積せず、間違いに気づくことが困難です。\n",
    "\n",
    "このようなリスクを減らすための原理として、「 **<font color=#AA0000>不確かな時は楽観的に</font>** 」という原理が知られています。つまりデータが蓄積されておらず、予測した期待値に不確実性があるときにはその期待値を**大きく見積もる**というものです。  \n",
    "\n",
    "**楽観的初期値法**(optimistic initial rewards)はこのような発想をもとに、学習前に各腕から報酬の最大値をK回得ていたことを仮定する事で、不確かな状態での期待値を大きく見積もり、そこからgreedy法を適用する手法となります。具体的には、K=5だとすると、全てのマシンで1(当たり)をすでに5回引いているとしてから期待値を計算し始めることになります。<br>\n",
    "\n",
    "楽観的初期値法のアルゴリズムは以下のように定義します。\n",
    "\n",
    "1. 報酬の上界を$r_{sup}$で定義します。(スロット問題においては1)\n",
    "2. すでに各マシンでK回上界が観測され、データとして蓄積しているとします。\n",
    "3. この状態からgreedy法を始めます。\n",
    "\n",
    "上界とは、とりうる最大値のことです。   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "楽観的初期値法を実装して下さい。\n",
    "変数の定義として\n",
    "* $r_{sup}$は報酬の有界\n",
    "* Kはすでに上界を観測した回数\n",
    "* R(N)は実際に測定した報酬   \n",
    "* Nは実際に測定した回数\n",
    "* $\\mu$(optimistic_mean)は報酬の期待値\n",
    "\n",
    "$\\mu=\\frac{R(N)+Kr_{sup}}{N + K}$\n",
    "\n",
    "であるものとします。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "index"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(0)\n",
    "\n",
    "# 楽観的初期値法を実装して下さい\n",
    "def optimistic(results, K, rsup):\n",
    "    # スロットごとの報酬の期待値を計算して、配列に格納してください\n",
    "    optimistic_mean = \n",
    "    # 最も報酬の期待値が大きいスロットを選択してください\n",
    "    slot_num = \n",
    "    return slot_num\n",
    "\n",
    "# (参考)１つ前のchapterで実装したepsilon_greedy法です\n",
    "def epsilon_greedy(results,epsilon):\n",
    "    if np.random.binomial(1, epsilon):\n",
    "        slot_num = random.randint(0, 4)\n",
    "    else:\n",
    "        slot_num = None\n",
    "        for i, d in enumerate(results):\n",
    "            if d[1] == 0:\n",
    "                slot_num = i\n",
    "                break\n",
    "        if slot_num == None:\n",
    "            slot_num = np.array([row[3] for row in results]).argmax()     \n",
    "    return slot_num\n",
    "\n",
    "# 環境を定義する関数です\n",
    "def environments(band_number):\n",
    "    coins_p = np.array([0.3, 0.4, 0.5, 0.6, 0.7])\n",
    "    results = np.random.binomial(1, coins_p)\n",
    "    result = results[band_number]\n",
    "    return result\n",
    "\n",
    "# 報酬を定義する関数です\n",
    "def reward(record, results, slot_num, time):\n",
    "    result = environments(slot_num)\n",
    "    record[time] = result\n",
    "    results[slot_num][1] += 1\n",
    "    results[slot_num][2] += result\n",
    "    results[slot_num][3] = results[slot_num][2] / results[slot_num][1]\n",
    "    return results, record\n",
    "\n",
    "# 初期変数を設定しています\n",
    "times = 10000\n",
    "results = [[0, 0, 0, 0], [1, 0, 0, 0], [2, 0, 0, 0], [3, 0, 0, 0], [4, 0, 0, 0]]\n",
    "record = np.zeros(times)\n",
    "Ks = range(1,6)\n",
    "\n",
    "# Kを変更させて収束の度合いをプロットしています\n",
    "for K in Ks:\n",
    "    for time in range(times):\n",
    "        slot_num = optimistic(results, K, 1)\n",
    "        results, record = reward(record, results, slot_num, time)\n",
    "    plt.plot(np.cumsum(record) / np.arange(1, record.size + 1), label =\"K = %d\" % K)\n",
    "plt.legend()\n",
    "plt.xlabel(\"試行回数\")\n",
    "plt.ylabel(\"平均報酬\")\n",
    "plt.title(\"楽観的初期値法のKの変動による結果の差異\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    " - results変数が最初から各マシンにおいてK回1が当たり続けた結果を格納しているものとして考えましょう。\n",
    " - optimistic_mean = np.array([(row[2] + K*rsup) / (row[1] + K) for row in results])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "answer"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(0)\n",
    "\n",
    "# 楽観的初期値法を実装して下さい\n",
    "def optimistic(results, K, rsup):\n",
    "    # スロットごとの報酬の期待値を計算して、配列に格納してください\n",
    "    optimistic_mean = np.array([(row[2] + K*rsup) / (row[1] + K) for row in results])\n",
    "    # 最も報酬の期待値が大きいスロットを選択してください\n",
    "    slot_num = optimistic_mean.argmax()\n",
    "    return slot_num\n",
    "\n",
    "# (参考)１つ前のchapterで実装したepsilon_greedy法です\n",
    "def epsilon_greedy(results,epsilon):\n",
    "    if np.random.binomial(1, epsilon):\n",
    "        slot_num = random.randint(0, 4)\n",
    "    else:\n",
    "        slot_num = None\n",
    "        for i, d in enumerate(results):\n",
    "            if d[1] == 0:\n",
    "                slot_num = i\n",
    "                break\n",
    "        if slot_num == None:\n",
    "            slot_num = np.array([row[3] for row in results]).argmax()     \n",
    "    return slot_num\n",
    "\n",
    "# 環境を定義する関数です\n",
    "def environments(band_number):\n",
    "    coins_p = np.array([0.3, 0.4, 0.5, 0.6, 0.7])\n",
    "    results = np.random.binomial(1, coins_p)\n",
    "    result = results[band_number]\n",
    "    return result\n",
    "\n",
    "# 報酬を定義する関数です\n",
    "def reward(record, results, slot_num, time):\n",
    "    result = environments(slot_num)\n",
    "    record[time] = result\n",
    "    results[slot_num][1] += 1\n",
    "    results[slot_num][2] += result\n",
    "    results[slot_num][3] = results[slot_num][2] / results[slot_num][1]\n",
    "    return results, record\n",
    "\n",
    "# 初期変数を設定しています\n",
    "times = 10000\n",
    "results = [[0, 0, 0, 0], [1, 0, 0, 0], [2, 0, 0, 0], [3, 0, 0, 0], [4, 0, 0, 0]]\n",
    "record = np.zeros(times)\n",
    "Ks = range(1,6)\n",
    "\n",
    "# Kを変更させて収束の度合いをプロットしています\n",
    "for K in Ks:\n",
    "    for time in range(times):\n",
    "        slot_num = optimistic(results, K, 1)\n",
    "        results, record = reward(record, results, slot_num, time)\n",
    "    plt.plot(np.cumsum(record) / np.arange(1, record.size + 1), label =\"K = %d\" % K)\n",
    "plt.legend()\n",
    "plt.xlabel(\"試行回数\")\n",
    "plt.ylabel(\"平均報酬\")\n",
    "plt.title(\"楽観的初期値法のKの変動による結果の差異\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "exerciseId": "jxt7hHbL1X",
    "id": "code_session_name",
    "important": false,
    "isDL": false,
    "timeoutSecs": 10
   },
   "source": [
    "### 1.2.4 soft-max法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "ε-greedy手法は確かに有用な手法ですが、探査を行う際にすべての行動を等しい確率で選択してしまう点が欠点です。  \n",
    "つまりほぼ最悪と思われる選択を取る確率とほぼ最適と思われる選択を取る確率が同程度になっているのです。例えばスロットマシーンの中に\n",
    "* 当たりだと1\n",
    "* 外れだと-100\n",
    "* 当たる確率は20%\n",
    "\n",
    "のものがあったとしましょう。このような極めて悪い行動があった場合、ε-greedy手法ではこれを少ない回数選択するということができません。そこで **<font color=#AA0000>soft-max</font>** 法では、推定される行動から価値が高そうな行動はより選ばれやすく、低そうな行動はより選ばれににくくします。ただし、全く選ばれなくなるわけではありません。正確に述べると、soft-max法とは**推定報酬によって選択する確率に重み付けをすることが可能になる手法**です。具体的には、 $Q_i$ を報酬の期待値、$\\tau$ (タウ)をパラメータとして$\\frac{\\exp{Q_{i}/ \\tau}}{\\sum^i \\exp{Q_{i}/ \\tau}}$と確率を表すことができます。$\\tau$が高ければすべての行動が選ばれるようになり、0に近づくと推定値の価値が高い行動が選ばれやすくなります。また、$\\tau$→0 の時、greedy手法と等価になります。\n",
    "\n",
    "soft-maxアルゴリズムは以下のように定義されます。\n",
    "1. 今までのデータがない場合、全ての手法の報酬を1で仮定します。\n",
    "2. 各マシンiの選択確率を$\\frac{\\exp{Q_{i}/ \\tau}}{\\sum_i \\exp{Q_{i}/ \\tau}}$で定義します。\n",
    "3. 2で定義した確率分布に基づき、選択を行います\n",
    "4. 選択で得られた報酬に基づき、報酬関数を更新します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "soft-max手法を実装して下さい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "index"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# 回答確認用です。変えないでください\n",
    "np.random.seed(55)\n",
    "\n",
    "tau = 0.1\n",
    "\n",
    "# soft-max法を実装して下さい\n",
    "def softmax(results, tau):\n",
    "    # 各マシンの予想期待値を配列にしてください\n",
    "    q = \n",
    "    \n",
    "    # 今までのデータがない場合、全ての手法の報酬を1で仮定してください\n",
    "    if np.sum(q) == 0:\n",
    "        q = \n",
    "        \n",
    "    # 各マシンの選択確率を設定してください\n",
    "    probability = \n",
    "    \n",
    "    # 確率分布に基づいて、slot_numを決定してください\n",
    "    slot_num = \n",
    "    \n",
    "    return slot_num\n",
    "\n",
    "\n",
    "\n",
    "# 環境を定義する関数です\n",
    "def environments(band_number):\n",
    "    coins_p = np.array([0.3, 0.4, 0.5, 0.6, 0.7])\n",
    "    results = np.random.binomial(1, coins_p)\n",
    "    result = results[band_number]\n",
    "    return result\n",
    "\n",
    "# 報酬を定義する関数です\n",
    "def reward(record, results, slot_num, time):\n",
    "    result = environments(slot_num)\n",
    "    record[time] = result\n",
    "    results[slot_num][1] += 1\n",
    "    results[slot_num][2] += result\n",
    "    results[slot_num][3] = results[slot_num][2] / results[slot_num][1]\n",
    "    return results, record\n",
    "\n",
    "\n",
    "results = [[0, 0, 0, 0], [1, 0, 0, 0], [2, 0, 0, 0], [3, 0, 0, 0], [4, 0, 0, 0]]\n",
    "times = 1000\n",
    "record = np.zeros(times)\n",
    "for time in range(0, times):\n",
    "    slot_num = softmax(results,tau)\n",
    "    results, record = reward(record, results, slot_num, time)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- 選択に`np.random.choice(配列, p=出現確率)`関数を使用してみましょう。\n",
    "- $e^0$ = 1 であるので $Q_i$=0でも確率は0にはなりません。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "answer"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# 回答確認用です。変えないでください\n",
    "np.random.seed(55)\n",
    "\n",
    "tau = 0.1\n",
    "\n",
    "# soft-max法を実装して下さい\n",
    "def softmax(results, tau):\n",
    "    # 各マシンの予想期待値を配列にしてください\n",
    "    q = np.array([row[3] for row in results])\n",
    "    \n",
    "    # 今までのデータがない場合、全ての手法の報酬を1で仮定してください\n",
    "    if np.sum(q) == 0:\n",
    "        q = np.ones(len(results))\n",
    "        \n",
    "    # 各マシンの選択確率を設定してください\n",
    "    probability = np.exp(q / tau) / sum(np.exp(q / tau))\n",
    "    \n",
    "    # 確率分布に基づいて、slot_numを決定してください\n",
    "    slot_num = np.random.choice([row[0] for row in results], p=probability)\n",
    "    \n",
    "    return slot_num\n",
    "\n",
    "\n",
    "# 環境を定義する関数です\n",
    "def environments(band_number):\n",
    "    coins_p = np.array([0.3, 0.4, 0.5, 0.6, 0.7])\n",
    "    results = np.random.binomial(1, coins_p)\n",
    "    result = results[band_number]\n",
    "    return result\n",
    "\n",
    "# 報酬を定義する関数です\n",
    "def reward(record, results, slot_num, time):\n",
    "    result = environments(slot_num)\n",
    "    record[time] = result\n",
    "    results[slot_num][1] += 1\n",
    "    results[slot_num][2] += result\n",
    "    results[slot_num][3] = results[slot_num][2] / results[slot_num][1]\n",
    "    return results, record\n",
    "\n",
    "\n",
    "results = [[0, 0, 0, 0], [1, 0, 0, 0], [2, 0, 0, 0], [3, 0, 0, 0], [4, 0, 0, 0]]\n",
    "times = 1000\n",
    "record = np.zeros(times)\n",
    "for time in range(0, times):\n",
    "    slot_num = softmax(results,tau)\n",
    "    results, record = reward(record, results, slot_num, time)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "exerciseId": "tSDhhIiapc",
    "id": "code_session_name",
    "important": false,
    "isDL": false,
    "timeoutSecs": 10
   },
   "source": [
    "### 1.2.5 UCB1アルゴリズム"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "\n",
    " **<font color=#AA0000>UCB1アルゴリズム</font>** は楽観的初期値法を改善した手法です。  \n",
    "greedy手法などで見られたような今までのデータから導出される期待値にバイアス(試行回数が少ない場合に大きくなる)を加えて最大となるマシンを選択します。<br>\n",
    "これはつまり、**そのマシンがどれほど当たってきたか**(成功率)という情報に**そのマシンについてどれだけ知っているか**(偶然による成功率のばらつきの大きさ)という情報を付け加えていくということに等しくなっています。こうすることによってあまり探索されていないマシンを積極的に探索するとともに、データが集まってくるにつれて最も当選確率の高いと見られるマシンを選択するということが同時にできるようになります。\n",
    "\n",
    "以下のようにしてUCB1アルゴリズムは定義されます。\n",
    "> 1. R : 報酬の最大値と最小値の差(今回は1です)とします。 \n",
    "> 2. まだ選んだことのないマシンがあればそれを選択する。\n",
    "> 3. マシンごとに今までの結果から得られた報酬の期待値$u_{i}$(腕の成功率)を計算する。   \n",
    "> <img src=\"https://aidemyexcontentspic.blob.core.windows.net/contents-pic/_RL/equation_1.png\">\n",
    "> 4. マシンごとの偶然による成功率のばらつきの大きさである$x_{i}$を計算する。  \n",
    "> <img src=\"https://aidemyexcontentspic.blob.core.windows.net/contents-pic/_RL/equation_2.png\">\n",
    "> 5. $UCB1 = u_{i} + x_{i}$が最大値になるマシンiを選択してプレイする。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "- 空欄を埋めてUCB手法を実装して下さい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "index"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "np.random.seed(0)\n",
    "\n",
    "R = 1\n",
    "\n",
    "#　UCB1法を実装して下さい\n",
    "def UCB(results, R):\n",
    "    slot_num = None\n",
    "    # まだ選んだことのないマシンがあればそれをslot_numにしてください\n",
    "    for i, d in enumerate(results):\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    # 全て一度は選んでいる場合\n",
    "    if slot_num == None:\n",
    "        \n",
    "        # これまでの総試行回数を計算してください\n",
    "        times = \n",
    "        \n",
    "        # マシンごとに今までの結果から得られた成功率uiを取り出してリストに格納してください\n",
    "        ui = \n",
    "        \n",
    "        # マシンごとの偶然による成功率のばらつきの大きさxi を計算してリストに格納してください\n",
    "        xi = \n",
    "        \n",
    "        # ui+xiを計算してリストに格納してください\n",
    "        uixi = \n",
    "        \n",
    "        # uixiが最大値となるマシンをslot_numにしてください\n",
    "        slot_num = \n",
    "        \n",
    "    return slot_num\n",
    "\n",
    "# 環境を定義する関数です\n",
    "def environments(band_number):\n",
    "    coins_p = np.array([0.3, 0.4, 0.5, 0.6, 0.7])\n",
    "    results = np.random.binomial(1, coins_p)\n",
    "    result = results[band_number]\n",
    "    return result\n",
    "\n",
    "# 報酬を定義する関数です\n",
    "def reward(record, results, slot_num, time):\n",
    "    result = environments(slot_num)\n",
    "    record[time] = result\n",
    "    results[slot_num][1] += 1\n",
    "    results[slot_num][2] += result\n",
    "    results[slot_num][3] = results[slot_num][2] / results[slot_num][1]\n",
    "    return results, record\n",
    "\n",
    "\n",
    "results = [[0, 0, 0, 0], [1, 0, 0, 0], [2, 0, 0, 0], [3, 0, 0, 0], [4, 0, 0, 0]]\n",
    "times = 1000\n",
    "record = np.zeros(times)\n",
    "for time in range(0, times):\n",
    "    slot_num = UCB(results,R)\n",
    "    results, record = reward(record, results, slot_num, time)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- 一定のプレイ回数に至るまで各腕をプレイし、配列から最大値を記録するインデックスを抽出する事は今までの手法と同じです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "answer"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "np.random.seed(0)\n",
    "\n",
    "R = 1\n",
    "\n",
    "#　UCB1法を実装して下さい\n",
    "def UCB(results, R):\n",
    "    slot_num = None\n",
    "    # まだ選んだことのないマシンがあればそれをslot_numにしてください\n",
    "    for i, d in enumerate(results):\n",
    "        if d[1] == 0:\n",
    "            slot_num = i\n",
    "            break\n",
    "\n",
    "    # 全て一度は選んでいる場合\n",
    "    if slot_num == None:\n",
    "        \n",
    "        # これまでの総試行回数を計算してください\n",
    "        times = sum([row[1] for row in results])\n",
    "        \n",
    "        # マシンごとに今までの結果から得られた成功率uiを取り出してリストに格納してください\n",
    "        ui = [row[3] for row in results]\n",
    "        \n",
    "        # マシンごとの偶然による成功率のばらつきの大きさxi を計算してリストに格納してください\n",
    "        xi = [R*math.sqrt(2*math.log(times) / row[1]) for row in results]\n",
    "        \n",
    "        # ui+xiを計算してリストに格納してください\n",
    "        uixi = [x+u for x, u in zip(xi, ui)]\n",
    "        \n",
    "        # uixiが最大値となるマシンをslot_numにしてください\n",
    "        slot_num = uixi.index(max(uixi))\n",
    "        \n",
    "    return slot_num\n",
    "\n",
    "# 環境を定義する関数です\n",
    "def environments(band_number):\n",
    "    coins_p = np.array([0.3, 0.4, 0.5, 0.6, 0.7])\n",
    "    results = np.random.binomial(1, coins_p)\n",
    "    result = results[band_number]\n",
    "    return result\n",
    "\n",
    "# 報酬を定義する関数です\n",
    "def reward(record, results, slot_num, time):\n",
    "    result = environments(slot_num)\n",
    "    record[time] = result\n",
    "    results[slot_num][1] += 1\n",
    "    results[slot_num][2] += result\n",
    "    results[slot_num][3] = results[slot_num][2] / results[slot_num][1]\n",
    "    return results, record\n",
    "\n",
    "\n",
    "results = [[0, 0, 0, 0], [1, 0, 0, 0], [2, 0, 0, 0], [3, 0, 0, 0], [4, 0, 0, 0]]\n",
    "times = 1000\n",
    "record = np.zeros(times)\n",
    "for time in range(0, times):\n",
    "    slot_num = UCB(results,R)\n",
    "    results, record = reward(record, results, slot_num, time)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "exerciseId": "GZKEpOpBU8",
    "id": "code_session_name",
    "important": false,
    "isDL": false,
    "timeoutSecs": 10
   },
   "source": [
    "### 1.2.6 まとめ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "これまで\n",
    "N腕バンディッド問題を解くための方策として\n",
    "- greedy手法\n",
    "- ε-greedy手法\n",
    "- soft-max手法\n",
    "- 楽観的初期値法\n",
    "- UCB1アルゴリズム  \n",
    "\n",
    "の5つについて説明してきました。ここで一つ強調しておくべきは、ある一つの問題について各手法を比較しても、手法の優劣を議論する根拠にはならないということです。重要なのはこれらの手法の間に明確な優劣があるというわけではなく、問題設定に応じて最適な手法を使い分ける必要があるということです。このことをまとめとして覚えておいて下さい。\n",
    "\n",
    "＜備考＞<br>\n",
    "最初から全ての成功率がわかっているときは、もっとも成功率の高いものを選び続ければ良いことになります。この場合とある方策との報酬の合計の差分のことを**リグレット**と呼びます。エージェントは必ず探索を行わないといけなため、リグレットを0にすることは不可能ですがある程度試行回数を重ねれば次第にリグレットは小さくなっていきます。<br>\n",
    "**UCB1アルゴリズムはこのリグレットを最小化することができることが知られています。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "- (80行目以降)greedy法、ε-greedy手法、楽観的初期値法、UCB1アルゴリズムの4つの手法の結果をそれぞれプロットし、それぞれの手法における精度を確認してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "index"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "def greedy(results, n):\n",
    "    slot_num = None\n",
    "    for i, d in enumerate(results):\n",
    "        if d[1] < n:\n",
    "            slot_num = i\n",
    "\n",
    "    if slot_num == None:\n",
    "        slot_num = [row[3] for row in results].index(max([row[3] for row in results]))\n",
    "    return slot_num\n",
    "\n",
    "\n",
    "def optimistic(results, K, rsup):\n",
    "    optimistic_mean = [(row[2] + K*rsup) / (row[1] + K) for row in results]\n",
    "    slot_num = optimistic_mean.index(max(optimistic_mean))\n",
    "    return slot_num\n",
    "\n",
    "\n",
    "def epsilon_greedy(results, epsilon):\n",
    "    if np.random.binomial(1, epsilon):\n",
    "        slot_num = random.randint(0, 4)\n",
    "    else:\n",
    "        slot_num = None\n",
    "        for i, d in enumerate(results):\n",
    "            if d[1] == 0:\n",
    "                slot_num = i\n",
    "        if slot_num == None:\n",
    "            slot_num = np.array([row[3] for row in results]).argmax()           \n",
    "    return slot_num\n",
    "\n",
    "\n",
    "def UCB(results, R):\n",
    "    slot_num = None\n",
    "    for i, d in enumerate(results):\n",
    "        if d[1] == 0:\n",
    "            slot_num = i\n",
    "    if slot_num == None:\n",
    "        times = sum([row[1] for row in results])\n",
    "        u = [row[3] for row in results]\n",
    "        xi = [R*math.sqrt(2*math.log(times) / row[1]) for row in results]\n",
    "        uixi = [x+u for x, u in zip(xi, u)]\n",
    "        slot_num = uixi.index(max(uixi))\n",
    "    return slot_num\n",
    "\n",
    "\n",
    "def environments(band_number):\n",
    "    coins_p = np.array([0.3, 0.4, 0.5, 0.6, 0.7])\n",
    "    results = np.random.binomial(1, coins_p)\n",
    "    result = results[band_number]\n",
    "    return result\n",
    "\n",
    "\n",
    "def reward(record, results, slot_num, time):\n",
    "    result = environments(slot_num)\n",
    "    \n",
    "    record[time] = result\n",
    "    results[slot_num][1] += 1\n",
    "    results[slot_num][2] += result\n",
    "    results[slot_num][3] = results[slot_num][2]/results[slot_num][1]\n",
    "    return results,record\n",
    "\n",
    "\n",
    "times = 10000\n",
    "n = 20\n",
    "K = 10\n",
    "rsup = 1\n",
    "R = 1\n",
    "epsilon = 0.2\n",
    "results = [[[0, 0, 0, 0], [1, 0, 0, 0], [2, 0, 0, 0], [3, 0, 0, 0], [4, 0, 0, 0]] for _ in range(4)]\n",
    "slot_num = np.zeros(4,dtype=np.int)\n",
    "record = np.zeros((4,times))\n",
    "# この続きをコーティングして手法ごとにプロットしてください\n",
    "\n",
    "# resultsからそれぞれの方策でslot_numを決定してください\n",
    "for time in range(0, times):\n",
    "    # greedy手法\n",
    "    slot_num[0] = \n",
    "    # ε-greedy手法\n",
    "    slot_num[1] = \n",
    "    # 楽観的初期値法\n",
    "    slot_num[2] = \n",
    "    # UCB1アルゴリズム\n",
    "    slot_num[3] = \n",
    "    \n",
    "    # results、recordを上書きしてください\n",
    "    for i in range(4):\n",
    "        results[i], record[i] = \n",
    "\n",
    "# 凡例を以下のように設定しています\n",
    "labels = [\"greedy\",\"epsilon_greedy\",\"optimistic\",\"UCB\"]\n",
    "for i in range(4):\n",
    "    # 4種の手法の結果をプロットしてください\n",
    "    \n",
    "\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.xlabel(\"試行回数\")\n",
    "plt.ylabel(\"平均報酬\")\n",
    "plt.title(\"N腕バンディッド問題の各手法における報酬の収束\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- results, slot_num, recordはすべて今までのものとは別の形で初期化されているので注意してください。\n",
    "- xi = [R*math.sqrt(2*math.log(times) / row[1]) for row in results]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "answer"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "%matplotlib inline\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "def greedy(results, n):\n",
    "    slot_num = None\n",
    "    for i, d in enumerate(results):\n",
    "        if d[1] < n:\n",
    "            slot_num = i\n",
    "\n",
    "    if slot_num == None:\n",
    "        slot_num = [row[3] for row in results].index(max([row[3] for row in results]))\n",
    "    return slot_num\n",
    "\n",
    "\n",
    "def optimistic(results, K, rsup):\n",
    "    optimistic_mean = [(row[2] + K*rsup) / (row[1] + K) for row in results]\n",
    "    slot_num = optimistic_mean.index(max(optimistic_mean))\n",
    "    return slot_num\n",
    "\n",
    "\n",
    "def epsilon_greedy(results, epsilon):\n",
    "    if np.random.binomial(1, epsilon):\n",
    "        slot_num = random.randint(0, 4)\n",
    "    else:\n",
    "        slot_num = None\n",
    "        for i, d in enumerate(results):\n",
    "            if d[1] == 0:\n",
    "                slot_num = i\n",
    "        if slot_num == None:\n",
    "            slot_num = np.array([row[3] for row in results]).argmax()           \n",
    "    return slot_num\n",
    "\n",
    "\n",
    "def UCB(results, R):\n",
    "    slot_num = None\n",
    "    for i, d in enumerate(results):\n",
    "        if d[1] == 0:\n",
    "            slot_num = i\n",
    "    if slot_num == None:\n",
    "        times = sum([row[1] for row in results])\n",
    "        u = [row[3] for row in results]\n",
    "        xi = [R * math.sqrt(2 * math.log(times) / row[1]) for row in results]\n",
    "        uixi = [x + u for x, u in zip(xi, u)]\n",
    "        slot_num = uixi.index(max(uixi))\n",
    "    return slot_num\n",
    "\n",
    "\n",
    "def environments(band_number):\n",
    "    coins_p = np.array([0.3, 0.4, 0.5, 0.6, 0.7])\n",
    "    results = np.random.binomial(1, coins_p)\n",
    "    result = results[band_number]\n",
    "    return result\n",
    "\n",
    "\n",
    "def reward(record, results, slot_num, time):\n",
    "    result = environments(slot_num)\n",
    "\n",
    "    record[time] = result\n",
    "    results[slot_num][1] += 1\n",
    "    results[slot_num][2] += result\n",
    "    results[slot_num][3] = results[slot_num][2] / results[slot_num][1]\n",
    "    return results, record\n",
    "\n",
    "\n",
    "times = 10000\n",
    "n = 20\n",
    "K = 10\n",
    "rsup = 1\n",
    "R = 1\n",
    "epsilon = 0.2\n",
    "results = [[[0, 0, 0, 0], [1, 0, 0, 0], [2, 0, 0, 0], [3, 0, 0, 0], [4, 0, 0, 0]] for _ in range(4)]\n",
    "slot_num = np.zeros(4,dtype=np.int)\n",
    "record = np.zeros((4,times))\n",
    "# この続きをコーティングして手法ごとにプロットしてください\n",
    "\n",
    "# resultsからそれぞれの方策でslot_numを決定してください\n",
    "for time in range(0, times):\n",
    "    # greedy手法\n",
    "    slot_num[0] = greedy(results[0],n)\n",
    "    # ε-greedy手法\n",
    "    slot_num[1] = epsilon_greedy(results[1],epsilon)\n",
    "    # 楽観的初期値法\n",
    "    slot_num[2] = optimistic(results[2], K, rsup)\n",
    "    # UCB1アルゴリズム\n",
    "    slot_num[3] = UCB(results[3], R)\n",
    "    \n",
    "    # results、recordを上書きしてください\n",
    "    for i in range(4):\n",
    "        results[i], record[i] = reward(record[i], results[i], slot_num[i], time)\n",
    "\n",
    "# 凡例を以下のように設定しています\n",
    "labels = [\"greedy\",\"epsilon_greedy\",\"optimistic\",\"UCB\"]\n",
    "for i in range(4):\n",
    "    # 4種の手法の結果をプロットしてください\n",
    "    plt.plot(np.cumsum(record[i]) / np.arange(1, record[i].size + 1),label=labels[i])\n",
    "    \n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.xlabel(\"試行回数\")\n",
    "plt.ylabel(\"平均報酬\")\n",
    "plt.title(\"N腕バンディッド問題の各手法における報酬の収束\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "exerciseId": "O3aqV9aZ2c",
    "id": "quiz_session_name",
    "important": false,
    "isDL": false,
    "timeoutSecs": 10
   },
   "source": [
    "### 1.2.7 探索と利用のトレードオフ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    " **<font color=#AA0000>探索と利用のトレードオフ</font>** について説明していきます。\n",
    "\n",
    "これは、探索と利用のバランスの問題であり、探索の割合を増やすと、探索中は最善の選択を選ばないため、そのぶん損失が発生してしまうのに対し、利用の割合を増やすと、最善の選択が今までされてこなかった場合にそれを見落とすリスクが発生してしまいます。このようにトレードオフの関係になっているのです。例えばε-greedy手法とUCB手法について比較してみましょう。\n",
    "\n",
    "<img src=\"https://aidemyexcontentspic.blob.core.windows.net/contents-pic/_RL/img_3.png\">\n",
    "\n",
    "\n",
    "それぞれの方策に従って100000回引き続けるシミュレーションです。8000回前後辺りまではε-greedy手法の方が高い平均報酬率を示していますが、回数が増えるにつれUCB手法の方が上回るようになります。それはε-greedy手法が十分に探索を終えた後でも一定確率で探索を行ってしまい、それが損失になっているからなのです。  \n",
    "\n",
    "この**トレードオフの関係を環境の状態によってどのように取るかが強化学習の本質**とも言えます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "以下のうち正しいものはどれでしょう？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "choices"
   },
   "source": [
    "- 正しい期待値を得るため探索回数は多いほうがいい\n",
    "- コストを抑えるため探索回数はある程度に抑える\n",
    "- コストは抑えて期待値の最も高い試行を実行する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- トレードオフの関係にあるものは両方のいいとこどりはできません。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "answer"
   },
   "source": [
    "- コストを抑えるため探索回数はある程度に抑える"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "chapter_exam"
   },
   "source": [
    "## 添削問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "nを2回、10回、200回というように変えて、  \n",
    "greedy手法を使用して4000回引くシミュレーションを行い、平均報酬率の変化を描画してください。 \n",
    "\n",
    "これを10回行い平均報酬率を計算して下さい。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "index"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "\n",
    "def environments(band_number):\n",
    "    coins_p = np.array([0.3, 0.4, 0.5, 0.6, 0.7])\n",
    "    results = np.random.binomial(1, coins_p)\n",
    "    result = results[band_number]\n",
    "    return result\n",
    "\n",
    "def reward(record, results, slot_num, time):\n",
    "    result = environments(slot_num)\n",
    "\n",
    "    record[time] = result\n",
    "    results[slot_num][1] += 1\n",
    "    results[slot_num][2] += result\n",
    "    results[slot_num][3] = results[slot_num][2]/results[slot_num][1]\n",
    "    return results, record\n",
    "\n",
    "#greedy法を実装してください。\n",
    "def greedy(results, n):\n",
    "\n",
    "    \n",
    "    \n",
    "    return slot_num\n",
    "\n",
    "\n",
    "\n",
    "# 与えられたn、総試行回数に基づいて一度実行し、plt.plot()の返り値と平均報酬率を返します\n",
    "# 答えを入力して下さい\n",
    "def nsearch(n, times):\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return p1, average\n",
    "\n",
    "# 初期化します\n",
    "#averagesにはn=2,10,200のときの平均報酬率をリストにして格納してください。\n",
    "plt.clf()  \n",
    "times = 4000\n",
    "averages = [0, 0, 0]\n",
    "\n",
    "\n",
    "# 最初の結果をaveragesに格納しプロットします　答えを入力して下さい\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# 残り9回を行いaveragesに格納します　答えを入力して下さい\n",
    "for i in range(9):\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# 答えを出力します　\n",
    "answer = list(map(lambda x: x/10, averages))\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "choices"
   },
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "平均報酬率の計算にはrecordの累積和を使いましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "#### 解答例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "answer"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "def environments(band_number):\n",
    "    coins_p = np.array([0.3, 0.4, 0.5, 0.6, 0.7])\n",
    "    results = np.random.binomial(1, coins_p)\n",
    "    result = results[band_number]\n",
    "    return result\n",
    "\n",
    "\n",
    "def reward(record,results,slot_num,time):\n",
    "    result = environments(slot_num)\n",
    "\n",
    "    record[time] = result\n",
    "    results[slot_num][1] += 1\n",
    "    results[slot_num][2] += result\n",
    "    results[slot_num][3] = results[slot_num][2]/results[slot_num][1]\n",
    "    return results, record\n",
    "\n",
    "def greedy(results, n):\n",
    "    slot_num = None\n",
    "    for i, d in enumerate(results):\n",
    "        if d[1] < n:\n",
    "            slot_num = i\n",
    "\n",
    "    if slot_num == None:\n",
    "        slot_num = [row[3] for row in results].index(max([row[3] for row in results]))\n",
    "    return slot_num\n",
    "\n",
    "\n",
    "\n",
    "# 与えられたn、総試行回数に基づいて一度実行し、plt.plot()の返り値と平均報酬率を返します\n",
    "# 答えを入力して下さい\n",
    "def nsearch(n, times):\n",
    "    results = [[0, 0, 0, 0], [1, 0, 0, 0], [2, 0, 0, 0], [3, 0, 0, 0], [4, 0, 0, 0]]\n",
    "    record = np.zeros(times)\n",
    "    for time in range(times):\n",
    "        slot_num = greedy(results, n)\n",
    "        \n",
    "        results, record = reward(record, results, slot_num, time)\n",
    "\n",
    "        \n",
    "    p1, = plt.plot(np.cumsum(record) / np.arange(1, record.size + 1))\n",
    "    average = (np.sum(record))/(record.size)\n",
    "    \n",
    "    return p1, average\n",
    "\n",
    "# 初期化します\n",
    "plt.clf()  \n",
    "times = 4000\n",
    "averages = [0, 0, 0]\n",
    "\n",
    "\n",
    "# 最初の結果をaveragesに格納しプロットします　答えを入力して下さい\n",
    "p1, averages[0] = nsearch(2, times)\n",
    "p2, averages[1] = nsearch(10, times)\n",
    "p3, averages[2] = nsearch(200, times)\n",
    "plt.legend([p1, p2, p3], [\"2\", \"10\", \"200\"])\n",
    "plt.xlabel(\"times\")\n",
    "plt.ylabel(\"result\")\n",
    "plt.show()\n",
    "\n",
    "# 残り9回を行いaveragesに格納します　答えを入力して下さい\n",
    "for i in range(9):\n",
    "    averages[0] += (nsearch(2, times))[1] \n",
    "    averages[1] += (nsearch(10, times))[1] \n",
    "    averages[2] += (nsearch(200, times))[1]\n",
    "\n",
    "    \n",
    "# 答えを出力します　\n",
    "answer = list(map(lambda x: x/10, averages))\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "answer"
   },
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "229.75px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}