{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "chapterId": "r1EW3ekM7IG",
    "id": "chapter_name"
   },
   "source": [
    "#  動的計画法とTD手法\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "table"
   },
   "source": [
    "- **[3.1 動的計画法](#3.1-動的計画法)** \n",
    "    - **[3.1.1 動的計画法とは](#3.1.1-動的計画法とは)** \n",
    "    - **[3.1.2 方策評価](#3.1.2-方策評価)** \n",
    "    - **[3.1.3 方策反復](#3.1.3-方策反復)** \n",
    "    - **[3.1.4 価値反復](#3.1.4-価値反復)** \n",
    "<br><br>\n",
    "- **[3.2 TD手法](#3.2-TD手法)** \n",
    "    - **[3.2.1 TD手法とは](#3.2.1-TD手法とは)** \n",
    "    - **[3.2.2 Sarsa](#3.2.2-Sarsa)**\n",
    "    - **[3.2.3 SarsaにおけるQ関数の実装](#3.2.3-SarsaにおけるQ関数の実装)**\n",
    "    - **[3.2.4 Sarsaでのε-greedy手法の実装](#3.2.4-Sarsaでのε-greedy手法の実装)**   \n",
    "    - **[3.2.5 Q学習](#3.2.5-Q学習)**\n",
    "<br><br>\n",
    "- **[添削問題](#添削問題)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section_name",
    "sectionId": "SyBb3xkGQIM"
   },
   "source": [
    "## 3.1 動的計画法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5040,
    "exerciseId": "SkU-3gkMmUf",
    "id": "quiz_session_name",
    "important": false,
    "isDL": false,
    "timeoutSecs": 10
   },
   "source": [
    "### 3.1.1 動的計画法とは"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "ここからは与えられたベルマン方程式を実際に解き、最適な方策を見つけ出すための手法について説明していきます。  \n",
    "環境のモデルがMDP（マルコフ決定過程）として**完全に**与えられている時の解法アルゴリズムのことを \n",
    "**<font color=#AA0000>動的計画法(DP)</font>** と呼称します。そして今回の目的は、ある環境下において最適価値関数または最適行動価値関数を導くことにあります。\n",
    "***\n",
    "**マルコフ決定過程とは？** <br>\n",
    "未来の状態は現在の状態・行動のみによって確率的に決定され、過去の挙動と無関係であるという条件を満たす強化学習過程のことをマルコフ決定過程といいます。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "\n",
    "$V^{*}(s) = \\max\\limits_{\\alpha \\in A} \\displaystyle\\sum_{s'\\in S}^{} P(s'|s,a)(r(s,a,s') + \\gamma V^{*}(s'\n",
    "))$\n",
    "\n",
    "$Q^{*}(s,a) = \\displaystyle\\sum_{s'\\in S}^{} P(s'|s,a)(r(s,a,s') + \\gamma \\max\\limits_{\\alpha \\in A} Q^{*}(s,a))$\n",
    "\n",
    "上の式は以下のうちどちらに当てはまるでしょうか？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "choices"
   },
   "source": [
    "- ベルマン最適方程式\n",
    "- ベルマン方程式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- アスタリスクの意味を思い出してみましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "answer"
   },
   "source": [
    "- ベルマン最適方程式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5040,
    "exerciseId": "SJvZhx1fmLM",
    "id": "code_session_name",
    "important": false,
    "isDL": false,
    "timeoutSecs": 10
   },
   "source": [
    "### 3.1.2 方策評価"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "まず、最適価値関数を見つけ出す前に、ある特定の手法$\\pi$を取った時の価値関数$V^{\\pi}(s)$を計算する必要があります。この計算方法を **<font color=#AA0000>方策評価(policy evaluation)</font>** と呼びます。方策評価は以下のように近似します。\n",
    "\n",
    ">1. 閾値$\\epsilon$を事前に設定。更新量が$\\epsilon$よりも小さくなるまで計算を行います\n",
    ">2. 方策$\\pi$を入力\n",
    ">3. $V(s)$を全ての状態において0として仮定   \n",
    ">4. 以下の動作を繰り返す:  \n",
    ">    1. $\\delta$ = 0  \n",
    ">    2. 全ての状態sについて :  \n",
    ">        1. $v = V(s)$\n",
    ">        2. $V(s) =\\displaystyle\\sum_{\\alpha \\in A(s)}^{} \\pi(a|s) \\displaystyle\\sum_{s' \\in S}^{} P(s'|s,a)(r(s,a,s') + \\gamma V(s'))$ で更新\n",
    ">        3. $\\delta$ = $\\max(\\delta,|v-V(s)|)$\n",
    ">    3. $\\delta < \\epsilon$  ならば、ループを脱出\n",
    ">5. V(s)を$V^{\\pi}$の近似解とみなして出力\n",
    "\n",
    "4.Aにおいてわざわざ$\\delta$をおいているのは、全ての状態の差分を閾値よりも小さくするためです。状態がひとつだけの場合$\\delta$を使わずに計算することが可能ですが、状態が複数になると全ての状態の中で差分の最も大きいものを保存しておく変数が必要になります。その役割を$\\delta$がみなしています。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "・環境1において方策評価を実装し価値関数を求めてください。 <br>\n",
    "・方策はState_A, State_B両方において常にaction Xを選択するものとします。 <br>\n",
    "        \n",
    "<img src=\"https://aidemyexcontentspic.blob.core.windows.net/contents-pic/_RL/img_7.png\">\n",
    "\n",
    "<center>(図:環境1)</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "index"
   },
   "outputs": [],
   "source": [
    "from markov import *\n",
    "\n",
    "# 空欄を埋めて価値評価関数を完成させてください\n",
    "def policy_evaluation(pi, states, epsilon=0.001):\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "            return V\n",
    "        \n",
    "\n",
    "state_transition = np.array(\n",
    "    [[0, 0, 0, 0.3, 5],\n",
    "    [0, 1, 0, 0.7, -10],\n",
    "    [0, 0, 1, 1, 5],\n",
    "    [1, 0, 1, 1, 5],\n",
    "    [1, 2, 0, 0.8, 100],\n",
    "    [1, 1, 0, 0.2, 1]]\n",
    ")\n",
    "\n",
    "states = np.unique(state_transition[:, 0])\n",
    "actlist = np.unique(state_transition[:, 2])\n",
    "terminals = [2]\n",
    "gamma = 0.8\n",
    "\n",
    "V = {s: 0 for s in np.hstack((states,terminals))}\n",
    "pi = {s: 0 for s in states}\n",
    "policy_evaluation(pi, states, epsilon =0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- Vは `{state : 価値関数の値}` の辞書型で表現してみましょう。\n",
    "- `policy_evaluation` 関数で渡しているstatesにterminalsの値は含まれていません。Vにはterminalsの状態も必要なので、V作成のときに注意してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "answer"
   },
   "outputs": [],
   "source": [
    "from markov import *\n",
    "\n",
    "# 空欄を埋めて価値評価関数を完成させてください\n",
    "def policy_evaluation(pi, states, epsilon=0.001):\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in states:\n",
    "            v = V.copy()\n",
    "            V[s] = sum([p * (R(s, pi[s], s1) + gamma * V[s1]) for (p, s1) in T(s, pi[s])])\n",
    "            delta = max(delta, abs(v[s] - V[s]))\n",
    "        if  delta < epsilon:\n",
    "            return V\n",
    "        \n",
    "\n",
    "state_transition = np.array(\n",
    "    [[0, 0, 0, 0.3, 5],\n",
    "    [0, 1, 0, 0.7, -10],\n",
    "    [0, 0, 1, 1, 5],\n",
    "    [1, 0, 1, 1, 5],\n",
    "    [1, 2, 0, 0.8, 100],\n",
    "    [1, 1, 0, 0.2, 1]]\n",
    ")\n",
    "\n",
    "states = np.unique(state_transition[:, 0])\n",
    "actlist = np.unique(state_transition[:, 2])\n",
    "terminals = [2]\n",
    "gamma = 0.8\n",
    "\n",
    "V = {s: 0 for s in np.hstack((states, terminals))}\n",
    "pi = {s: 0 for s in states}\n",
    "policy_evaluation(pi, states, epsilon =0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5040,
    "exerciseId": "BJOWhxJzmUM",
    "id": "code_session_name",
    "important": false,
    "isDL": false,
    "timeoutSecs": 10
   },
   "source": [
    "### 3.1.3 方策反復"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "方策評価ではある手法${\\pi}$に対して価値関数$V^{\\pi}(s)$を計算しました。  \n",
    "ここでこのような価値関数においてgreedy手法をとるような方策を$\\pi'$と置きます。このような$\\pi'$に対して前回の手法を適用し、新たな価値関数$V^{\\pi'}(s)$を計算します。すると$V^{\\pi} < V^{\\pi'}$となることが一般に知られています。これを **<font color=#AA0000>価値改善</font>** と言います。greedy手法とは期待値が大きいと考えられる手法です。<br>\n",
    "\n",
    "ここではこの価値改善を繰り返していくことを考えましょう。<br>\n",
    "改善と評価を繰り返すことで最適価値関数を導き出すことが可能になります。これを **<font color=#AA0000>方策反復</font>** といい、以下のアルゴリズムで評価されます。\n",
    "\n",
    ">1. 全ての状態$s\\in S$に対してV(s)と$\\pi(s)$を初期化する  \n",
    ">2. $\\delta$が閾値以下になるまで以下を繰り返す:(方策評価)\n",
    ">    1. $\\delta = 0$\n",
    ">    2. 各状態$s\\in S$について:\n",
    ">        1. $v = V(s)$ \n",
    ">        2. $V(s)$ = $\\displaystyle\\sum_{s'\\in S}^{} P(s'|s,\\pi(s))(r(s,\\pi(s),s') + \\gamma V(s'))$で更新\n",
    ">        3. $\\delta$ = $\\max(\\delta,|v-V(s)|)$ \n",
    ">3. policy-flag = True とおく\n",
    ">4. 各状態$s\\in S$について:\n",
    ">     1. $b = \\pi(s)$\n",
    ">     2. $\\pi(s)=arg\\max\\limits_{a \\in A} \\displaystyle\\sum_{s'\\in S}^{} P(s'|s,a)(r(s,a,s') + \\gamma V(s'))$\n",
    ">     3. もし$b \\neq$$\\pi(s)$ ならば　policy-flag = False\n",
    ">5. もしpolicy-flag = Trueならば終了。それ以外は2から繰り返す\n",
    "\n",
    "4.Bは2.の方策評価を行ったあとの価値関数を使って全てのactionにおける価値関数を求めています。二度手間のように感じられますが、4.の方の価値関数はすでに計算されたものであり、方策評価とは少し異なることに注意してください。<br>\n",
    "4.Bでは方策の更新もしています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "環境1において、空欄を埋めて方策反復関数を実装してください。<br>\n",
    "\n",
    "\n",
    "<img src=\"https://aidemyexcontentspic.blob.core.windows.net/contents-pic/_RL/img_7.png\">\n",
    "\n",
    "<center>(図:環境1)</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "index"
   },
   "outputs": [],
   "source": [
    "from markov import *\n",
    "random.seed(0)\n",
    "\n",
    "# 方策評価関数です\n",
    "def policy_evaluation(pi, V, states, epsilon=0.001):\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in states:\n",
    "            v = V.copy()\n",
    "            V[s] = sum([p * (R(s, pi[s], s1) + gamma * V[s1]) for (p, s1) in T(s,pi[s])])\n",
    "            delta = max(delta,abs(v[s] - V[s]))\n",
    "        if  delta < epsilon:\n",
    "            return V\n",
    "\n",
    "# 方策反復関数を実装して下さい\n",
    "def policy_iteration(states):\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "            return pi\n",
    "\n",
    "state_transition = np.array(\n",
    "    [[0, 0, 0, 0.3, 5],\n",
    "    [0, 1, 0, 0.7, -10],\n",
    "    [0, 0, 1, 1, 5],\n",
    "    [1, 0, 1, 1, 5],\n",
    "    [1, 2, 0, 0.8, 100],\n",
    "    [1, 1, 0, 0.2, 1]]\n",
    ")\n",
    "\n",
    "states = np.unique(state_transition[:,0])\n",
    "terminals = [2]\n",
    "init = [0]\n",
    "gamma = 0.8\n",
    "\n",
    "# 関数を動かして結果を表示します\n",
    "pi = policy_iteration(states)\n",
    "print(pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- 上のアルゴリズムの2番は `policy_evaluation` 関数が担っています。\n",
    "- `pi` の初期値はどんなactionでも大丈夫です。解答例ではランダムに選んでいます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "answer"
   },
   "outputs": [],
   "source": [
    "from markov import *\n",
    "random.seed(0)\n",
    "\n",
    "# 方策評価関数です\n",
    "def policy_evaluation(pi, V, states, epsilon=0.001):\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in states:\n",
    "            v = V.copy()\n",
    "            V[s] = sum([p * (R(s, pi[s], s1) + gamma * V[s1]) for (p, s1) in T(s,pi[s])])\n",
    "            delta = max(delta,abs(v[s] - V[s]))\n",
    "        if  delta < epsilon:\n",
    "            return V\n",
    "\n",
    "# 方策反復関数を実装して下さい\n",
    "def policy_iteration(states):\n",
    "    V = {s: 0 for s in np.hstack((states, terminals))}\n",
    "    pi = {s: random.choice(actions(s)) for s in states}\n",
    "    while True:\n",
    "        V = policy_evaluation(pi, V, states, epsilon=0.001)\n",
    "        policy_flag = True\n",
    "\n",
    "        for s in states:\n",
    "            action =sorted(actions(s), key=lambda a:sum([p * gamma * V[s1] + p * R(s, a, s1) for (p, s1) in T(s, a)]), reverse=True)\n",
    "            if action[0] != pi[s]:\n",
    "                pi[s] = action[0]\n",
    "                policy_flag = False\n",
    "        if policy_flag:\n",
    "            return pi\n",
    "\n",
    "state_transition = np.array(\n",
    "    [[0, 0, 0, 0.3, 5],\n",
    "    [0, 1, 0, 0.7, -10],\n",
    "    [0, 0, 1, 1, 5],\n",
    "    [1, 0, 1, 1, 5],\n",
    "    [1, 2, 0, 0.8, 100],\n",
    "    [1, 1, 0, 0.2, 1]]\n",
    ")\n",
    "\n",
    "states = np.unique(state_transition[:,0])\n",
    "terminals = [2]\n",
    "init = [0]\n",
    "gamma = 0.8\n",
    "\n",
    "# 関数を動かして結果を表示します\n",
    "pi = policy_iteration(states)\n",
    "print(pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5040,
    "exerciseId": "B1tWnlyG7Lf",
    "id": "code_session_name",
    "important": false,
    "isDL": false,
    "timeoutSecs": 10
   },
   "source": [
    "### 3.1.4　価値反復"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "**方策反復**でのアルゴリズムには、毎回全ての状態について価値関数を計算し直す事を複数回しなければならないという問題があります。  \n",
    "その結果、状態が増えていく場合計算量が大きく増加してしまいます。これを解決するために生み出されたのが **<font color=#AA0000>価値反復</font>** です。以下のようなアルゴリズムで実行されます。\n",
    "\n",
    ">1. 全ての状態$s\\in S$に対してV(s)と$\\pi(s)$を初期化する  \n",
    ">2. 繰り返し\n",
    ">    1. $\\delta = 0$\n",
    ">    2. 各状態$s\\in S$について:\n",
    ">        1. v = V(s) \n",
    ">        2. $V(s) = \\max\\limits_{\\alpha \\in A} \\displaystyle\\sum_{s'\\in S}^{} P(s'|s,a)(r(s,a,s') + \\gamma V(s'))$で更新\n",
    ">        3. $\\delta$ = max($\\delta$,|v-V(s)|)\n",
    ">        4. $\\delta$ < 閾値ならば繰り返しを終了\n",
    ">3. $\\pi = arg \\max\\limits_{\\alpha \\in A} \\displaystyle\\sum_{s'\\in S}^{} P(s'|s,a)(r(s,a,s') + \\gamma V(s'))$を出力\n",
    "\n",
    "価値関数の計算が一度で済んでいることがわかると思います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "価値反復関数を実際に実装してみて下さい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "index"
   },
   "outputs": [],
   "source": [
    "from markov import *\n",
    "random.seed(0)\n",
    "\n",
    "# 方策評価をする関数です\n",
    "def policy_evaluation(pi, V, states, epsilon=0.001):\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in states:\n",
    "            v = V.copy()\n",
    "            V[s] = sum([p * (R(s, pi[s], s1) + gamma * V[s1]) for (p, s1) in T(s,pi[s])])\n",
    "            delta = max(delta,abs(v[s] - V[s]))\n",
    "        if  delta < epsilon:\n",
    "            return V\n",
    "        \n",
    "# 空欄を埋めて価値反復関数を実装してください\n",
    "def value_iteration(states, epsilon=0.001):\n",
    "    V = \n",
    "    pi = \n",
    "    V = \n",
    "    for s in states:\n",
    "        action =\n",
    "        pi[s] = \n",
    "    return pi\n",
    "\n",
    "\n",
    "state_transition = np.array(\n",
    "    [[0, 0, 0, 0.3, 5],\n",
    "    [0, 1, 0, 0.7, -10],\n",
    "    [0, 0, 1, 1, 5],\n",
    "    [1, 0, 1, 1, 5],\n",
    "    [1, 2, 0, 0.8, 100],\n",
    "    [1, 1, 0, 0.2, 1]]\n",
    ")\n",
    "\n",
    "states = np.unique(state_transition[:, 0])\n",
    "actlist = np.unique(state_transition[:, 2])\n",
    "terminals = [2]\n",
    "init = [0]\n",
    "gamma = 0.8\n",
    "\n",
    "pi = value_iteration(states,epsilon=0.001)\n",
    "print(pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- 実装は方策反復関数とほとんど同じです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "answer"
   },
   "outputs": [],
   "source": [
    "from markov import *\n",
    "random.seed(0)\n",
    "\n",
    "# 方策評価をする関数です\n",
    "def policy_evaluation(pi, V, states, epsilon=0.001):\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in states:\n",
    "            v = V.copy()\n",
    "            V[s] = sum([p * (R(s, pi[s], s1) + gamma * V[s1]) for (p, s1) in T(s,pi[s])])\n",
    "            delta = max(delta, abs(v[s] - V[s]))\n",
    "        if  delta < epsilon:\n",
    "            return V\n",
    "        \n",
    "# 空欄を埋めて価値反復関数を実装してください\n",
    "def value_iteration(states, epsilon=0.001):\n",
    "    V = {s: 0 for s in np.hstack((states,terminals))}\n",
    "    pi = {s: random.choice(actions(s)) for s in states}\n",
    "    V = policy_evaluation(pi, V, states, epsilon=0.001)\n",
    "    for s in states:\n",
    "        action = sorted(actions(s), key=lambda a:sum([p * gamma * V[s1] + p * R(s,a,s1) for (p, s1) in T(s, a)]), reverse=True)\n",
    "        pi[s] = action[0]\n",
    "    return pi\n",
    "\n",
    "\n",
    "state_transition = np.array(\n",
    "    [[0, 0, 0, 0.3, 5],\n",
    "    [0, 1, 0, 0.7, -10],\n",
    "    [0, 0, 1, 1, 5],\n",
    "    [1, 0, 1, 1, 5],\n",
    "    [1, 2, 0, 0.8, 100],\n",
    "    [1, 1, 0, 0.2, 1]]\n",
    ")\n",
    "\n",
    "states = np.unique(state_transition[:, 0])\n",
    "actlist = np.unique(state_transition[:, 2])\n",
    "terminals = [2]\n",
    "init = [0]\n",
    "gamma = 0.8\n",
    "\n",
    "pi = value_iteration(states,epsilon=0.001)\n",
    "print(pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section_name",
    "sectionId": "S1qb3gkMmIM"
   },
   "source": [
    "## 3.2 TD手法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5040,
    "exerciseId": "rJiW3gkG7UM",
    "id": "code_session_name",
    "important": false,
    "isDL": false,
    "timeoutSecs": 5
   },
   "source": [
    "### 3.2.1 TD手法とは"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "これまで**動的計画法**について学習してきましたが、非常に大きな欠点が存在します。  \n",
    "それは状態遷移確率があらかじめわかっていなければならない点です。状態遷移確率がわかっていない状況下では、chapter1で行なったように実際に集めた報酬から状態遷移確率を推定していく必要があります。そのための手法として **<font color=#AA0000>TD手法</font>** が存在します。TDとはTime Differenceの事で、最終結果を見ずに、現在の推定値を利用して次の推定値を更新していく方法です。今回はTD手法の中で有名な手法として\n",
    "- Sarsa\n",
    "- Q-learning  \n",
    "\n",
    "の二つの学習手法を紹介します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "これから環境2を用意して使用します。<br>\n",
    "<img src=\"https://aidemyexcontentspic.blob.core.windows.net/contents-pic/_RL/img_8.png\">\n",
    "<center>(図:環境2)</center>\n",
    "図のような迷路となっており、以下の条件があります。  \n",
    "\n",
    "\n",
    "\n",
    "- 開始地点 : 図中のstartとあるマス。\n",
    "- 終端地点 : +1または-1とあるマス。\n",
    "- 遷移不可能地点 : 黒いマスは通ることができません。また、マスがない箇所には移動できません。\n",
    "- 移動 : 隣の上下左右に動くことができます。\n",
    "- 状態遷移確率 : 選択した行動に対して、\n",
    "     - 8割が行動した通りに遷移します\n",
    "     - 1割が行動と90度時計回りの方向に遷移します（上を選択すると右に遷移　下を選択すると左に遷移）\n",
    "     - 1割が行動と90度反時計回りの方向に遷移します（上を選択すると左に遷移　下を選択すると右に遷移）\n",
    "- 報酬 : +1,-1のマスはそのまま報酬となり、それ以外のマスは全て-0.04であるとします。\n",
    "\n",
    "\n",
    "    スタート地点から上に２つ進もうとし、右に3つ進もうとした後の位置を出力する関数を作成して下さい。  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "index"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random \n",
    "np.random.seed(0)\n",
    "\n",
    "# 新しくエピソードTを再定義しました\n",
    "def T(state, direction, actions):\n",
    "        return [(0.8, go(state, actions[direction])),\n",
    "                (0.1, go(state, actions[(direction + 1) % 4])),\n",
    "                (0.1, go(state, actions[(direction - 1) % 4]))]\n",
    "\n",
    "# stateの状態を変化させる関数です\n",
    "def go(state, direction):\n",
    "    return [s + d for s,d in zip(state,direction)]\n",
    "\n",
    "# chapter2.3 報酬と収益 で実装した関数を少し変更して使用します\n",
    "def take_single_action(state, direction,actions):\n",
    "    x = np.random.uniform(0,1)\n",
    "    cumulative_probability = 0.0\n",
    "    for probability_state in T(state,direction,actions):\n",
    "        probability, next_state = probability_state\n",
    "        cumulative_probability += probability\n",
    "        if x < cumulative_probability:\n",
    "            break\n",
    "    reward = situation[next_state[0],next_state[1]]\n",
    "    if reward is None:\n",
    "        return state\n",
    "    else:\n",
    "        return next_state\n",
    "\n",
    "# 空欄を埋めて上に２回、右に３回移動する関数を実装してください\n",
    "def move(s):\n",
    "    actions = ((1, 0), (0, 1), (-1, 0), (0, -1)) # up,right,down,left\n",
    "    up = \n",
    "    right = \n",
    "    for _ in range(2):\n",
    "        s1 = \n",
    "        s = \n",
    "    for _ in range(3):\n",
    "        s2 = \n",
    "        s1 = \n",
    "    return s2\n",
    "\n",
    "\n",
    "# 環境を定義します\n",
    "situation = np.array([[None, None, None, None, None, None],\n",
    "                      [None, -0.04, -0.04, -0.04, -0.04, None],\n",
    "                      [None, -0.04, None, -0.04, -1, None],\n",
    "                      [None, -0.04, -0.04, -0.04, +1, None],\n",
    "                      [None, None, None, None, None, None]])\n",
    "            \n",
    "terminals=[(2, 4), (3, 4)]\n",
    "init = [1,1]\n",
    "# 関数を実行します\n",
    "move(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- `take_single_action` 関数は次の移動先を返す関数です。\n",
    "- 繰り返しの動作はfor文を利用して簡略化しましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "answer"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "np.random.seed(0)\n",
    "\n",
    "# 新しくエピソードTを再定義しました\n",
    "def T(state, direction, actions):\n",
    "        return [(0.8, go(state, actions[direction])),\n",
    "                (0.1, go(state, actions[(direction + 1) % 4])),\n",
    "                (0.1, go(state, actions[(direction - 1) % 4]))]\n",
    "\n",
    "# stateの状態を変化させる関数です\n",
    "def go(state, direction):\n",
    "    return [s + d for s, d in zip(state, direction)]\n",
    "\n",
    "# chapter2.3 報酬と収益 で実装した関数を少し変更して使用します\n",
    "def take_single_action(state, direction,actions):\n",
    "    x = np.random.uniform(0, 1)\n",
    "    cumulative_probability = 0.0\n",
    "    for probability_state in T(state, direction, actions):\n",
    "        probability, next_state = probability_state\n",
    "        cumulative_probability += probability\n",
    "        if x < cumulative_probability:\n",
    "            break\n",
    "    reward = situation[next_state[0], next_state[1]]\n",
    "    if reward is None:\n",
    "        return state\n",
    "    else:\n",
    "        return next_state\n",
    "\n",
    "# 空欄を埋めて上に２回、右に３回移動する関数を実装してください\n",
    "def move(s):\n",
    "    actions = ((1, 0), (0, 1), (-1, 0), (0, -1)) # up,right,down,left\n",
    "    up = 0\n",
    "    right = 1\n",
    "    for _ in range(2):\n",
    "        s1 = take_single_action(s, up, actions)\n",
    "        s = s1\n",
    "    for _ in range(3):\n",
    "        s2 = take_single_action(s1, right, actions)\n",
    "        s1 = s2\n",
    "    return s2\n",
    "\n",
    "\n",
    "# 環境を定義します\n",
    "situation = np.array([[None, None, None, None, None, None],\n",
    "                      [None, -0.04, -0.04, -0.04, -0.04, None],\n",
    "                      [None, -0.04, None, -0.04, -1, None],\n",
    "                      [None, -0.04, -0.04, -0.04, +1, None],\n",
    "                      [None, None, None, None, None, None]])\n",
    "            \n",
    "terminals=[(2, 4), (3, 4)]\n",
    "init = [1,1]\n",
    "# 関数を実行します\n",
    "move(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5040,
    "exerciseId": "rJnb2gJzQLz",
    "id": "code_session_name",
    "important": false,
    "isDL": false,
    "timeoutSecs": 5
   },
   "source": [
    "### 3.2.2 Sarsa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "**Sarsa手法**とはベルマン方程式を試行錯誤しながら解いていくアルゴリズムの一つです。  \n",
    "\n",
    "ここからはV(s)の代わりに行動価値関数Q(s,a)を使用します。以下のようなアルゴリズムで更新していきます。\n",
    "\n",
    ">1. Q(s,a)を初期化\n",
    ">2. エピソードについて繰り返し:\n",
    ">   1. sを初期化 \n",
    ">   2. Q関数から導かれる方策($\\epsilon$-greedy手法を使用しましょう)を使用して行動を決定する\n",
    ">   3. エピソード中の時間ステップについて繰り返し:\n",
    ">       1. 行動aをして遷移した状態s',報酬rを観測する\n",
    ">       2. Q関数から導かれる方策($\\epsilon$-greedy手法を使用しましょう)を使用して次の行動a'を決定する\n",
    ">       3. $Q(s,a) += \\alpha[r + \\gamma Q(s',a') - Q(s,a)]$で更新する\n",
    ">       4. 状態sに遷移した状態s'、行動aに次の行動a'を代入する\n",
    ">   4. sが終端になれば繰り返しを終了\n",
    ">3. 予め決めていた目標も満たせばエピソードの繰り返し終了\n",
    "\n",
    "$\\alpha$は学習率で、事前に設定されるパラメーターです。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "\n",
    "環境2においてエージェントはstartにいることを考えます。 <br>\n",
    "ランダムに行動を決定してその時のエピソードの報酬を求めてください。 <br>\n",
    "- 行動は常に上、右、下、左が選択できるものとします。\n",
    "- 行動先に壁や障害物があり移動できない場合は、その場にとどまることとし報酬は-0.04になります。\n",
    "- エピソードの終了条件は報酬が0.7を超えるエピソードが5回連続で続いたときとします。\n",
    "- エピソード繰り返し数の最大は30回、時間ステップ繰り返し数の最大は100回とします。\n",
    "\n",
    " **<font color=#AA0000>今回はランダムに行動しているのでまだQは使用しません。</font>** \n",
    "\n",
    "\n",
    "<img src=\"https://aidemyexcontentspic.blob.core.windows.net/contents-pic/_RL/img_8.png\">\n",
    "<center>(図:環境2)</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "index"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "def T(state, direction, actions):\n",
    "        return [(0.8, go(state, actions[direction])),\n",
    "                (0.1, go(state, actions[(direction + 1) % 4])),\n",
    "                (0.1, go(state, actions[(direction - 1) % 4]))]\n",
    "\n",
    "def go(state, direction):\n",
    "    return [s + d for s, d in zip(state, direction)]\n",
    "\n",
    "# ランダムに次の行動を決定してください\n",
    "def get_action(t_state, episode):\n",
    "    # 解答を書いてください\n",
    "    next_action = \n",
    "    return next_action\n",
    "\n",
    "\n",
    "# chapter2.3 報酬と収益 で実装した関数を少し変更して使用します\n",
    "# 返し値にrewardが追加されました\n",
    "def take_single_action(state, direction,actions):\n",
    "    x = np.random.uniform(0, 1)\n",
    "    cumulative_probability = 0.0\n",
    "    for probability_state in T(state,direction,actions):\n",
    "        probability, next_state = probability_state\n",
    "        cumulative_probability += probability\n",
    "        if x < cumulative_probability:\n",
    "            break\n",
    "    reward = situation[next_state[0],next_state[1]]\n",
    "    if reward is None:\n",
    "        return state, -0.04\n",
    "    else:\n",
    "        return next_state, reward\n",
    "\n",
    "\n",
    "num_episodes = 30\n",
    "max_steps = 100\n",
    "total_reward = np.zeros(5)\n",
    "goal_average_reward = 0.7\n",
    "\n",
    "# 環境を定義します\n",
    "situation = np.array([[None, None, None, None, None, None],\n",
    "                      [None, -0.04, -0.04, -0.04, -0.04, None],\n",
    "                      [None, -0.04, None, -0.04, -1, None],\n",
    "                      [None, -0.04, -0.04, -0.04, +1,None],\n",
    "                      [None, None, None, None, None, None]])\n",
    "            \n",
    "\n",
    "terminals=[[2, 4], [3, 4]]\n",
    "init = [1, 1]\n",
    "actions = ((1, 0), (0, 1), (-1, 0), (0, -1))\n",
    "\n",
    "\n",
    "# エピソードの繰り返しを定義します\n",
    "for episode in range(num_episodes):\n",
    "    state = init\n",
    "    # エピソードの繰り返しを定義します\n",
    "    action = get_action(state, episode)\n",
    "    episode_reward = 0\n",
    "\n",
    "    # 時間ステップのループを定義します\n",
    "    for t in range(max_steps):\n",
    "        # 空欄を埋めて時間ステップ中の振る舞いを実装してください\n",
    "        next_state, reward = \n",
    "        episode_reward += \n",
    "        next_action = \n",
    "        state = \n",
    "        action = \n",
    "        if state in terminals :\n",
    "            break\n",
    "            \n",
    "    #報酬を記録        \n",
    "    total_reward = np.hstack((total_reward[1:],episode_reward))\n",
    "    print(total_reward)\n",
    "    print(\"Episode %d has finished. t=%d\" %(episode+1, t+1))\n",
    "    # 直近の100エピソードが規定報酬以上であれば成功\n",
    "    if (min(total_reward) >= goal_average_reward):\n",
    "        print('Episode %d train agent successfuly! t=%d' %(episode,t))\n",
    "        break \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- `get_action` 関数と時間ステップに対応する部分の空欄を埋めて、コードを完成させてください。\n",
    "- `get_action` では０〜３のどれかをランダムに返すようにコーディングしてください。\n",
    "- 時間ステップでは次の状態、行動、エピソード中における報酬の合計を計算してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "answer"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "def T(state, direction, actions):\n",
    "        return [(0.8, go(state, actions[direction])),\n",
    "                (0.1, go(state, actions[(direction + 1) % 4])),\n",
    "                (0.1, go(state, actions[(direction - 1) % 4]))]\n",
    "\n",
    "def go(state, direction):\n",
    "    return [s + d for s, d in zip(state, direction)]\n",
    "\n",
    "# ランダムに次の行動を決定してください\n",
    "def get_action(state, episode):\n",
    "    next_action = np.random.choice(len(actions))\n",
    "    return next_action\n",
    "\n",
    "\n",
    "# chapter2.3 報酬と収益 で実装した関数を少し変更して使用します\n",
    "# 返し地にrewardが追加されました\n",
    "def take_single_action(state, direction, actions):\n",
    "    x = np.random.uniform(0, 1)\n",
    "    cumulative_probability = 0.0\n",
    "    for probability_state in T(state, direction, actions):\n",
    "        probability, next_state = probability_state\n",
    "        cumulative_probability += probability\n",
    "        if x < cumulative_probability:\n",
    "            break\n",
    "    reward = situation[next_state[0], next_state[1]]\n",
    "    if reward is None:\n",
    "        return state, -0.04\n",
    "    else:\n",
    "        return next_state, reward\n",
    "\n",
    "\n",
    "num_episodes = 30\n",
    "max_steps = 100\n",
    "total_reward = np.zeros(5)\n",
    "goal_average_reward = 0.7\n",
    "\n",
    "# 環境を定義します\n",
    "situation = np.array([[None, None, None, None, None, None],\n",
    "                      [None, -0.04, -0.04, -0.04, -0.04, None],\n",
    "                      [None, -0.04, None, -0.04, -1, None],\n",
    "                      [None, -0.04, -0.04, -0.04, +1, None],\n",
    "                      [None, None, None, None, None, None]])\n",
    "            \n",
    "\n",
    "terminals=[[2, 4], [3, 4]]\n",
    "init = [1,1]\n",
    "actions = ((1,0),(0,1),(-1,0),(0,-1))\n",
    "\n",
    "\n",
    "# エピソードの繰り返しを定義します\n",
    "for episode in range(num_episodes): \n",
    "    state = init\n",
    "    # はじめの行動を決定します\n",
    "    action = get_action(state, episode)\n",
    "    episode_reward = 0\n",
    "\n",
    "    # 時間ステップのループを定義します\n",
    "    for t in range(max_steps):\n",
    "        # 空欄を埋めて時間ステップ中の振る舞いを実装してください\n",
    "        next_state, reward = take_single_action(state, action, actions)\n",
    "        episode_reward += reward\n",
    "        next_action = get_action(state, episode)\n",
    "        state = next_state\n",
    "        action = next_action\n",
    "        if state in terminals :\n",
    "            break\n",
    "            \n",
    "    #報酬を記録        \n",
    "    total_reward = np.hstack((total_reward[1:],episode_reward))\n",
    "    print(total_reward)\n",
    "    print(\"Episode %d has finished. t=%d\" %(episode+1, t+1))\n",
    "    # 直近の100エピソードが規定報酬以上であれば成功\n",
    "    if (min(total_reward) >= goal_average_reward):\n",
    "        print('Episode %d train agent successfuly! t=%d' %(episode,t))\n",
    "        break \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5040,
    "exerciseId": "S16bnxkz78M",
    "id": "code_session_name",
    "important": false,
    "isDL": false,
    "timeoutSecs": 5
   },
   "source": [
    "### 3.2.3 SarsaにおけるQ関数の実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "次は実際に**Q関数**を実装していきます。 <br>\n",
    "**Q関数**は　[全ての状態×全ての行動]　の配列で表していきます。全ての状態ですが、環境2では取りうる状態は11個(11マスのどこかにいる)、行動は上下左右の4種類です。そこで11×4の配列にしたいのですが、実際の状態は座標で表されておりうまく数字を割り振ることができません。\n",
    "そこで状態の座標の、<br>\n",
    "$$ x座標 × 10 + y座標 × 1 $$\n",
    "をqのインデックスに対応させることとします。 <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "環境2においてエージェントはstartにいることを考えます。 <br>\n",
    "Sarsaにてエピソードの報酬を求めてください。 <br>\n",
    "\n",
    "- 行動の仕方はq関数の値が最も大きいものを採用するとしてください。\n",
    "- q関数では通常の `state` とは別にq関数用に値を変換した `t_state` という変数を使用します。\n",
    "- 行動は常に上、右、下、左が選択できるものとします。\n",
    "- 行動先に壁や障害物があり移動できない場合は、その場にとどまることとし報酬は-0.04になります。\n",
    "- エピソードの終了条件は報酬が0.7を超えるエピソードが5回連続で続いたときとします。\n",
    "- エピソード繰り返し数の最大は30回、時間ステップ繰り返し数の最大は100回とします。\n",
    "\n",
    "q関数更新の際の$\\alpha$と$\\gamma$は0.4と0.8とします。 <br>\n",
    "\n",
    "\n",
    "<img src=\"https://aidemyexcontentspic.blob.core.windows.net/contents-pic/_RL/img_8.png\">\n",
    "\n",
    "<center>(図:環境2)</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "index"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "def T(state, direction, actions):\n",
    "        return [(0.8, go(state, actions[direction])),\n",
    "                (0.1, go(state, actions[(direction + 1) % 4])),\n",
    "                (0.1, go(state, actions[(direction - 1) % 4]))]\n",
    "\n",
    "def go(state, direction):\n",
    "    return [s+d for s,d in zip(state,direction)]\n",
    "\n",
    "# 次の行動を行動価値関数に基づいて決定する関数を実装してください\n",
    "def get_action(t_state, episode):\n",
    "    # 解答を書いてください\n",
    "    next_action = \n",
    "    return next_action\n",
    "\n",
    "\n",
    "def take_single_action(state, direction,actions):\n",
    "    x = np.random.uniform(0,1)\n",
    "    cumulative_probability = 0.0\n",
    "    for probability_state in T(state,direction,actions):\n",
    "        probability, next_state = probability_state\n",
    "        cumulative_probability += probability\n",
    "        if x < cumulative_probability:\n",
    "            break\n",
    "    reward = situation[next_state[0],next_state[1]]\n",
    "    if reward is None:\n",
    "        return state, -0.04\n",
    "    else:\n",
    "        return next_state, reward\n",
    "\n",
    "\n",
    "# 行動価値関数の更新を行う関数を実装してください\n",
    "def update_qtable(q_table, t_state, action, reward, t_next_state, next_action):\n",
    "    gamma = 0.8\n",
    "    alpha = 0.4\n",
    "    q_table[t_state, action] += \n",
    "     \n",
    "    return q_table\n",
    "\n",
    "# stateを行動価値関数で使用できるように変換する関数を実装してください\n",
    "def trans_state(state):\n",
    "    return \n",
    "\n",
    "# 行動価値関数を初期化します\n",
    "q_table = np.random.uniform(\n",
    "    low=-0.01, high=0.01, size=(10 ** 2, 4))\n",
    "\n",
    "num_episodes = 500\n",
    "max_number_of_steps = 1000\n",
    "total_reward_vec = np.zeros(5)\n",
    "goal_average_reward = 0.7\n",
    "\n",
    "# 環境を定義します\n",
    "situation = np.array([[None, None, None, None, None, None],\n",
    "                      [None, -0.04, -0.04, -0.04, -0.04, None],\n",
    "                      [None, -0.04, None, -0.04, -1,None],\n",
    "                      [None, -0.04, -0.04, -0.04, +1,None],\n",
    "                      [None, None, None, None, None, None]])\n",
    "            \n",
    "\n",
    "terminals=[[2, 4], [3, 4]]\n",
    "init = [1,1]\n",
    "actions = ((1, 0), (0, 1), (-1, 0), (0, -1))\n",
    "\n",
    "\n",
    "for episode in range(num_episodes):  \n",
    "    state = init\n",
    "    # stateは方向を計算するための変数でt_stateはQ関数に渡すための変数になります\n",
    "    t_state = trans_state(state)\n",
    "    action = get_action(t_state, episode)\n",
    "    episode_reward = 0\n",
    "\n",
    "    for t in range(max_number_of_steps): \n",
    "        next_state, reward = take_single_action(state, action, actions)\n",
    "        episode_reward += reward \n",
    "        next_action = get_action(t_state, episode)\n",
    "        t_next_state = trans_state(next_state)\n",
    "        # 行動価値関数を更新してください\n",
    "        q_table = \n",
    "        state = next_state\n",
    "        t_state = trans_state(state)\n",
    "        action = next_action\n",
    "        \n",
    "        if state in terminals :\n",
    "            break\n",
    "    total_reward_vec = np.hstack((total_reward_vec[1:],episode_reward))\n",
    "    print(total_reward_vec)\n",
    "    print(\"Episode %d has finished. t=%d\" %(episode+1, t+1))\n",
    "    print(min(total_reward_vec),goal_average_reward)\n",
    "    if (min(total_reward_vec) >= goal_average_reward): \n",
    "        print('Episode %d train agent successfuly! t=%d' %(episode, t))\n",
    "        break \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- `update_qtable` 関数での更新方法は上のアルゴリズムを参照してください。\n",
    "- `trans_state` 関数では座標を数字に変換する関数を実装します。数字のみを返すようにしてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "answer"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "def T(state, direction, actions):\n",
    "        return [(0.8, go(state, actions[direction])),\n",
    "                (0.1, go(state, actions[(direction + 1) % 4])),\n",
    "                (0.1, go(state, actions[(direction - 1) % 4]))]\n",
    "\n",
    "def go(state, direction):\n",
    "    return [s+d for s, d in zip(state, direction)]\n",
    "\n",
    "# 次の行動を行動価値関数に基づいて決定する関数を実装してください\n",
    "def get_action(t_state, episode):\n",
    "    next_action = np.argmax(q_table[t_state])\n",
    "    return next_action\n",
    "\n",
    "\n",
    "def take_single_action(state, direction,actions):\n",
    "    x = np.random.uniform(0,1)\n",
    "    cumulative_probability = 0.0\n",
    "    for probability_state in T(state,direction,actions):\n",
    "        probability, next_state = probability_state\n",
    "        cumulative_probability += probability\n",
    "        if x < cumulative_probability:\n",
    "            break\n",
    "    reward = situation[next_state[0],next_state[1]]\n",
    "    if reward is None:\n",
    "        return state, -0.04\n",
    "    else:\n",
    "        return next_state, reward\n",
    "\n",
    "\n",
    "# 行動価値関数の更新を行う関数を実装してください\n",
    "def update_qtable(q_table, t_state, action, reward, t_next_state, next_action):\n",
    "    gamma = 0.8\n",
    "    alpha = 0.4\n",
    "    q_table[t_state, action] += alpha * (reward + gamma * q_table[t_next_state,next_action] - q_table[t_state,action])\n",
    "    return q_table\n",
    "\n",
    "# stateを行動価値関数で使用できるように変換する関数を実装してください\n",
    "def trans_state(state):\n",
    "    return sum([n*(10**i) for i, n in enumerate(state)])\n",
    "\n",
    "# 行動価値関数を初期化します\n",
    "q_table = np.random.uniform(\n",
    "    low=-0.01, high=0.01, size=(10 ** 2, 4))\n",
    "\n",
    "num_episodes = 500\n",
    "max_number_of_steps = 1000\n",
    "total_reward_vec = np.zeros(5)\n",
    "goal_average_reward = 0.7\n",
    "\n",
    "# 環境を定義します\n",
    "situation = np.array([[None, None, None, None, None, None],\n",
    "                      [None, -0.04, -0.04, -0.04, -0.04, None],\n",
    "                      [None, -0.04, None, -0.04, -1,None],\n",
    "                      [None, -0.04, -0.04, -0.04, +1,None],\n",
    "                      [None, None, None, None, None, None]])\n",
    "            \n",
    "\n",
    "terminals=[[2, 4], [3, 4]]\n",
    "init = [1,1]\n",
    "actions = ((1, 0), (0, 1), (-1, 0), (0, -1))\n",
    "\n",
    "\n",
    "for episode in range(num_episodes):  \n",
    "    state = init\n",
    "    # stateは方向を計算するための変数でt_stateはQ関数に渡すための変数になります\n",
    "    t_state = trans_state(state)\n",
    "    action = get_action(t_state, episode)\n",
    "    episode_reward = 0\n",
    "\n",
    "    for t in range(max_number_of_steps): \n",
    "        next_state, reward = take_single_action(state, action, actions)\n",
    "        episode_reward += reward\n",
    "        next_action = get_action(t_state, episode)\n",
    "        t_next_state = trans_state(next_state)\n",
    "        # 行動価値関数を更新してください\n",
    "        q_table = update_qtable(q_table, t_state, action, reward, t_next_state, next_action)\n",
    "        state = next_state\n",
    "        t_state = trans_state(state)\n",
    "        action = next_action\n",
    "        \n",
    "        if state in terminals :\n",
    "            break\n",
    "    total_reward_vec = np.hstack((total_reward_vec[1:], episode_reward)) \n",
    "    print(total_reward_vec)\n",
    "    print(\"Episode %d has finished. t=%d\" %(episode+1, t+1))\n",
    "    print(min(total_reward_vec),goal_average_reward)\n",
    "    if (min(total_reward_vec) >= goal_average_reward):\n",
    "        print('Episode %d train agent successfuly! t=%d' %(episode, t))\n",
    "        break \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5040,
    "exerciseId": "SyCZ2l1zX8G",
    "id": "code_session_name",
    "important": false,
    "isDL": false,
    "timeoutSecs": 5
   },
   "source": [
    "### 3.2.4 Sarsaでのε-greedy 手法の実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "先程の例ではQ関数の値が大きいもののみを選択していました。<br>\n",
    "ただこれではより優れた方法を見逃してしまう恐れがあります。そこで **$\\epsilon$-greedy手法** を用いることによって新しい手法を探索していきます。\n",
    "chapter1で行った$\\epsilon$-greedy手法とは少し変えて、$\\epsilon$の値をepisodeの早い段階では探索の割合を大きくし、episodeが進むに連れ探索を狭めていきます。今回は、 <br>\n",
    "$$\\epsilon = 0.5 * (1 / ( episode + 1)) $$\n",
    "で計算してみましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "環境2においてエージェントはstartにいることを考えます。 <br>\n",
    "Sarsaにてエピソードの報酬を求めてください。 <br>\n",
    "\n",
    "- 行動の仕方は$\\epsilon$-greedy手法を利用したq関数の値が最も大きいものを採用するとしてください。\n",
    "- エピソード繰り返し数の最大は200回、時間ステップ繰り返し数の最大は100回とします。\n",
    "- 行動は常に上、右、下、左が選択できるものとします。\n",
    "- 行動先に壁や障害物があり移動できない場合は、その場にとどまることとし報酬は-0.04になります。\n",
    "- エピソードの終了条件は報酬が0.7を超えるエピソードが5回連続で続いたときとします。\n",
    "\n",
    "<img src=\"https://aidemyexcontentspic.blob.core.windows.net/contents-pic/_RL/img_8.png\">\n",
    "<center>(図:環境2)</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "index"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "def T(state, direction, actions):\n",
    "        return [(0.8, go(state, actions[direction])),\n",
    "                (0.1, go(state, actions[(direction + 1) % 4])),\n",
    "                (0.1, go(state, actions[(direction - 1) % 4]))]\n",
    "\n",
    "\n",
    "def go(state, direction):\n",
    "    return [s+d for s, d in zip(state, direction)]\n",
    "\n",
    "# 空欄を埋めてepsilon-greedy手法により次の行動を決定してください\n",
    "def get_action(t_state, episode):\n",
    "    epsilon = \n",
    "    if epsilon <= \n",
    "        next_action = \n",
    "    else:\n",
    "        next_action = \n",
    "    return next_action\n",
    "\n",
    "\n",
    "\n",
    "def take_single_action(state, direction,actions):\n",
    "    x = np.random.uniform(0,1)\n",
    "    cumulative_probability = 0.0\n",
    "    for probability_state in T(state,direction,actions):\n",
    "        probability, next_state = probability_state\n",
    "        cumulative_probability += probability\n",
    "        if x < cumulative_probability:\n",
    "            break\n",
    "    reward = situation[next_state[0], next_state[1]]\n",
    "    if reward is None:\n",
    "        return state, -0.04\n",
    "    else:\n",
    "        return next_state, reward\n",
    "\n",
    "\n",
    "\n",
    "def update_Qtable(q_table, t_state, action, reward, t_next_state, next_action):\n",
    "    gamma = 0.8\n",
    "    alpha = 0.4\n",
    "    q_table[t_state, action] += alpha * (reward + gamma * q_table[t_next_state, next_action] - q_table[t_state, action])\n",
    "    return q_table\n",
    "\n",
    "def trans_state(state):\n",
    "    return [n*(11**i) for i, n in enumerate(state)][0]\n",
    "\n",
    "q_table = np.random.uniform(\n",
    "    low=-0.01, high=0.01, size=(10 ** 2, 4))\n",
    "\n",
    "num_episodes = 500\n",
    "max_number_of_steps = 1000\n",
    "total_reward_vec = np.zeros(5)\n",
    "goal_average_reward = 0.7\n",
    "\n",
    "situation = np.array([[None, None, None, None, None, None],\n",
    "                      [None, -0.04, -0.04, -0.04, -0.04, None],\n",
    "                      [None, -0.04, None, -0.04, -1,None],\n",
    "                      [None, -0.04, -0.04, -0.04, +1,None],\n",
    "                      [None, None, None, None, None, None]])\n",
    "            \n",
    "\n",
    "terminals=[[2, 4], [3, 4]]\n",
    "init = [1,1]\n",
    "actions = ((1,0),(0,1),(-1,0),(0,-1))\n",
    "state = [n*(11**i) for i, n in enumerate(init)]\n",
    "\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = init\n",
    "    t_state = trans_state(state)\n",
    "    action = np.argmax(q_table[t_state])\n",
    "    episode_reward = 0\n",
    "\n",
    "    for t in range(max_number_of_steps): \n",
    "        next_state, reward = take_single_action(state, action, actions)\n",
    "        episode_reward += reward  \n",
    "        next_action = get_action(t_state, episode)\n",
    "        t_next_state = trans_state(next_state)\n",
    "        q_table = update_Qtable(q_table, t_state, action, reward, t_next_state, next_action)\n",
    "        state = next_state\n",
    "        t_state = trans_state(state)\n",
    "        action = next_action\n",
    "        \n",
    "        if state in terminals :\n",
    "            break\n",
    "    total_reward_vec = np.hstack((total_reward_vec[1:],episode_reward))  \n",
    "    print(total_reward_vec)\n",
    "    print(\"Episode %d has finished. t=%d\" %(episode+1, t+1))\n",
    "    print(min(total_reward_vec),goal_average_reward)\n",
    "    if (min(total_reward_vec) >= goal_average_reward): \n",
    "        print('Episode %d train agent successfuly! t=%d' %(episode, t))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- $\\epsilon$の値は $\\epsilon$ = $0.5*\\frac{1}{episode + 1}$ になります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "answer"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "def T(state, direction, actions):\n",
    "        return [(0.8, go(state, actions[direction])),\n",
    "                (0.1, go(state, actions[(direction + 1) % 4])),\n",
    "                (0.1, go(state, actions[(direction - 1) % 4]))]\n",
    "\n",
    "\n",
    "def go(state, direction):\n",
    "    return [s+d for s, d in zip(state, direction)]\n",
    "\n",
    "# 空欄を埋めてepsilon-greedy手法により次の行動を決定してください\n",
    "def get_action(t_state, episode):\n",
    "    epsilon = 0.5 * (1 / (episode + 1))\n",
    "    if epsilon <= np.random.uniform(0, 1):\n",
    "        next_action = np.argmax(q_table[t_state])\n",
    "    else:\n",
    "        next_action = np.random.choice(len(actions))\n",
    "    return next_action\n",
    "\n",
    "\n",
    "\n",
    "def take_single_action(state, direction, actions):\n",
    "    x = np.random.uniform(0, 1)\n",
    "    cumulative_probability = 0.0\n",
    "    for probability_state in T(state, direction, actions):\n",
    "        probability, next_state = probability_state\n",
    "        cumulative_probability += probability\n",
    "        if x < cumulative_probability:\n",
    "            break\n",
    "    reward = situation[next_state[0], next_state[1]]\n",
    "    if reward is None:\n",
    "        return state, -0.04\n",
    "    else:\n",
    "        return next_state, reward\n",
    "\n",
    "\n",
    "\n",
    "def update_Qtable(q_table, t_state, action, reward, t_next_state, next_action):\n",
    "    gamma = 0.8\n",
    "    alpha = 0.4\n",
    "    q_table[t_state, action] += alpha * (reward + gamma * q_table[t_next_state, next_action] - q_table[t_state, action])\n",
    "    return q_table\n",
    "\n",
    "def trans_state(state):\n",
    "    return sum([n*(10**i) for i, n in enumerate(state)])\n",
    "\n",
    "q_table = np.random.uniform(\n",
    "    low=-0.01, high=0.01, size=(10 ** 2, 4))\n",
    "\n",
    "num_episodes = 500\n",
    "max_number_of_steps = 1000\n",
    "total_reward_vec = np.zeros(5)\n",
    "goal_average_reward = 0.7\n",
    "\n",
    "situation = np.array([[None, None, None, None, None, None],\n",
    "                      [None, -0.04, -0.04, -0.04, -0.04, None],\n",
    "                      [None, -0.04, None, -0.04, -1, None],\n",
    "                      [None, -0.04, -0.04, -0.04, +1, None],\n",
    "                      [None, None, None, None, None, None]])\n",
    "            \n",
    "\n",
    "terminals=[[2, 4], [3, 4]]\n",
    "init = [1,1]\n",
    "actions = ((1, 0), (0, 1), (-1, 0), (0, -1))\n",
    "state = [n*(10**i) for i,n in enumerate(init)]\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = init\n",
    "    t_state = trans_state(state)\n",
    "    action = np.argmax(q_table[t_state])\n",
    "    episode_reward = 0\n",
    "\n",
    "    for t in range(max_number_of_steps): \n",
    "        next_state, reward = take_single_action(state, action, actions)\n",
    "        episode_reward += reward  \n",
    "        next_action = get_action(t_state, episode)\n",
    "        t_next_state = trans_state(next_state)\n",
    "        q_table = update_Qtable(q_table, t_state, action, reward, t_next_state, next_action)\n",
    "        state = next_state\n",
    "        t_state = trans_state(state)\n",
    "        action = next_action\n",
    "        \n",
    "        if state in terminals :\n",
    "            break\n",
    "    total_reward_vec = np.hstack((total_reward_vec[1:],episode_reward))  \n",
    "    print(total_reward_vec)\n",
    "    print(\"Episode %d has finished. t=%d\" %(episode+1, t+1))\n",
    "    print(min(total_reward_vec),goal_average_reward)\n",
    "    if (min(total_reward_vec) >= goal_average_reward): \n",
    "        print('Episode %d train agent successfuly! t=%d' %(episode, t))\n",
    "        break "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5040,
    "exerciseId": "rkyGhekMmIG",
    "id": "quiz_session_name",
    "important": false,
    "isDL": false,
    "timeoutSecs": 10
   },
   "source": [
    "### 3.2.5 Q学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "別の有名なTD手法として**Q学習**があります。 <br>\n",
    "Q学習も全体の流れはSarsaと変わりなく、 <br>\n",
    "1. q関数の更新を行ったあとに次の行動を決定すること\n",
    "2. q関数の更新の方法  \n",
    "\n",
    "のみが異なります。\n",
    "\n",
    "流れとしては、\n",
    ">1. Q(s,a)を初期化\n",
    ">2. エピソードについて繰り返し:\n",
    ">   1. sを初期化 \n",
    ">   2. Q関数から導かれる方策($\\epsilon$-greedy手法を使用しましょう)を使用して行動を決定する\n",
    ">   3. エピソード中の時間ステップについて繰り返し:\n",
    ">       1. 行動aをして遷移した状態s',報酬rを観測する\n",
    ">       2. $Q(s,a) +=  \\alpha [r + \\gamma \\max\\limits_{a'\\in A} Q(s',a')-Q(s,a)]$で更新する\n",
    ">       3. Q関数から導かれる方策($\\epsilon$-greedy手法を使用しましょう)を使用して次の行動a'を決定する\n",
    ">       4. 状態sに遷移した状態s'、行動aに次の行動a'を代入する\n",
    ">   4. sが終端になれば繰り返しを終了\n",
    ">3. 予め決めていた目標も満たせばエピソードの繰り返し終了\n",
    "\n",
    "$\\epsilon$の値は $\\epsilon$ = $0.5*\\frac{1}{episode + 1}$ になります。\n",
    "\n",
    "$\\alpha$は学習率を示しています。 <br>\n",
    "\n",
    "流れを見てもSarsaとほとんど変わりないことがわかっていただけると思います。 <br>\n",
    "\n",
    "Sarsaではq関数を実際次に行う行動を使って更新していましたが、Q学習では次に考えられる行動のうち最もq関数が大きいものを採用します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "SarsaとQ学習の説明として適切なものを選択してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "choices"
   },
   "source": [
    "- SarsaはTD手法のため状態遷移確率が必要ないが、Q学習では必要である。\n",
    "- SersaとQ学習において異なるのはq関数の更新の方法のみである。\n",
    "- Q学習では更新の際に使用した行動と、実際に行う行動が異なることがある。\n",
    "- 上の3つの選択肢の中に適切なものは存在しない。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- Q学習では次の行動を決定する前にQ関数の更新を行います。<br>\n",
    "そのため更新前は最適だと思われた行動も、更新後には他の行動が最適になることがあります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "answer"
   },
   "source": [
    "- Q学習では更新の際に使用した行動と、実際に行う行動が異なることがある。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "chapter_exam"
   },
   "source": [
    "## 添削問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "環境2においてエージェントはstartにいることを考えます。 <br>\n",
    "Q学習を用いてエピソードの報酬を求めてください。 <br>\n",
    "\n",
    "- 行動の仕方はq関数の値が最も大きいものを採用するとしてください。\n",
    "- q関数では通常のstateとは別にq関数用に値を変換したt_stateという変数を使用します。\n",
    "- 行動は常に上、右、下、左が選択できるものとします。\n",
    "- 行動先に壁や障害物があり移動できない場合は、その場にとどまることとし報酬は-0.04になります。\n",
    "- エピソードの終了条件は報酬が0.7を超えるエピソードが5回連続で続いたときとします。\n",
    "- エピソード繰り返し数の最大は30回、時間ステップ繰り返し数の最大は100回とします。\n",
    "\n",
    "q関数更新の際の$\\alpha$と$\\gamma$は0.4と0.8とします。 <br>\n",
    "\n",
    "<img src=\"https://aidemyexcontentspic.blob.core.windows.net/contents-pic/_RL/img_8.png\">\n",
    "<center>(図:環境2)</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "index"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "def T(state, direction, actions):\n",
    "        return [(0.8, go(state, actions[direction])),\n",
    "                (0.1, go(state, actions[(direction + 1) % 4])),\n",
    "                (0.1, go(state, actions[(direction - 1) % 4]))]\n",
    "\n",
    "\n",
    "def go(state, direction):\n",
    "    return [s+d for s, d in zip(state, direction)]\n",
    "\n",
    "def get_action(t_state, episode):\n",
    "    epsilon = 0.5 * (1 / (episode + 1))\n",
    "    if epsilon <= np.random.uniform(0, 1):\n",
    "        next_action = np.argmax(q_table[t_state])\n",
    "    else:\n",
    "        next_action = np.random.choice(len(actions))\n",
    "    return next_action\n",
    "\n",
    "\n",
    "def take_single_action(state, direction, actions):\n",
    "    x = np.random.uniform(0,1)\n",
    "    cumulative_probability = 0.0\n",
    "    for probability_state in T(state, direction, actions):\n",
    "        probability, next_state = probability_state\n",
    "        cumulative_probability += probability\n",
    "        if x < cumulative_probability:\n",
    "            break\n",
    "    reward = situation[next_state[0], next_state[1]]\n",
    "    if reward is None:\n",
    "        return state, -0.04\n",
    "    else:\n",
    "        return next_state, reward\n",
    "\n",
    "\n",
    "\n",
    "def update_Qtable(q_table, t_state, action, reward, t_next_state, next_action):\n",
    "    gamma = 0.8\n",
    "    alpha = 0.4\n",
    "    # Q学習の方法でq関数を完成させてください。\n",
    "    \n",
    "    \n",
    "    return q_table\n",
    "\n",
    "def trans_state(state):\n",
    "    return sum([n*(10**i) for i, n in enumerate(state)])\n",
    "\n",
    "q_table = np.random.uniform(\n",
    "    low=-0.01, high=0.01, size=(10 ** 2, 4))\n",
    "\n",
    "num_episodes = 500\n",
    "max_number_of_steps = 1000\n",
    "total_reward_vec = np.zeros(5)\n",
    "goal_average_reward = 0.7\n",
    "\n",
    "situation = np.array([[None, None, None, None, None, None],\n",
    "                      [None, -0.04, -0.04, -0.04, -0.04, None],\n",
    "                      [None, -0.04, None, -0.04, -1,None],\n",
    "                      [None, -0.04, -0.04, -0.04, +1,None],\n",
    "                      [None, None, None, None, None, None]])\n",
    "            \n",
    "\n",
    "terminals=[[2, 4], [3, 4]]\n",
    "init = [1,1]\n",
    "actions = ((1, 0), (0, 1), (-1, 0), (0, -1))\n",
    "state = [n*(10**i) for i, n in enumerate(init)]\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = init\n",
    "    t_state = trans_state(state)\n",
    "    action = np.argmax(q_table[t_state])\n",
    "    episode_reward = 0\n",
    "    \n",
    "    # 以下の空欄を埋めてコードを完成させてください。\n",
    "    for t in range(max_number_of_steps): \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        if state in terminals :\n",
    "            break\n",
    "    total_reward_vec = np.hstack((total_reward_vec[1:],episode_reward))  \n",
    "    print(total_reward_vec)\n",
    "    print(\"Episode %d has finished. t=%d\" %(episode+1, t+1))\n",
    "    print(min(total_reward_vec),goal_average_reward)\n",
    "    if (min(total_reward_vec) >= goal_average_reward): \n",
    "        print('Episode %d train agent successfuly! t=%d' %(episode, t))\n",
    "        break \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- Q学習ではq関数の更新を行ったあとに、次の行動を決定します。\n",
    "- q関数の更新にはまずstateにおけるq関数の最大値を求めてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "answer"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "def T(state, direction, actions):\n",
    "        return [(0.8, go(state, actions[direction])),\n",
    "                (0.1, go(state, actions[(direction + 1) % 4])),\n",
    "                (0.1, go(state, actions[(direction - 1) % 4]))]\n",
    "\n",
    "\n",
    "def go(state, direction):\n",
    "    return [s+d for s, d in zip(state, direction)]\n",
    "\n",
    "def get_action(t_state, episode):\n",
    "    epsilon = 0.5 * (1 / (episode + 1))\n",
    "    if epsilon <= np.random.uniform(0, 1):\n",
    "        next_action = np.argmax(q_table[t_state])\n",
    "    else:\n",
    "        next_action = np.random.choice(len(actions))\n",
    "    return next_action\n",
    "\n",
    "\n",
    "def take_single_action(state, direction,actions):\n",
    "    x = np.random.uniform(0, 1)\n",
    "    cumulative_probability = 0.0\n",
    "    for probability_state in T(state, direction, actions):\n",
    "        probability, next_state = probability_state\n",
    "        cumulative_probability += probability\n",
    "        if x < cumulative_probability:\n",
    "            break\n",
    "    reward = situation[next_state[0], next_state[1]]\n",
    "    if reward is None:\n",
    "        return state, -0.04\n",
    "    else:\n",
    "        return next_state, reward\n",
    "\n",
    "\n",
    "\n",
    "def update_Qtable(q_table, t_state, action, reward, t_next_state):\n",
    "    gamma = 0.8\n",
    "    alpha = 0.4\n",
    "    # Q学習の方法でq関数を完成させてください。\n",
    "    next_max_q = max(q_table[t_next_state])\n",
    "    q_table[t_state, action] += alpha * (reward + gamma*next_max_q - q_table[t_state, action] )\n",
    "    return q_table\n",
    "\n",
    "def trans_state(state):\n",
    "    return sum([n*(10**i) for i, n in enumerate(state)])\n",
    "\n",
    "q_table = np.random.uniform(\n",
    "    low=-0.01, high=0.01, size=(10 ** 2, 4))\n",
    "\n",
    "num_episodes = 500\n",
    "max_number_of_steps = 1000\n",
    "total_reward_vec = np.zeros(5)\n",
    "goal_average_reward = 0.7\n",
    "\n",
    "situation = np.array([[None, None, None, None, None, None],\n",
    "                      [None, -0.04, -0.04, -0.04, -0.04, None],\n",
    "                      [None, -0.04, None, -0.04, -1,None],\n",
    "                      [None, -0.04, -0.04, -0.04, +1,None],\n",
    "                      [None, None, None, None, None, None]])\n",
    "            \n",
    "\n",
    "terminals=[[2, 4], [3, 4]]\n",
    "init = [1,1]\n",
    "actions = ((1, 0), (0, 1), (-1, 0), (0, -1))\n",
    "state = [n*(10**i) for i,n in enumerate(init)]\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = init\n",
    "    t_state = trans_state(state)\n",
    "    action = np.argmax(q_table[t_state])\n",
    "    episode_reward = 0\n",
    "    \n",
    "    # 以下の空欄を埋めてコードを完成させてください。\n",
    "    for t in range(max_number_of_steps): \n",
    "        next_state, reward = take_single_action(state, action, actions)\n",
    "        episode_reward += reward  \n",
    "        t_next_state = trans_state(next_state)\n",
    "        q_table = update_Qtable(q_table, t_state, action, reward, t_next_state)\n",
    "        state = next_state\n",
    "        next_action = get_action(t_state, episode)\n",
    "        t_state = trans_state(state)\n",
    "        action = next_action\n",
    "        \n",
    "        if state in terminals :\n",
    "            break\n",
    "    total_reward_vec = np.hstack((total_reward_vec[1:], episode_reward))  \n",
    "    print(total_reward_vec)\n",
    "    print(\"Episode %d has finished. t=%d\" %(episode+1, t+1))\n",
    "    print(min(total_reward_vec),goal_average_reward)\n",
    "    if (min(total_reward_vec) >= goal_average_reward): \n",
    "        print('Episode %d train agent successfuly! t=%d' %(episode, t))\n",
    "        break \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "341px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}