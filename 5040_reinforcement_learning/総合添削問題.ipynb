{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "final_exam"
      },
      "source": [
        "# 総合添削問題"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 問題"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "question"
      },
      "source": [
        "環境3のような迷路の解き方を強化学習により学習します。<br><br>\n",
        "<img src=\"https://aidemyexcontentspic.blob.core.windows.net/contents-pic/_RL/img_9.png\">\n",
        "\n",
        "STARTから出発し、GOALまで最短で到達する方法を強化学習により探索します。<br>\n",
        "迷路には左上から順番に番号を割り振っていきます。<br>\n",
        "迷路の壁情報はsituationにて定義しました。\n",
        "[左 上 右 下]を表しており、移動が可能な方向は1が、壁により移動ができない方向は0が割り当てられています。<br>\n",
        "\n",
        "探索には$\\epsilon$-greedy法を利用します。$\\epsilon$の値は、$\\epsilon$ = 0.5×$\\frac{1}{episode + 1}$を利用します。\n",
        "報酬はGOALを100、GOAL以外を-1で設定しています。\n",
        "\n",
        "\n",
        "- get_action関数では$\\epsilon$-greedy法を利用して次の行動を決定します。<br>\n",
        "- take_single_action関数では状態と行動を与えて、次の状態とその報酬を返してください。<br>\n",
        "- 連続してgoal_average_rewardを5回超えたら学習完了とします。<br>\n",
        "\n",
        "コードの穴を埋めて実装してください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "index"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "\n",
        "# epsilon-greedyを利用して次の行動を決定してください。\n",
        "def get_action(state, episode):\n",
        "\n",
        "    \n",
        "    \n",
        "    return next_action\n",
        "\n",
        "# 現在の状態と行動を入力として、次の状態と報酬を返してください。\n",
        "def take_single_action(state, action):\n",
        "\n",
        "    \n",
        "    \n",
        "    return state,reward\n",
        "\n",
        "# Qtableはchapter3で利用したQtableと同じものを利用しています。\n",
        "def update_Qtable(q_table, state, action, reward, next_state, next_action):\n",
        "    gamma = 0.8\n",
        "    alpha = 0.4\n",
        "    next_max_q=np.nanmax(q_table[next_state[0]])\n",
        "    q_table[state[0], action] += alpha * (reward + gamma*next_max_q - q_table[state[0], action] )\n",
        "    return q_table\n",
        "\n",
        "\n",
        "\n",
        "NUM_BLOCKS = 36\n",
        "\n",
        "# 状態の数が36、行動のパターン数が4なので36×4のQtableを作成しています。\n",
        "q_table = np.random.uniform(\n",
        "    low=-0.01, high=0.01, size=(NUM_BLOCKS, 4))\n",
        "\n",
        "situation = [\n",
        "    [0,0,0,1], [0,0,1,1], [1,0,1,0], [1,0,0,1], [0,0,1,1], [1,0,0,1],\n",
        "    [0,1,1,0], [1,1,0,1], [0,0,1,1], [1,1,0,0], [0,1,0,1], [0,1,0,0],\n",
        "    [0,0,1,1], [1,1,0,0], [0,1,1,0], [1,0,1,0], [1,1,1,0], [1,0,0,1],\n",
        "    [0,1,0,1], [0,0,0,1], [0,0,1,1], [1,0,0,0], [0,0,1,1], [1,1,0,1],\n",
        "    [0,1,1,1], [1,1,0,0], [0,1,0,1], [0,0,1,1], [1,1,0,0], [0,1,0,0],\n",
        "    [0,1,1,0], [1,0,1,0], [1,1,0,0], [0,1,1,0], [1,0,1,0], [1,0,0,0]\n",
        "    ]\n",
        "\n",
        "# 壁のある方向のQtableにはNoneを代入しています。\n",
        "for i in range(36):\n",
        "    wall = [flag == 0 for flag in situation[i]]\n",
        "    q_table[i][wall] = None\n",
        "    \n",
        "\n",
        "num_episodes = 100\n",
        "max_number_of_steps = 1000\n",
        "total_reward_vec = np.zeros(5)\n",
        "goal_average_reward = 70\n",
        "\n",
        "\n",
        "terminals=[35]\n",
        "init = [0]\n",
        "actions = (-1,-6,1,6)\n",
        "\n",
        "# コーディングして学習させてください。\n",
        "for episode in range(num_episodes):\n",
        "\n",
        "    \n",
        "    \n",
        "    \n",
        "    for t in range(max_number_of_steps): \n",
        "\n",
        "        \n",
        "        \n",
        "        \n",
        "  \n",
        "    total_reward_vec = np.hstack((total_reward_vec[1:],episode_reward))  \n",
        "    print(total_reward_vec)\n",
        "    print(\"Episode %d has finished. t=%d\" %(episode+1, t+1))\n",
        "    print(min(total_reward_vec),goal_average_reward)\n",
        "    if (min(total_reward_vec) >= goal_average_reward): \n",
        "        print('Episode %d train agent successfuly! t=%d' %(episode,t))\n",
        "        break \n",
        "\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ヒント"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hint"
      },
      "source": [
        "- 値にNoneが含まれていると、argmax,maxの返し値はNoneになってしまします。None以外の最大値を返す関数としてnanargmax,nanmaxがあります。\n",
        "- get_actionではNoneを選ばないように注意してください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  解答例"
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Edit Metadata",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}