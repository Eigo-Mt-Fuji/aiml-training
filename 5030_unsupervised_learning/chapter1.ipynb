{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "chapterId": "HkUXOC7ZgZz",
    "id": "chapter_name"
   },
   "source": [
    "#  教師なし学習の基礎"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "table"
   },
   "source": [
    "- **[1.1 教師なし学習](#1.1-教師なし学習)**\n",
    "    - **[1.1.1 教師なし学習とは](#1.1.1-教師なし学習とは)**\n",
    "<br><br>\n",
    "- **[1.2 教師なし学習の種類](#1.2-教師なし学習の種類)**\n",
    "    - **[1.2.1 クラスタリング](#1.2.1-クラスタリング)**\n",
    "    - **[1.2.2 主成分分析](#1.2.2-主成分分析)**\n",
    "<br><br>\n",
    "- **[1.3 事前知識](#1.3-事前知識)**\n",
    "    - **[1.3.1 ユークリッド距離](#1.3.1-ユークリッド距離)**\n",
    "    - **[1.3.2 コサイン類似度](#1.3.2-コサイン類似度)**\n",
    "<br><br>\n",
    "- **[1.4 添削問題](#1.5-添削問題)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section_name",
    "sectionId": "SJPm_0QbeWM"
   },
   "source": [
    "## 1.1 教師なし学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5030,
    "exerciseId": "H1SWqhIo8lz",
    "id": "quiz_session_name",
    "important": true,
    "isDL": false,
    "timeoutSecs": 5
   },
   "source": [
    "### 1.1.1 教師なし学習とは"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "　ここまでの章では、答えがわかっている(入力値とそれに対応する出力値がセットになっている)データを用いてAIを訓練していく、「教師あり学習」について学習してきました。\n",
    "\n",
    "教師あり学習とは対照的に、 **答えの与えられていないデ\n",
    "ータセットに対して、AIが自ら判断して答えを決める** という<font color=#AA0000>「 **教師なし学習** 」</font>があります。このコースでは、教師なし学習である「クラスタリング」と「主成分分析」について学習することを目的とします。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "- 次の「」に入る **最も適切な言葉の組み合わせ** を選んでください。\n",
    "- 教師なし学習には、「」と「」があります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "choices"
   },
   "source": [
    "- 「クラスタリング」「主成分分析」\n",
    "- 「ランダムフォレスト」「因子分析」\n",
    "- 「線形回帰」「主成分分析」\n",
    "- 「クラスタリング」「因子分析」"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- クラスタリング、主成分分析は教師なし学習の代表的な手法です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "answer"
   },
   "source": [
    "- 「クラスタリング」「主成分分析」"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section_name",
    "sectionId": "Hy_X_AXWxZf"
   },
   "source": [
    "## 1.2 教師なし学習の種類"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5030,
    "exerciseId": "BJIbqhUoIgM",
    "id": "quiz_session_name",
    "important": true,
    "isDL": false,
    "timeoutSecs": 5
   },
   "source": [
    "### 1.2.1 クラスタリング"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "　例えば下図のような与えられたデータが数個の集まりを帯びていた時、\n",
    "人間は集まりが3つあると即座に認識できます。\n",
    "データの集まりは、**クラスター** と呼ばれており、 **機械にクラスターを認識させる手法** を **<font color=#AA0000>「クラスタリング」</font>** と呼ばれています。\n",
    " \n",
    "<img src=\"https://aidemyexcontentspic.blob.core.windows.net/contents-pic/5030_unsupervised_learning/unsupervised_chap1_10.png\">\n",
    "\n",
    " ですが、 **機械は簡単には3つのデータの集まりがあると認識することができません。** 人間が機械にデータの集まりを認識させるためにアルゴリズムを考え機械に適用させる必要があります。その代表的なアルゴリズムに **k-means法** というものがあります。この手法に関しては、後の章にて評説いたします。k-means法を適用した結果が以下になります。3つのクラスターが正しく認識されていることがわかります。\n",
    " \n",
    "<img src=\"https://aidemyexcontentspic.blob.core.windows.net/contents-pic/5030_unsupervised_learning/unsupervised_chap1_20.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "- 次の「」に入る **最も適切な言葉の組み合わせ** を選んでください。\n",
    "- データの集まりのことを「」と呼び、それを機械に認識させることを「」と呼びます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "choices"
   },
   "source": [
    "- 「クラスター」「ディープラーニング」\n",
    "- 「クラスター」「クラスタリング」\n",
    "- 「ランダムフォレスト」「ディープラーニング」\n",
    "- 「ランダムフォレスト」「クラスタリング」"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- 英語の「cluster」は「集まり、集団、群れ」を意味します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "answer"
   },
   "source": [
    "- 「クラスター」「クラスタリング」"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5030,
    "exerciseId": "rJvb938sUeG",
    "id": "quiz_session_name",
    "important": true,
    "isDL": false,
    "timeoutSecs": 5
   },
   "source": [
    "### 1.2.2 主成分分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "代表的な教師なし学習は、クラスタリングの他に **<font color=#AA0000>主成分分析</font>** があります。\n",
    "\n",
    "**主成分分析は、PCA(Principal Component Analysis)とも呼ばれ、様々な分野にわたって広く使用されている手法です。**\n",
    "\n",
    "特に **次元削減** に頻繁に用いられます。 **次元削減** に関しては、後の章で取り上げます。\n",
    "それ以外にも、探索的データ解析や株取引での信号のノイズ除去、バイオインフォマティクス分野でのゲノムデータや遺伝子発現量の解析にも応用されています。\n",
    " \n",
    "　**主成分分析** は、簡単に説明すると以下の図のように散布図にそれっぽい線を引くことを意味します。\n",
    " \n",
    "<img src=\"https://aidemyexcontentspic.blob.core.windows.net/contents-pic/5030_unsupervised_learning/unsupervised_chap1_25.png\">\n",
    "\n",
    "青い点がデータを表しており、赤い線が **主成分軸** と呼ばれています。この**主成分軸は、データの分散を最大にするように引かれています。** 簡単に言えば、**散布図においてデータの幅が最も広がるように引かれた線です。**\n",
    "\n",
    "最も幅の広いものを**第一主成分軸**、それに垂直なものの内最も幅の広いものを**第二主成分軸**と呼びます。このような線を引くことによって、複雑な散布図でも、主成分で点の形状をわかりやすい形にまとめることができます。\n",
    "\n",
    "そして、**このデータの分布を模した線を引くことを主成分分析と呼びます。**\n",
    "今回の例では2次元で考えましたが、より高次元のデータであればあるほど、主成分分析の効果が発揮されます。\n",
    "\n",
    "<img src=\"https://aidemyexcontentspic.blob.core.windows.net/contents-pic/5030_unsupervised_learning/unsupervised_chap1_30.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "- 次の「」に入る **最も適切な言葉の組み合わせ** を選んでください。\n",
    "- 2次元データにおける主成分分析とは、散布図においてデータの「」を最大にするように、2つの「」を引くことを指します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "choices"
   },
   "source": [
    "- 「分散」「正規分布」\n",
    "- 「平均」「主成分軸」\n",
    "- 「平均」「正規分布」\n",
    "- 「分散」「主成分軸」"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- データのばらつきは「分散」と呼ばれています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "answer"
   },
   "source": [
    "- 「分散」「主成分軸」"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section_name",
    "sectionId": "H1K7_AXblbz"
   },
   "source": [
    "## 1.3 事前知識"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5030,
    "exerciseId": "HkOWc2LiIlM",
    "id": "quiz_session_name",
    "important": false,
    "isDL": false,
    "timeoutSecs": 5
   },
   "source": [
    "### 1.3.1 ユークリッド距離"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    " 　二点の座標$x(x_1, x_2), y(y_1, y_2)$が与えられたとき、二点間の距離は **ピタゴラスの定理** より求めることができます。\n",
    "\n",
    "　　　**「2次元における距離」** $= \\sqrt{(x_1-y_1)^2+(x_2-y_2)^2}$\n",
    "   \n",
    "　これをより一般的に、n次元空間における二点間に拡張したものが **<font color=#AA0000>ユークリッド距離</font>** と呼ばれています。\n",
    "   \n",
    "　　　**<font color=#AA0000>「n次元におけるユークリッド距離」</font>**　\n",
    "　　　\n",
    "　　　\n",
    "　\n",
    "　\n",
    "$= \\sqrt{(x_1-y_1)^2+(x_2-y_2)^2+...+(x_n-y_n)^2}$\n",
    "   \n",
    "　n=4以上の空間における「距離」とは、もはや人間の直感的な空間認識では想像することが出来ないものですが、数式上では上の様に単純に拡張した式を距離と定義します。\n",
    "また、ユークリッド距離のことを **ノルム** と呼ぶこともあります。\n",
    "  \n",
    "　また **numpyを用いてもユークリッド距離を求めることができます。** \n",
    "```python\n",
    "import numpy as np\n",
    "vec_a = np.array([1, 2, 3])\n",
    "vec_b = np.array([2, 3, 4])\n",
    "print(np.linalg.norm(vec_a - vec_b))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "- 3次元において以下のように2点の座標が与えられた時、 **2点間のユークリッド距離** を選択肢から選んでください。\n",
    "- $p(1, 3, 5), q(2, 4, 6)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "choices"
   },
   "source": [
    "- $1$\n",
    "- $\\sqrt{2}$\n",
    "- $\\sqrt{3}$\n",
    "- $2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- 問題のユークリッド距離は$\\sqrt{(1-2)^2+(3-4)^2+(5-6)^2)}$で算出できます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "answer"
   },
   "source": [
    "- $\\sqrt{3}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5030,
    "exerciseId": "B1tZcn8iUxG",
    "id": "quiz_session_name",
    "important": false,
    "isDL": false,
    "timeoutSecs": 5
   },
   "source": [
    "### 1.3.2 コサイン類似度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "2次元のベクトル$\\vec{a} = (a_1, a_2), \\vec{b} = (b_1, b_2)$が与えられた時、この2つのベクトル(実際には何らかの2次元データ)がどれほど似た性質を持つのかを評価したいと考えます。\n",
    "\n",
    "　**ベクトルを表す性質**　として挙げられるのは　**「長さ」と「方向」**　でしょう。ここでは **「方向」** に着目します。2つのベクトルが向いている **「方向」の類似度** とは、単純にその **なす角度** と対応していると考えてよいでしょう。 **なす角度を$\\theta$** とすると、 **$\\theta$が小さいほど2つのデータは似ていることになります。** \n",
    "<br>ここで、ベクトルの内積の計算式\n",
    " \n",
    "　　　　$\\vec{a} \\cdot \\vec{b} = |\\vec{a}|\\, |\\vec{b}| \\, \\mathrm{cos}\\theta$\n",
    "    \n",
    "より、少し計算すると\n",
    " \n",
    "　　　　$\\Large \\mathrm{cos}\\theta=\\frac{a_1b_1+a_2b_2}{\\sqrt{a_1^2+a_2^2}\\sqrt{b_1^2+b_2^2}}$   \n",
    "    \n",
    "となります。$\\mathrm{cos}\\theta$は$\\theta$**が小さいほど大きい値となります。** \n",
    "\n",
    "**以上から、$\\mathrm{cos}\\theta$は2つのデータの類似度を表していることがわかりました。**<br>\n",
    "このように**成す角度の$\\mathrm{cos}$をデータの類似度の指標としたもの**を、**<font color=#AA0000>「コサイン類似度」</font>** と言います。\n",
    "    \n",
    "**<font color=#AA0000>「コサイン類似度」</font>** を先程の **ユークリッド距離** と同様に、n次元データに対しても使えるように拡張します。2つのn次元ベクトル$\\vec{a} = (a_1, a_2, \\cdots, a_n), \\vec{b} = (b_1, b_2, \\cdots, b_n)$が与えられた時、「コサイン類似度」は以下の式で表されます。\n",
    " \n",
    "**<font color=#AA0000>「コサイン類似度」</font>**     \n",
    "$\\LARGE =\\frac{a_1b_1+...+a_nb_n}{\\sqrt{a_1^2+...+a_n^2}\\sqrt{b_1^2+...+b_n^2}}$\n",
    "\n",
    "<br>    \n",
    "　また、**コサイン類似度は、以下のコードで算出できます。**\n",
    "```python\n",
    "import numpy as np\n",
    "vec_a = np.array([1, 2, 3])\n",
    "vec_b = np.array([2, 3, 4])\n",
    "print(np.dot(vec_a, vec_b) / (np.linalg.norm(vec_a) * np.linalg.norm(vec_b)))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "- 以下の2つのベクトルが与えられた時の **コサイン類似度** を算出してください。\n",
    "- $\\vec{a}=(0,1), \\vec{b}=(3,4)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "choices"
   },
   "source": [
    "- 0.7\n",
    "- 0.8\n",
    "- 0.9\n",
    "- 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- $\\frac{0 \\times 3 + 1 \\times 4}{\\sqrt{0 + 1^2}\\sqrt{3^2 + 4^2}}$でコサイン類似度を算出することができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "answer"
   },
   "source": [
    "- 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "chapter_exam"
   },
   "source": [
    "## 1.4 添削問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "　ユークリッド距離とコサイン類似度はよく使わられる指標です。このチャプターの添削問題を通して、確実に身につけましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "- 以下の2つのベクトルのユークリッド距離と、コサイン類似度を算出しなさい。\n",
    "- $\\vec{a}=(0, 5, 10), \\vec{b}=(1, 6, 11)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "index"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "# コードを書く、または手計算をして、次のEuclidean_disにユークリッド距離を、Cosine_simにコサイン類似度を代入してください\n",
    "\n",
    "Euclidean_dis = \n",
    "Cosine_sim = \n",
    "print(\"ユークリッド距離:%.3f\"%Euclidean_dis)\n",
    "print(\"コサイン類似度:%.3f\"%(Cosine_sim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- ユークリッド距離は、$\\sqrt{(0-1)^2+(5-6)^2+(10-11)^2)}$になります。\n",
    "- コサイン類似度は、$\\frac{0×1+5×6+10×11}{\\sqrt{0^2+5^2+10^2}\\sqrt{1^2+6^2+11^2}}$になります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "answer"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "a = np.array([0, 5, 10])\n",
    "b = np.array([1, 6, 11])\n",
    "Euclidean_dis = np.linalg.norm(a - b)\n",
    "Cosine_sim = 1-cosine(a, b)\n",
    "print(\"ユークリッド距離:%.3f\"%Euclidean_dis)\n",
    "print(\"コサイン類似度:%.3f\"%(Cosine_sim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解説"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "commentary"
   },
   "source": [
    "　本章では、教師なし学習としてよく用いられるクラスタリングと主成分分析の概要をつかみました。次章からは、クラスタリングや主成分分析についてより詳しく見ていきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}