{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "chapterId": "ry1e3lJf7UM",
    "id": "chapter_name"
   },
   "source": [
    "# マルコフ決定過程とベルマン方程式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "table"
   },
   "source": [
    "\n",
    "- **[2.1 強化学習の構成要素](#2.1-強化学習の構成要素)**\n",
    "    - **[2.1.1 強化学習の構成要素](#2.1.1-強化学習の構成要素)**\n",
    "    - **[2.1.2 強化学習のモデル化](#2.1.2-強化学習のモデル化)**\n",
    "    - **[2.1.3 状態・行動・報酬](#2.1.3-状態・行動・報酬)**\n",
    "<br><br>\n",
    "- **[2.2 マルコフ決定過程](#2.2-マルコフ決定過程)**\n",
    "    - **[2.2.1 マルコフ性](#2.2.1-マルコフ性)**\n",
    "    - **[2.2.2 環境変化の記述-1](#2.2.2-環境変化の記述-1)**\n",
    "    - **[2.2.3 環境変化の記述-2](#2.2.3-環境変化の記述-2)**\n",
    "    - **[2.2.4 エピソード](#2.2.4-エピソード)**\n",
    "<br><br>\n",
    "- **[2.3 価値・収益・状態](#2.3-価値・収益・状態)**\n",
    "    - **[2.3.1 報酬と収益](#2.3.1-報酬と収益)**\n",
    "    - **[2.3.2 価値関数・状態価値関数](#2.3.2-価値関数・状態価値関数)**\n",
    "<br><br>\n",
    "- **[2.4 最適な方策の探求](#2.4-最適な方策の探求)**\n",
    "    - **[2.4.1 最適方策](#2.4.1-最適方策)**\n",
    "    - **[2.4.2 行動価値](#2.4.2-行動価値)**\n",
    "    - **[2.4.3 最適状態価値関数・最適行動価値関数](#2.4.3-最適状態価値関数・最適行動価値関数)**\n",
    "<br><br>\n",
    "- **[2.5 ベルマン方程式](#2.5-ベルマン方程式)**\n",
    "    - **[2.5.1 最適な状態価値](#2.5.1-最適な状態価値)**\n",
    "    - **[2.5.2 ベルマン方程式](#2.5.2-ベルマン方程式)**\n",
    "    - **[2.5.3 ベルマン最適方程式](#2.5.3-ベルマン最適方程式)**\n",
    "<br><br>\n",
    "- **[添削問題](#添削問題)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section_name",
    "sectionId": "Hkll2lyzmLM"
   },
   "source": [
    "## 2.1 強化学習の構成要素"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5040,
    "exerciseId": "S1ZgnxJMXIM",
    "id": "quiz_session_name",
    "important": false,
    "isDL": false,
    "timeoutSecs": 10
   },
   "source": [
    "### 2.1.1 強化学習の構成要素"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "chapter1で考えてきたn腕バンディッド問題は非常に単純化された問題でした。  \n",
    "具体的にどこが単純化されていたのでしょうか。それは**自分の行動によって状態が変化しない**という点と、報酬が全て**その時点でもらえる報酬(即時報酬)** である点です。**状態**とは、環境が今現在どうなっているのか表すものです。オセロを例にすると「プレイヤー」が**エージェント**で、「オセロ」が**環境**、「ある時点での盤面」が**状態**になります。一般的な強化学習は前の行動によって状態が変化するため、次に取るべき行動の選択が変化していくような学習スタイルをとります。また、その時点でもらえる報酬(即時報酬)ではなく、**将来的な価値を最大化する行動**をすることを目的とします。\n",
    "\n",
    "そこで今までの強化学習の考え方に「状態」「時間」という概念を新しく取り込む必要が出てきます。ここからはより一般的な強化学習の理念を習得していくことになります。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "n腕バンディッド問題と一般的な強化学習で異なる点は次のうちのどれでしょうか。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "choices"
   },
   "source": [
    "- 行動すると報酬がもらえる点\n",
    "- 自分の前の行動が次の行動に影響を及ぼさない点\n",
    "- 環境と相互作用をする点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "n腕バンディッド問題では自分の行動は環境に影響を及ぼしません。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "answer"
   },
   "source": [
    "- 自分の前の行動が次の行動に影響を及ぼさない点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5040,
    "exerciseId": "ryfe2e1fmLM",
    "id": "quiz_session_name",
    "important": false,
    "isDL": false,
    "timeoutSecs": 10
   },
   "source": [
    "### 2.1.2 強化学習のモデル化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "強化学習は、基本的に\n",
    "1. 対応したい環境・エージェント・報酬をマルコフ決定過程(後述)によってモデル化\n",
    "2. アルゴリズムを解析する\n",
    "3. 学習する  \n",
    "\n",
    "の流れに沿っています。まずは、どのように強化学習を**モデル化**して行くかを考えていきます。強化学習の基本的な枠組みは以下の図のように表すことができます。  \n",
    "\n",
    "<img src=\"https://aidemyexcontentspic.blob.core.windows.net/contents-pic/_RL/img_5.png\">\n",
    "\n",
    "図のように、\n",
    "1. エージェントが状態$s_{t}$を受けて行動 $a_{t}$ を起こし環境に作用する\n",
    "2. それを受けて環境は状態$s_{t+1}$に遷移する\n",
    "3. 環境はエージェントに報酬$r_{t+1}$を与える\n",
    "4. エージェントは2,3を受けて次の行動$a_{t+1}$を決定する\n",
    "\n",
    "という流れを繰り返していきます。ここでの添え字tは開始してから何回目の動作であるかを示しています。動作の回数は0,1,2,3,4,5....t,t+1というように離散化され時間の役割を果たしています。これを **<font color=#AA000>時間ステップ</font>** と呼び、強化学習タスクにおける時間の基本単位として定義されます。エージェントの行動する指針は **<font color=#AA0000>方策</font>** と呼ばれます。この方策を改善していくことが強化学習の目的です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "机の上のペットボトルを動かすためにロボットアームを動かす事を考えます。\n",
    "この時環境とエージェントの境目は以下のどれが正しいでしょうか？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "choices"
   },
   "source": [
    "- エージェント : プログラム・機構・モーター・センサー 、環境 : ペットボトルの乗った机  \n",
    "- エージェント : プログラム・機構・モーター 、環境 : ペットボトルの乗った机・センサー  \n",
    "- エージェント : プログラム・機構 、環境 : ペットボトルの乗った机・センサー・モーター  \n",
    "- エージェント : プログラム 、環境 : ペットボトルの乗った机・センサー・モーター・機構\n",
    "- 全て"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- 環境とエージェントの違いについて考えてみましょう。\n",
    "- 環境とエージェントの境目は明確に決めることはできず、その時々の状況に応じてユーザーが決定します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "answer"
   },
   "source": [
    "- 全て"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5040,
    "exerciseId": "r1XlhgyMXIM",
    "id": "quiz_session_name",
    "important": false,
    "isDL": false,
    "timeoutSecs": 10
   },
   "source": [
    "### 2.1.3 状態・行動・報酬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "**モデル化**に伴い、今までの**状態・行動・報酬**といったものを数式化しやすくなりました。ここではこれら３つの単語を一度整理し、数式化を試みます。\n",
    "\n",
    "環境の**状態集合**を表すと以下のようになります。<br>\n",
    "$S = \\{ s_{0}, s_{1}, s_{2}, ....\\}$ 　<br>\n",
    "上式は、エピソード内で取りうる全ての状態をさします。また$S_t$と表すと、時間ステップ$t$における状態を表します。\n",
    "\n",
    "次に**行動集合**を定義します。<br>\n",
    "$A(s) =  \\{ a_{0},a_{1},a_{2},...\\}$  <br>\n",
    "ある状態内で、取りうる行動を全て格納します。sを引数に取っている理由は、自分の置かれている状況によって、行動範囲が変わるためです。また$A_t$と表すと、時間ステップ$t$における行動を表します。\n",
    "\n",
    "次に**報酬**を定義します。<br>\n",
    "$R_{T+1}=r(S_t,A_t,S_{t+1})$<br>\n",
    "現在の現在の状態$S_t$と行動$A_t$、および次の状態$S_{t+1}$に応じて、一意にきまる関数を**報酬関数**と定義します。例えばボードゲームでは勝利すると$+1$、敗北すると$-1$が報酬として与えられることが一般的です。ここで、「未来の状態は現在の状態・行動のみによって確率的に決定され、過去の挙動と無関係である」という性質を「**マルコフ性**」と呼びます。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "chapter1でやった「N腕バンディッド問題」においてスロットを3台使用した時、考えられる状態数はいくつでしょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "choices"
   },
   "source": [
    "- 3\n",
    "- 6\n",
    "- 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- 当選する・当選しないまでが状態、当選してから支払われるまでが報酬で定義されます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "answer"
   },
   "source": [
    "- 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section_name",
    "sectionId": "ry4x2e1M78G"
   },
   "source": [
    "## 2.2 マルコフ決定過程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5040,
    "exerciseId": "Syrx3gJMmLf",
    "id": "code_session_name",
    "important": false,
    "isDL": false,
    "timeoutSecs": 10
   },
   "source": [
    "### 2.2.1 マルコフ性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    " **未来の状態は現在の状態・行動のみによって確率的に決定され、過去とは依存しない。** <br>\n",
    "これをマルコフ性と呼び、これを満たす強化学習過程の事を、 **<font color=#AA0000>マルコフ決定過程(MDP)</font>** と呼びます。  \n",
    "\n",
    "**マルコフ過程**は\n",
    "- 状態空間 $S$<br>\n",
    "- 行動空間 $A(s)$<br>\n",
    "- 初期状態分布　$P_{o}$<br>\n",
    "- 状態遷移確率 $P(s'|s,a)$<br>\n",
    "- 報酬関数　$r(s,a,s')$<br>\n",
    "\n",
    "の５つの要素を備えています。\n",
    "\n",
    " **<font color=#AA0000>状態空間</font>** は **環境のとりうる状態の集合** 、 **<font color=#AA0000>行動空間</font>** は**エージェントがとりうる行動の集合**のことを指し示します。\n",
    "\n",
    " **<font color=#AA0000>初期状態分布</font>** とは、**エピソードの開始時点の状態を表す確率変数**です。  \n",
    "\n",
    "環境中のある状態$s\\in{S}$において、エージェントが$a\\in{A(s)}$を実行すると、環境は$s'\\in{S}$へ遷移します。つまり$P(s'|s,a)$は状態$s$で行動$a$をしたとき$s'$になる確率を表しています。その変遷率を **<font color=#AA0000>状態遷移確率</font>** と言います。この時環境からエージェントへ報酬$r(s,a,s')$が与えられます。以下の章ではこれらの説明を行なっていきます。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "以後の問題では以下の図ような環境1を使用します。<br>\n",
    "スタートは`State A`から開始され、 `End`に至ると停止します。矢印についた数字は、状態遷移確率を示しています。<br>\n",
    "\n",
    "例えば、`State B`からactionXを選択して`End`に行くケースを考えましょう。<br>\n",
    "数字は`State B`から`End`へactionXで移動する確率は0.8で結果として報酬を100受け取るという意味になります。このときの状態遷移図は[s,s',a,p(s'|a,s),r(s,a,s')] = [1, 2, 0, 0.8, 100]と表すことができます。<br>\n",
    "\n",
    "<img src=\"https://aidemyexcontentspic.blob.core.windows.net/contents-pic/_RL/img_7.png\">\n",
    "\n",
    "<center>(図:環境1)</center>\n",
    "\n",
    "環境1において状態遷移図を書いてください。\n",
    "\n",
    "ある状態からはどんな行動があるのか、またそのときどれ程の報酬を得ることができるのかなどシステムの情報をプログラムで扱えるよう配列で表したものを状態遷移図といいます。今回の場合ですとState A = 0などそれぞれの状態や行動に数字を割り振ります。問題ではStateはAから順番に、行動もXから順に0からの数字を割り振ってください。形式としては\n",
    "\n",
    "```\n",
    "np.array = ([\n",
    "[s,s',a,p(s'|a,s),r(s,a,s')],\n",
    "[s,s',a,p(s'|a,s),r(s,a,s')],\n",
    "[s,s',a,p(s'|a,s),r(s,a,s')]\n",
    "...\n",
    "])\n",
    "```\n",
    "のようにして実装してください。状態は、StateA = 0, StateB = 1 とおいてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "index"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "state_transition = np.array(\n",
    "# ここに答えを入力してください\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    )\n",
    "\n",
    "print(state_transition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- 状態遷移図には全パターン(6パターン)をかいてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "answer"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "state_transition = np.array(\n",
    "# ここに答えを入力してください\n",
    "                    [[0, 0, 0, 0.3, 5],\n",
    "                    [0, 1, 0, 0.7, -10],\n",
    "                    [0, 0, 1, 1, 5],\n",
    "                    [1, 0, 1, 1, 5],\n",
    "                    [1, 2, 0, 0.8, 100],\n",
    "                    [1, 1, 0, 0.2, -10]]\n",
    "                    )\n",
    "\n",
    "print(state_transition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5040,
    "exerciseId": "rJ8e2xyzQIM",
    "id": "code_session_name",
    "important": false,
    "isDL": false,
    "timeoutSecs": 10
   },
   "source": [
    "### 2.2.2 環境変化の記述-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "今までモデルでは時間の概念を導入してきましたが、これまで紹介してきた変数には時間の概念は導入されていません。そこでマルコフ決定過程の構成要素である\n",
    "\n",
    "- 状態空間 $S$<br>\n",
    "- 行動空間 $A(s)$<br>\n",
    "- 初期状態分布　$P_{o}$<br>\n",
    "- 状態遷移確率 $P(s'|s,a)$<br>\n",
    "- 報酬関数　$r(s,a,s')$<br>\n",
    "\n",
    "の**５つに時間の概念を導入し、数式化**を行なっていきます。まず、状態に時間の概念を導入した新しい変数$S_{t}$を定義します。これは時間$t$での状態を表す変数です。また行動に時間の概念を導入した新しい変数$A_{t}$ を定義します。これも同様に状態$S_{t}$によって決定された行動を表す変数になります。  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "環境1において、現状の状態(`StateB`)から可能な行動を計算するactions関数を実装してください。  \n",
    "terminalsは終点を格納する変数です。  \n",
    "<img src=\"https://aidemyexcontentspic.blob.core.windows.net/contents-pic/_RL/img_7.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "index"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "terminals = [2]\n",
    "state = [1]\n",
    "\n",
    "\n",
    "state_transition = np.array(\n",
    "                    [[0, 0, 0, 0.3, 5],\n",
    "                    [0, 1, 0, 0.7, -10],\n",
    "                    [0, 0, 1, 1, 5],\n",
    "                    [1, 0, 1, 1, 5],\n",
    "                    [1, 2, 0, 0.8, 100],\n",
    "                    [1, 1, 0, 0.2, 1]]\n",
    "                    )\n",
    "\n",
    "# 答えを入力してください\n",
    "def actions(state):\n",
    "    A = \n",
    "    return \n",
    "\n",
    "actions(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- stateBからは、stateAもしくはstateBのみにしか行動できないので、出力は0と1のみになります。\n",
    "- actionsの返し値は可能な行動なので、ユニーク(固有な要素)でなければいけません。\n",
    "- ユニークにするにはnp.unique()が便利です。\n",
    "- 現在地がterminalの場合は何も返さないようにしてください。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-26T03:51:53.886901Z",
     "start_time": "2018-09-26T03:51:53.859427Z"
    },
    "id": "answer"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "terminals = [2]\n",
    "state = [1]\n",
    "\n",
    "state_transition = np.array(\n",
    "                    [[0, 0, 0, 0.3, 5],\n",
    "                    [0, 1, 0, 0.7, -10],\n",
    "                    [0, 0, 1, 1, 5],\n",
    "                    [1, 0, 1, 1, 5],\n",
    "                    [1, 2, 0, 0.8, 100],\n",
    "                    [1, 1, 0, 0.2, 1]]\n",
    "                    )\n",
    "# 答えを入力してください\n",
    "def actions(state):\n",
    "    A = state_transition[(state_transition[:, 0] == state)]\n",
    "    return  np.unique(A[:,2])\n",
    "\n",
    "actions(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5040,
    "exerciseId": "H1wxng1fXLG",
    "id": "code_session_name",
    "important": false,
    "isDL": false,
    "timeoutSecs": 10
   },
   "source": [
    "### 2.2.3 環境変化の記述-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "前回に引き続き、これまで紹介してきた**マルコフ決定過程**の構成要素に**時間**の概念を導入していきます。  \n",
    "まずは**初期状態分布**を再定義します。初期状態分布は$S_{0}$、$t=0$の時の状態を決定するものと考えます。  \n",
    "\n",
    "次に**状態遷移確率**を再定義します。マルコフ決定過程より、時刻$t+1$の状態$S_{t+1}$は時刻$t$での行動$A_{t}$、状態$S_{t}$のみによって決定されます。これを状態遷移確率を使用して以下のように表式します。    \n",
    "$P(s'|s_{t},a_{t})$  \n",
    "ただし$s_{t}\\in{S_{t}}$,$a_{t}\\in{A_{t}}$です。\n",
    "\n",
    "最後に**報酬関数**を再定義します。報酬も各時間ステップ毎に得られるため、状態$S_{t+1}$に遷移したことで得られた報酬を$R_{t+1}$と定義し、  \n",
    "$R_{t+1} = r(s_{t},a_{t},s_{t+1})$<br>\n",
    "ただし$s_{t}\\in{S_{t}}$,$a_{t}\\in{A_{t}}$,$s_{t+1}\\in{S_{t+1}}$ <br>\n",
    "のように報酬関数を再定義します。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "環境1において、現在の状態・行動・遷移した状態を変数にとって報酬を返す報酬関数を実装してください。  \n",
    "すでに現在の状態が終点である場合、0を返すようにしてください。\n",
    "<img src=\"https://aidemyexcontentspic.blob.core.windows.net/contents-pic/_RL/img_7.png\">\n",
    "<center>環境1</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "index"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "after_state = [2]\n",
    "state = [1]\n",
    "action = [0]\n",
    "terminals = [2]\n",
    "\n",
    "state_transition = np.array(\n",
    "                    [[0, 0, 0, 0.3, 5],\n",
    "                    [0, 1, 0, 0.7, -10],\n",
    "                    [0, 0, 1, 1, 5],\n",
    "                    [1, 0, 1, 1, 5],\n",
    "                    [1, 2, 0, 0.8, 100],\n",
    "                    [1, 1, 0, 0.2, 1]]\n",
    "                    )\n",
    "\n",
    "# 答えを入力してください\n",
    "def R(state, action, after_state):\n",
    "    if state in terminals:\n",
    "        return \n",
    "    \n",
    "    X = \n",
    "    return \n",
    "\n",
    "R(state, action, after_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- 状況に適切な報酬を返すようにしましょう。\n",
    "- 現在地がterminalのときは0を返してください。\n",
    "- 複数の条件で指定する場合は、state_transition[(条件) & (条件)]とします。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "answer"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "after_state = [2]\n",
    "state = [1]\n",
    "action = [0]\n",
    "terminals = [2]\n",
    "\n",
    "state_transition = np.array(\n",
    "                    [[0, 0, 0, 0.3, 5],\n",
    "                    [0, 1, 0, 0.7, -10],\n",
    "                    [0, 0, 1, 1, 5],\n",
    "                    [1, 0, 1, 1, 5],\n",
    "                    [1, 2, 0, 0.8, 100],\n",
    "                    [1, 1, 0, 0.2, 1]]\n",
    "                    )\n",
    "\n",
    "# 答えを入力してください\n",
    "def R(state, action, after_state):\n",
    "    if state in terminals:\n",
    "        return 0\n",
    "    \n",
    "    X = state_transition[(state_transition[:,0] == state) & (state_transition[:,1] == after_state) & (state_transition[:,2] == action)]\n",
    "    return X[0, 4]\n",
    "\n",
    "R(state, action, after_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5040,
    "exerciseId": "BJughl1f7IM",
    "id": "code_session_name",
    "important": false,
    "isDL": false,
    "timeoutSecs": 10
   },
   "source": [
    "### 2.2.4 エピソード"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "タスク開始から終了までの間にかかる時間のことを **<font color=#AA0000>エピソード</font>** と言います。<br>\n",
    "例えば、三目並べであれば、石のない状態から、勝敗が決まるまでが**エピソード**で、これは複数の**時間ステップ**からなる単位です。\n",
    "\n",
    "行動→状態変化というサイクルが複数回なされることで1つの**エピソード**が**構成**されていきます。一回のエピソードの中で、環境を**初期化**しエージェントに行動させ、受け取った**報酬**を元に行動モデルを**最適化**していきます。<br>エピソードを複数回繰り返すことによってエージェントは学習を進めていくことになります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "環境1において、現在の状態・行動を変数にして  \n",
    "遷移する可能性のある状態とその確率を返す遷移モデル関数を実装してください。  \n",
    "出力は(確率・遷移状態)のタプルのリストであるものとします。  \n",
    "また現在の状態が終端である場合、`[(0, terminals)]`を返すようにしてください。\n",
    "<img src=\"https://aidemyexcontentspic.blob.core.windows.net/contents-pic/_RL/img_7.png\">\n",
    "<center>環境1</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "index"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "state = [1]\n",
    "action = [0]\n",
    "terminals = [2]\n",
    "\n",
    "state_transition = np.array(\n",
    "                    [[0, 0, 0, 0.3, 5],\n",
    "                    [0, 1, 0, 0.7, -10],\n",
    "                    [0, 0, 1, 1, 5],\n",
    "                    [1, 0, 1, 1, 5],\n",
    "                    [1, 2, 0, 0.8, 100],\n",
    "                    [1, 1, 0, 0.2, -10]]\n",
    "                    )\n",
    "\n",
    "# 答えを入力してください\n",
    "def T(state, action):\n",
    "     if (state in terminals):\n",
    "        return \n",
    "    \n",
    "    X = \n",
    "    \n",
    "    A = \n",
    "    return \n",
    "\n",
    "print(T(state, action))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- stateがterminalのときはif文で場合分けしてください。\n",
    "- A = X[:, [3, 1]]\n",
    "- return [tuple(A[i, :]) for i in range(A.shape[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "id": "answer",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "state = [1]\n",
    "action = [0]\n",
    "terminals = [2]\n",
    "\n",
    "state_transition = np.array(\n",
    "                    [[0, 0, 0, 0.3, 5],\n",
    "                    [0, 1, 0, 0.7, -10],\n",
    "                    [0, 0, 1, 1, 5],\n",
    "                    [1, 0, 1, 1, 5],\n",
    "                    [1, 2, 0, 0.8, 100],\n",
    "                    [1, 1, 0, 0.2, 1]]\n",
    "                    )\n",
    "\n",
    "# 答えを入力してください\n",
    "def T(state, action):\n",
    "    if (state in terminals):\n",
    "        return [(0, terminals)]\n",
    "\n",
    "    X = state_transition[(state_transition[:, 0] == state)&(state_transition[:, 2] == action)]\n",
    "\n",
    "    A = X[:, [3, 1]]\n",
    "    return [tuple(A[i, :]) for i in range(A.shape[0])]\n",
    "\n",
    "print(T(state, action))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section_name",
    "sectionId": "B1KxheJMQ8G"
   },
   "source": [
    "## 2.3 価値・収益・状態"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5040,
    "exerciseId": "rycgnlJGm8M",
    "id": "code_session_name",
    "important": false,
    "isDL": false,
    "timeoutSecs": 10
   },
   "source": [
    "### 2.3.1 報酬と収益"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "ここまでは強化学習のモデル化を行なってきました。ここからは何を基準にして最適な方策を探っていくのかということについて進めていきます。  \n",
    "\n",
    "**報酬**は**直前の**行動と状態のみで決定されるため、これを指標にしてしまうとすぐには小さい報酬しか得られなくてもその後非常に大きな報酬が得られるような行動を見落としてしまうことにつながります。  \n",
    "\n",
    "これを解決するために **<font color=#AA0000>収益</font>** という概念が存在します。**収益はある期間で得られた報酬を合計することによって導出されるため、その状態より後の報酬も考慮に入れることが出来る**様になります。具体的に示すと、報酬$R_{t+1}$が行動$a_{t}$を選択した直後にもらえるものであるのに対して、最終的に合計でどれくらいの報酬を得られたかを示しているものなので収益は以下のように表記されます。\n",
    "\n",
    "$G_{t} =R_{t+1} + \\gamma R_{t+2}+ \\gamma^2 R_{t+3}+....= \\displaystyle\\sum_{\\tau=0}^{\\infty} \\gamma^{\\tau} R_{t+1+\\tau}$\n",
    "\n",
    "$\\gamma$は割引率と呼ばれるもので、$0 \\leq \\gamma \\leq 1$の範囲で値をとります。  \n",
    "$\\gamma$は将来もらえる報酬をどれくらい現在の価値として考慮に入れるかのパラメーターになるため、<br>\n",
    "$\\gamma=0$なら直後のみを考慮に入れることになり、$\\gamma=1$なら将来もらえる報酬を現在の価値と同様に考慮に入れることになります。\n",
    "\n",
    "また、報酬の合計の仕方にはいくつか方法があり、\n",
    "1. 時刻tからある期間Tまで報酬を合計する\n",
    "2. 時刻tからある時間Tまでの収益の平均を計算し、Tを∞に飛ばした極限を取る\n",
    "\n",
    "などの方法がありますが、2.のような **<font color=#AA0000>割引報酬和</font>** という手法が一般的です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "環境1において、\n",
    "\n",
    "常に行動Xを取り続けるエージェントを実際に動かすことで、  \n",
    "$G_{t} = \\displaystyle\\sum_{\\tau=0}^{1} \\gamma^{\\tau} R_{t+1+\\tau}$  \n",
    "を求めて下さい。\n",
    "\n",
    "take_single_action ではある行動を選んだときどのように遷移するかを計算します。<br>\n",
    "具体的にState_Bでaction Xを選択したとしてもふたとおりの遷移の仕方が考えられます。<br>\n",
    "これを確率に基づいて計算してください。<br>\n",
    "\n",
    "calculate_value では収益を計算してください。\n",
    "\n",
    "***\n",
    "\n",
    "このchapterより、以上の関数をまとめたライブラリを`markov.py`として使用します。 <br>\n",
    "```python\n",
    "from markov import *\n",
    "```\n",
    "によってここまで定義してきたacitons, R, T関数を使うことができます。<br>\n",
    "またmarkovライブラリのなかで必要なモジュールはすべてインストールすることとします。<br>\n",
    "具体的には、numpy,randomのインポートとrandomのseed、state_transition、terminals、今まで出てきたモジュールの定義を行っています。<br>\n",
    "\n",
    "<img src=\"https://aidemyexcontentspic.blob.core.windows.net/contents-pic/_RL/img_7.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "index"
   },
   "outputs": [],
   "source": [
    "from markov import *\n",
    "\n",
    "# 答えを入力して下さい\n",
    "def take_single_action(state, action):\n",
    "    x = \n",
    "    cumulative_probability = \n",
    "    for probability_state in T(state, action):\n",
    "        probability, next_state = \n",
    "    cumulative_probability += \n",
    "    \n",
    "    \n",
    "    return next_state\n",
    "\n",
    "# 答えを入力して下さい\n",
    "def calculate_value():\n",
    "    state = \n",
    "    action = \n",
    "    sumg = \n",
    "    for i in range(2):\n",
    "        after_state = \n",
    "        sumg += \n",
    "        state = \n",
    "    return sumg\n",
    "\n",
    "state_transition = np.array(\n",
    "                    [[0, 0, 0, 0.3, 5],\n",
    "                    [0, 1, 0, 0.7, -10],\n",
    "                    [0, 0, 1, 1, 5],\n",
    "                    [1, 0, 1, 1, 5],\n",
    "                    [1, 2, 0, 0.8, 100],\n",
    "                    [1, 1, 0, 0.2, 1]]\n",
    "                    )\n",
    "\n",
    "states = np.unique(state_transition[:, 0])\n",
    "actlist = np.unique(state_transition[:, 2])\n",
    "terminals = [2]\n",
    "init = [0]\n",
    "gamma = 0.8\n",
    "\n",
    "\n",
    "# 答えとなる関数を実行してください\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- 遷移確率に基づいて次の状態を決定する、take_single_actionの実装において、\n",
    "```python\n",
    "x = ０〜１の数字\n",
    "for 確率, 状態 in T(state, action):\n",
    "    確率の和 += 確率\n",
    "    if x < 確率の和:\n",
    "        break\n",
    "return 状態\n",
    "```\n",
    "とするとうまく実装できます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "answer"
   },
   "outputs": [],
   "source": [
    "from markov import *\n",
    "\n",
    "# 答えを入力して下さい\n",
    "def take_single_action(state, action):\n",
    "    x = random.uniform(0, 1)\n",
    "    cumulative_probability = 0.0\n",
    "    for probability_state in T(state, action):\n",
    "        probability, next_state = probability_state\n",
    "        cumulative_probability += probability\n",
    "        if x < cumulative_probability:\n",
    "            break\n",
    "    return next_state\n",
    "\n",
    "# 答えを入力して下さい\n",
    "def calculate_value():\n",
    "    state = init\n",
    "    action = [0]\n",
    "    sumg = 0\n",
    "    for i in range(2):\n",
    "        after_state = take_single_action(state, action)\n",
    "        sumg += (gamma**(i))*R(state, action, after_state)\n",
    "        state = after_state\n",
    "    return sumg\n",
    "\n",
    "state_transition = np.array(\n",
    "                    [[0, 0, 0, 0.3, 5],\n",
    "                    [0, 1, 0, 0.7, -10],\n",
    "                    [0, 0, 1, 1, 5],\n",
    "                    [1, 0, 1, 1, 5],\n",
    "                    [1, 2, 0, 0.8, 100],\n",
    "                    [1, 1, 0, 0.2, 1]]\n",
    "                    )\n",
    "\n",
    "states = np.unique(state_transition[:, 0])\n",
    "actlist = np.unique(state_transition[:, 2])\n",
    "terminals = [2]\n",
    "init = [0]\n",
    "gamma = 0.8\n",
    "\n",
    "\n",
    "# 答えとなる関数を実行してください\n",
    "calculate_value()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5040,
    "exerciseId": "r1sxnxyzm8z",
    "id": "quiz_session_name",
    "important": false,
    "isDL": false,
    "timeoutSecs": 10
   },
   "source": [
    "### 2.3.2 価値関数・状態価値関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "前回の報酬をそのまま評価基準に持ってきても良いように思うのですが、欠点が存在します。\n",
    "先ほども説明したように状態の遷移は確率変数で定義されています。したがって  \n",
    "$G_{t} = \\displaystyle\\sum_{\\tau=0}^{\\infty} \\gamma^{\\tau} R_{t+1+\\tau}$    \n",
    "はRが確率に基づいて決定されるため確率変数の形で出てきてしまい、状態が分岐すればするほどそれは複雑になっていきます。(以後G関数は上記の式で定義されたものを使用するとします。)<br>\n",
    "これを解決するために期待値をとることによってある一定量にするということを考えましょう。\n",
    "このようにある状態をスタート地点にした時の今後の行動すべて考慮に入れた報酬の期待値を**状態価値**または**価値**と呼びます。ある方策(エージェントの行動指針)を$\\pi$とした時の状態価値の事を以下のように示します。<br>\n",
    "$V^{\\pi}(s) = \\mathbb{E}^{\\pi}[G_{t+1} |S_{t} = s]$  \n",
    "これは、  \n",
    "「状態sをスタート地点にして方針$\\pi$にしたがって行動した時の報酬の期待値」が  \n",
    "「$V^{\\pi}(s)$」  \n",
    "で表記されるという事を示しています。<br>\n",
    "\n",
    "私たちはこの概念の導入により、以下の二つの事が可能となります。\n",
    "- ある方策においてどの状態が優れているかを比較できるようになる。\n",
    "- ある状態をスタート地点においた時、方策毎の良し悪しを比較できるようになる。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "次のうち正しいものはどれでしょう？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "choices"
   },
   "source": [
    "- 現在の状態のみに依存している\n",
    "- 方策が変われば状態価値の値も変わる\n",
    "- エピソードtでの状態価値は$G_{t}$である"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- 状態価値とは方策を評価するための指標です。\n",
    "- 方策がいいものであれば状態価値は大きくなり、悪いものであれば小さくなります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "answer"
   },
   "source": [
    "- 方策が変われば状態価値の値も変わる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section_name",
    "sectionId": "r12x2eyM78M"
   },
   "source": [
    "## 2.4 最適な方策の探求"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5040,
    "exerciseId": "Hk6lnl1fQ8M",
    "id": "quiz_session_name",
    "important": false,
    "isDL": false,
    "timeoutSecs": 10
   },
   "source": [
    "### 2.4.1 最適方策"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "状態価値関数の導入によって、ある状態をスタート地点においた時、方策毎の良し悪しを比較することが可能になりました。\n",
    "\n",
    "またこれを広げて、全ての状態において２つの方策の良し悪しを決めることが出来る様になります。ここで二つの方策$\\pi_{1},\\pi_{2}$を比較する場合において\n",
    "\n",
    "- 状態空間Sの中の全ての状態sにおいて、$V^{\\pi_{1}}(s)$が$V^{\\pi_{2}}(s)$以上である。\n",
    "- 状態空間Sの中の少なくとも一つの状態sにおいて　$V^{\\pi_{1}}(s)$が$V^{\\pi_{2}}(s)$ より大きい。\n",
    "\n",
    "以上の二点が満たされている時、「$\\pi_{1}$は$\\pi_{2}$よりも良い方策である」と定義します。<br>\n",
    "さらにこの定義下において、全ての方策に対して比較を行えば最も良い方策が少なくとも一つ存在するでしょう。   \n",
    "これを**最適方策**と呼び、$\\pi^{*}$で定義します。(複数ある可能性もありますが、まとめて一つの記号で定義しましょう。）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "- 次の選択肢のうち誤っているものを選んでください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "choices"
   },
   "source": [
    "- $V^{\\pi_{1}}(s)$ は3つの状態で$V^{\\pi_{2}}(s)$より勝っていて、１状態では負けている。勝っている状態のほうが多いので $V^{\\pi_{1}}(s)$ は最適方策と言える。\n",
    "- 最適方策になるためには全てが勝っている必要はない。\n",
    "- 最適方策のほうが良い結果が出ると言える。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- 最適方策とは最も大きな収益が期待できる方策のことです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "answer"
   },
   "source": [
    "- $V^{\\pi_{1}}(s)$ は3つの状態で$V^{\\pi_{2}}(s)$より勝っていて、１状態では負けている。勝っている状態のほうが多いので $V^{\\pi_{1}}(s)$ は最適方策と言える。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5040,
    "exerciseId": "SkRenxkGQLG",
    "id": "code_session_name",
    "important": false,
    "isDL": false,
    "timeoutSecs": 10
   },
   "source": [
    "### 2.4.2 行動価値"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "状態関数というのは\n",
    " - ある状態をスタート地点にした時の報酬の期待値  \n",
    "\n",
    "という前提が存在していました。しかし実際のタスクにおいては\n",
    " - ある状態をスタート地点にしてある行動を起こした時の報酬の期待値  \n",
    "\n",
    "とした方が良い場合が多いです。このような新しい行動まで含んだ価値の事を**行動価値**(action value)と呼び <br>\n",
    "「ある方策下において、ある状態をスタート地点にしてある行動を起こした時の報酬の期待値」を示す関数である**行動価値関数**を<br>\n",
    "\n",
    "$Q^{\\pi}(s) = \\mathbb{E}^{\\pi}[G_{t+1} |S_{t} = s,A_{t} = a]$  \n",
    "\n",
    "と定義します。端的にいうと、行動価値関数とは、各行動がどれだけよいかを判断する関数と言えます。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "常に行動Yを取り続けるとした時、状態Aにおいて行動Yを取った時の行動状態価値を計算して下さい。<br>\n",
    "ただし $\\gamma$ は0.8、報酬は5とします。<br>\n",
    "\n",
    "出力の方法は上に計算しても、プログラムに計算させても構いません。<br>\n",
    "数字のみを出力してください。\n",
    "<img src=\"https://aidemyexcontentspic.blob.core.windows.net/contents-pic/_RL/img_7.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "index"
   },
   "outputs": [],
   "source": [
    "# 答えを計算して出力してください\n",
    "# NUM は十分大きい数とする\n",
    "NUM = 10000\n",
    "gamma = \n",
    "reward = \n",
    "\n",
    "gamma_n = \n",
    "Q = \n",
    "# NUMが無限ではないためQには誤差が生じてしまいます\n",
    "# そのためQを小数点5桁程度で近似します\n",
    "Q = round(Q, 5)\n",
    "\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- 等比級数の公式を利用します。<br>\n",
    "等比級数の公式は、<br>\n",
    "$a+ar + ar^2 + ar^n$ = $\\large\\frac{a(1 - r^n)}{1 - r}$ <br>\n",
    " と表すことができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "answer"
   },
   "outputs": [],
   "source": [
    "# 答えを計算して出力してください\n",
    "# NUM は十分大きい数とする\n",
    "NUM = 10000\n",
    "gamma = 0.8\n",
    "reward = 5\n",
    "\n",
    "gamma_n = gamma**NUM\n",
    "Q = reward*(1 - gamma_n) / (1 - gamma)\n",
    "# NUMが無限ではないためQには誤差が生じてしまいます\n",
    "# そのためQを小数点5桁程度で近似します\n",
    "Q = round(Q, 5)\n",
    "\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5040,
    "exerciseId": "B1yW2g1zQ8G",
    "id": "quiz_session_name",
    "important": false,
    "isDL": false,
    "timeoutSecs": 10
   },
   "source": [
    "### 2.4.3 最適状態価値関数・最適行動価値関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "**最適方策**を定義することができました。強化学習の目的はこれを試行錯誤の中から見つけていくことにあります。    \n",
    "\n",
    "これから最適な方策をいかに見つけていくかということを議論するため、最適な方策に従った場合の状態価値関数についても再定義します。これを**最適状態価値関数**と定義し、$V^{*}(s)$と置きます。端的にいうと、各状態がどれだけよいかを判断する関数と言えます。\n",
    "\n",
    "また状態価値関数と同様に行動価値関数にも**最適行動価値関数**が存在し、\n",
    "\n",
    "$Q^{*}(s) = =\\max\\limits_{\\pi} Q^{\\pi}(s,a)$  \n",
    "\n",
    "と定義します。\n",
    "\n",
    "\n",
    "以上これまでで\n",
    "- 最適状態価値関数\n",
    "- 状態価値関数\n",
    "- 最適行動価値関数\n",
    "- 行動価値関数  \n",
    "\n",
    "を定義することができました。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "ではこのような最適行動価値関数が完全に求まったとするとどの方策を選べばいいのでしょうか。  \n",
    "以下の中から選んで下さい"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "choices"
   },
   "source": [
    "- greedy手法\n",
    "- ε-greedy手法\n",
    "- ソフトマックス手法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- 最適行動価値関数が求まっているということはどの行動を取れば収益が最も大きくなるかがわかっている状態を指します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "answer"
   },
   "source": [
    "- greedy手法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section_name",
    "sectionId": "SylbnekfXLM"
   },
   "source": [
    "## 2.5 ベルマン方程式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5040,
    "exerciseId": "SyZ-2xyMQUz",
    "id": "quiz_session_name",
    "important": false,
    "isDL": false,
    "timeoutSecs": 10
   },
   "source": [
    "### 2.5.1 最適な状態価値"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "我々の目的は「環境に対する最適な方策」を取得することにありました。  \n",
    "つまり、一度最適状態価値関数がわかってしまえば、それに従って一番価値が大きくなるように動けばいいわけです。この **<font color=#AA0000>最適状態価値関数</font>** を求めるためにはそれぞれの方策における**状態価値関数**を計算して比較する必要があります。(ここまでが前回の復習です。）\n",
    "\n",
    "ここで状態価値関数と行動価値関数を思い出してみましょう。\n",
    "\n",
    "$V^{\\pi}(s) = \\mathbb{E}^{\\pi}[G_{t+1} |S_{t} = s]$  \n",
    "\n",
    "$Q^{\\pi}(s) = \\mathbb{E}^{\\pi}[G_{t+1} |S_{t} = s,A_{t} = a]$  \n",
    "\n",
    "このように期待値を計算する必要があります。期待値を計算するためにはN腕バンディッド問題で行なったように回数を増やせば増やすほど実際の期待値に収束していくことがわかります。このように試行錯誤を繰り返し続けることでこれらの価値関数を求める手法を**モンテカルロ法**と呼びます。しかしこれには大きく以下の3点の問題があります。\n",
    "- 収益を計算する必要があるため、１つのエピソードが終了するまで学習ができません。\n",
    "- 方策が変わるたびに全て学び直しをしなければなりません。\n",
    "- 計算のためには各状態sにおける期待値を保持し続けなければなりません。  \n",
    "\n",
    "このような欠点は状態数が増えれば増えるほど難しくなっていきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "モンテカルロ法を使う際、必要ないものはどれでしょう？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "choices"
   },
   "source": [
    "- 報酬関数\n",
    "- 状態遷移確率\n",
    "- 行動空間"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- モンテカルロ法では実際に計算してみた結果を利用します。\n",
    "- 報酬関数：$s_t$から$a_t$をして$s_{t+1}$になった時にもらえる報酬   \n",
    "状態遷移確率：$s_t$から$a_t$をして$s_{t+1}$になれる確率    \n",
    "行動空間：行動できるパターンの集合   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "answer"
   },
   "source": [
    "- 状態遷移確率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モンテカルロ法では何度も繰り返すことで報酬を得ていきます。状態遷移確率は繰り返したものを平均化することで期待値が求まるので不要です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5040,
    "exerciseId": "SyGb2eyz7If",
    "id": "quiz_session_name",
    "important": false,
    "isDL": false,
    "timeoutSecs": 10
   },
   "source": [
    "### 2.5.2 ベルマン方程式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "前回のセクションのような欠点を解決するために<br>\n",
    "**ある時点での状態sとそこから行動した結果移行する可能性のある状態s'との間に価値関数の関係式が成り立てば良い**　<br>\n",
    "という発想が生まれました。これにより状態価値関数・行動価値関数を漸化式のような形に書き下すことで、逐次的に更新しながら価値関数を求める  \n",
    "ということが可能になりました。このようにして導出された方程式を**ベルマン方程式**と呼称します。このベルマン方程式を得られたデータから、いかに解くことが得策かという議論を進めていきます。\n",
    "\n",
    "$V^{\\pi}(s) =\\displaystyle\\sum_{\\alpha \\in A(s)}^{} \\pi(a|s) \\displaystyle\\sum_{s' \\in S}^{} P(s'|s,a)(r(s,a,s') + \\gamma V^{\\pi}(s'))$\n",
    "\n",
    "$Q^{\\pi}(s,a) =\\displaystyle\\sum_{s' \\in S}^{} (P(s'|s,a) \n",
    "(r(s,a,s') + \\displaystyle\\sum_{\\alpha \\in A(s')}^{} \\gamma P(a'|s')Q^{\\pi}(s',a')))$\n",
    "\n",
    "$\\pi(a|s)$ は方策の選ばれる確率を表しています。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "環境1において  \n",
    "常に行動Xを取るように$\\pi$を設定した場合、  \n",
    "State_Bでの状態価値  \n",
    "を選んでください。\n",
    "\n",
    "ただし$\\gamma$ = 0.8 とします。\n",
    "\n",
    "<img src=\"https://aidemyexcontentspic.blob.core.windows.net/contents-pic/_RL/img_7.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "choices"
   },
   "source": [
    "- 87.267\n",
    "- 90.735\n",
    "- 92.857"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- ベルマン方程式を立てて価値関数を計算し、それぞれの状態について比較してみましょう\n",
    "\n",
    "$V^\\pi(B)$ = $0.2(-10 + \\gamma V^\\pi(B))$ + $0.8(100 + \\gamma * 0)$   \n",
    "$(1-0.16)V^\\pi(B)$ = $78$   \n",
    "$V^\\pi(B)$ = $92.857$   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "answer"
   },
   "source": [
    "- 92.857"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5040,
    "exerciseId": "SkmZ2xkfXUG",
    "id": "quiz_session_name",
    "important": false,
    "isDL": false,
    "timeoutSecs": 10
   },
   "source": [
    "### 2.5.3 ベルマン最適方程式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "ある方策に従った時の**状態価値関数**・**行動価値関数**に関してベルマン方程式が存在することを前回の式で示しましたが、Chapter2.4で触れたような最適状態価値関数・最適行動価値関数も当然**状態価値関数**・**行動価値関数**のひとつであるため、対応するベルマン方程式が定義でき、以下の式のように書き下すことができます。  \n",
    "\n",
    "$V^{*}(s) = \\max\\limits_{\\alpha \\in A} \\displaystyle\\sum_{s'\\in S}^{} P(s'|s,a)(r(s,a,s') + \\gamma V^{*}(s'\n",
    "))$\n",
    "\n",
    "$Q^{*}(s,a) = \\displaystyle\\sum_{s'\\in S}^{} P(s'|s,a)(r(s,a,s') + \\gamma \\max\\limits_{\\alpha \\in A} Q^{*}(s,a))$\n",
    "\n",
    "これらのことを特別に **<font color=#AA0000>ベルマン最適方程式</font>** と呼びます。**ベルマン最適方程式**とは最適な方策をとった時の価値関数、行動価値関数を定義するものです。先ほどのベルマン方程式において常に最適な行動をとるようにするとこの式が求まります。\n",
    "\n",
    "数式の意味を理解するために、以下の問いを解きましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "環境1,State_Aにおいて  \n",
    "- $\\pi_{1}$を、常に行動Xを取り続ける方策    \n",
    "- $\\pi_{2}$を、常に行動Yを取り続ける方策    \n",
    "\n",
    "と定義した時、どちらの方策が良い方策と言えるでしょうか \n",
    "\n",
    "上で求めたState_Bで方策$\\pi_{1}$における状態価値である、92.857は使っていただいて構いません。\n",
    "<img src=\"https://aidemyexcontentspic.blob.core.windows.net/contents-pic/_RL/img_7.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "choices"
   },
   "source": [
    "- 方策$\\pi_{1}$\n",
    "- 方策$\\pi_{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- ベルマン方程式を立てて価値関数を計算し、それぞれの状態について比較してみましょう\n",
    "\n",
    "方策$\\pi_{1}$ <br>\n",
    "$V(A)=0.3(5 + 0.8 \\times V(0))$\n",
    "$(1 - 0.24) \\times V(A) = 1.5 - 7 + 0.56 \\times 92.857$<br>\n",
    "$V(A) = 61.184$\n",
    "\n",
    "方策$\\pi_{2}$ <br>\n",
    "$V(A) = \\frac{5(1 - 0.8^{\\infty})} {1 - 0.8}$<br>\n",
    "$V(A) = 25$ <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "answer"
   },
   "source": [
    "- 方策$\\pi_{1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "chapter_exam"
   },
   "source": [
    "## 2.5 添削問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "環境1においてすべての行動価値関数を実際に計算して、最適行動価値関数とその時の方策を求めてください。\n",
    "\n",
    "<img src=\"https://aidemyexcontentspic.blob.core.windows.net/contents-pic/_RL/img_7.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "index"
   },
   "outputs": [],
   "source": [
    "from markov import *\n",
    "\n",
    "# 以下の空欄を埋めてください。\n",
    "def take_single_action(state, action):\n",
    "    terminals = \n",
    "    x = \n",
    "    cumulative_probability = \n",
    "    for probability_state in T(state,action):\n",
    "        probability, next_state = \n",
    "        cumulative_probability += \n",
    "        if x < cumulative_probability:\n",
    "            break\n",
    "    return next_state\n",
    "\n",
    "# 以下の空欄を埋めてください。\n",
    "def calculate_value():\n",
    "    sumg = \n",
    "    for n in range(2):\n",
    "        for m in range(2):\n",
    "            state = \n",
    "            action = \n",
    "            for i in range(10):\n",
    "                after_state = \n",
    "                sumg[n][m] += \n",
    "                state = \n",
    "    return sumg\n",
    "\n",
    "state_transition = np.array(\n",
    "                    [[0, 0, 0, 0.3, 5],\n",
    "                    [0, 1, 0, 0.7, -10],\n",
    "                    [0, 0, 1, 1, 5],\n",
    "                    [1, 0, 1, 1, 5],\n",
    "                    [1, 2, 0, 0.8, 100],\n",
    "                    [1, 1, 0, 0.2, 1]]\n",
    "                    )\n",
    "\n",
    "states = np.unique(state_transition[:, 0])\n",
    "actlist = np.unique(state_transition[:, 2])\n",
    "terminals = [2]\n",
    "init = [0]\n",
    "gamma = 0.8\n",
    "\n",
    "\n",
    "# 答えとなる関数を実行します。\n",
    "Q = calculate_value()\n",
    "Q_optimum = Q.max(axis = 1)\n",
    "Q_pi = Q.argmax(axis = 1)\n",
    "print(Q_optimum)\n",
    "print(Q_pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- sumgにはstateが0,1それぞれの状態におけるaction X,Yを繰り返した結果の行動価値関数を収納してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-26T04:39:29.643156Z",
     "start_time": "2018-09-26T04:39:29.281258Z"
    },
    "id": "answer"
   },
   "outputs": [],
   "source": [
    "from markov import *\n",
    "\n",
    "# 答えを入力して下さい。\n",
    "def take_single_action(state, action):\n",
    "    terminals = [2]\n",
    "    x = random.uniform(0, 1)\n",
    "    cumulative_probability = 0.0\n",
    "    for probability_state in T(state,action):\n",
    "        probability, next_state = probability_state\n",
    "        cumulative_probability += probability\n",
    "        if x < cumulative_probability:\n",
    "            break\n",
    "    return next_state\n",
    "\n",
    "# 答えを入力して下さい。\n",
    "def calculate_value():\n",
    "    sumg = np.zeros((2,2))\n",
    "    for n in range(2):\n",
    "        for m in range(2):\n",
    "            state = [n]\n",
    "            action = [m]\n",
    "            for i in range(10):\n",
    "                after_state = take_single_action(state, action)\n",
    "                sumg[n][m] += (gamma**(i))*R(state, action, after_state)\n",
    "                state = after_state\n",
    "    return sumg\n",
    "\n",
    "state_transition = np.array(\n",
    "                    [[0, 0, 0, 0.3, 5],\n",
    "                    [0, 1, 0, 0.7, -10],\n",
    "                    [0, 0, 1, 1, 5],\n",
    "                    [1, 0, 1, 1, 5],\n",
    "                    [1, 2, 0, 0.8, 100],\n",
    "                    [1, 1, 0, 0.2, 1]]\n",
    "                    )\n",
    "\n",
    "states = np.unique(state_transition[:, 0])\n",
    "actlist = np.unique(state_transition[:, 2])\n",
    "terminals = [2]\n",
    "init = [0]\n",
    "gamma = 0.8\n",
    "\n",
    "\n",
    "# 答えとなる関数を実行します。\n",
    "Q = calculate_value()\n",
    "Q_optimum = Q.max(axis = 1)\n",
    "Q_pi = Q.argmax(axis = 1)\n",
    "print(Q_optimum)\n",
    "print(Q_pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "341px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}